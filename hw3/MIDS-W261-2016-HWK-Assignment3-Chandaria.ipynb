{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #3=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "DATSCIW261 ASSIGNMENT #3\n",
    "\n",
    "Version 2016-01-27 (FINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hetal Chandaria (hetalchandaria@berkeley.edu)\n",
    "W261 - 2 , ASSIGNMENT #3\n",
    "\n",
    "Submission Date : 02/01/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.0.\n",
    "What is a merge sort? Where is it used in Hadoop?\n",
    "\n",
    "How is  a combiner function in the context of Hadoop?  Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "\n",
    "What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue;\">Answer</span>\n",
    "\n",
    "#### Merge Sort \n",
    "\n",
    "Merge sort is a sort algorithm that sorts a dataset by divide-and-conquer. The data set is split continuously into sub-sets until each sub-set is size of 1 and therefore considered automatically sorted. Now, sub-sets are merged together to form a final fully sorted data-set. Given n sorted-subsets, the complexity to merge these sorted sub-sets is proportional to the total no. of elements in all the sub-sets combined.\n",
    "\n",
    "Hadoop can be considered in some sense an implementation of merge sort as it first partitions the dataset into multiple map tasks, sorts each map output and eventually does a \"merge\" of each map's sorted output in the reducer. A lot of the heavy lifting of this implementation is done in the shuffle layer. \n",
    "\n",
    "#### Combiner Function\n",
    "\n",
    "Given that data comes to the mapper in no defined order, the mapper output can be quite verbose as it can potentially not make any optimizations. The combiner function is run on mapper output and can \"coalesce/combine\" the mapper output so that the verbose map output does not fully need to be sent to the reducer. The combiner pretty much does the same work as that of the reducer. It has to process data in the format of the mapper output but unlike the reducer, it also generate output that is consumable by the reducer. \n",
    "\n",
    "Let us consider a job that accepts the following input (word, priority) and the aim is to generate the avg. priority for each word. For a job without a combiner, the mapper could generate the following output (word, priority). This would be then averaged upon by the reducer. However, let's say that the word \"foo\" occurs a million times in the same mapper. In this scenario, without a combiner, 1 million rows will be sent to the reducer. A combiner if run could potentially optimize this into an key-value output such as (word, (sum(priority), frequency of word)). Using this tuple, the reducer can then calculate the avg. priority. Note, that the mapper output would also change to (word, (priority, 1)).\n",
    "\n",
    "Not all operations support a combiner. Only commutative and associative operations can work as the output of the reducer has to be the same regardless of whether the combiner ran or not. \n",
    "\n",
    "#### Hadoop shuffle\n",
    "\n",
    "The hadoop shuffle is the layer that transfers the data from the mappers to the reducers. Consider 4 maps and 2 reducers. Each mapper will generate data in 2 partitions considering that there are 2 reducers. This is governed by the partitioner function which can be overridden by the user. The shuffle layer will try to efficiently transfer the data from each mapper to a particular reducer, ensure that the data is sorted correctly by the configured keys ( by defining an approrpiate comparator used to compare keys when sorting ) and that data for a given key is provided in a sorted/grouped manner to the reducer. This is what enables us to write simple reducers without requiring us to do a lot of in-memory buffering. For example, consider the input data of format (word, priority) where the aim is to find max(priority) for each word. If the words came to the reducer in a random order, the reducer would need to keep a lookup table to track each word and its max priority given that there is no information on whether the word may or may not show up later. Given the sorted nature of input to the reducer, the simplistic reducer knows that as soon as a new word is seen, the previous word will never again show up and the max priority can then be generated for that word. This means no need to have a large lookup table for each seen word but rather just one object to track the data for the current word and its max priority.\n",
    "\n",
    "The diagram below depicts the various components of hadoop shuffle. \n",
    "\n",
    "<img src=\"./images/shuffle.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Create directories in HDFS for HW3\n",
    "!hdfs dfs -mkdir hw3\n",
    "!hdfs dfs -mkdir hw3/src\n",
    "!hdfs dfs -mkdir hw3/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.1 \n",
    "Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.1 In this question ,we will only have a mapper which emit counters based on the type of complaint.\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    key = None\n",
    "    if(token[1]=='Debt collection'):\n",
    "        sys.stderr.write(\"reporter:counter:Debt-counter,debt,1\\n\")\n",
    "    elif(token[1]=='Mortgage'):\n",
    "        sys.stderr.write(\"reporter:counter:Mortgage-counter,mortgage,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:Other-counter,other,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 16:18:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw31\n",
      "16/01/31 16:18:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/src/Consumer_Complaints_mod.csv\n",
      "16/01/31 16:18:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 16:18:39 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 16:18:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar6525908040360619174/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob381896063830971794.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_1():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw31\n",
    "    \n",
    "    #remove header line from Consumer_Complaints.csv\n",
    "    !tail -n $(($(wc -l < Consumer_Complaints.csv) - 1))  < Consumer_Complaints.csv > Consumer_Complaints_mod.csv\n",
    "    \n",
    "    #move input file to hdfs \n",
    "    !hdfs dfs -rm hw3/src/Consumer_Complaints_mod.csv\n",
    "    !hdfs dfs -put Consumer_Complaints_mod.csv  hw3/src/\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.reduce.tasks=0 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -input hw3/src/Consumer_Complaints_mod.csv \\\n",
    "    -output hw3/output/hw31 \\\n",
    "\n",
    "hw3_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the above from job tracker\n",
    "\n",
    "<img src=\"./images/hw31.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2a In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is word and 1\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32a,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    for token in line.strip().split(\" \"):\n",
    "        print '%s,%s' % (token, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2a In this question, we will emit a counter for everytime the reducer is called. \n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32a,num_reducers,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split(',', 1)\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '%s\\t%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "if last_key == word:\n",
    "    print '%s\\t%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 21:13:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `hw3/output/hw32a': No such file or directory\n",
      "16/02/01 21:13:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/src/hw32_a_input.txt\n",
      "16/02/01 21:13:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 21:13:22 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 21:13:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar1434841608207789672/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob4885378584051724945.jar tmpDir=null\n",
      "\n",
      "\n",
      "Word and its frequency\n",
      "16/02/01 21:13:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "labs\t1\n",
      "quux\t2\n",
      "foo\t3\n",
      "bar\t1\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_a():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #create inpur file and move it to hdfs \n",
    "    !echo \"foo foo quux labs foo bar quux\" > hw32_a_input.txt \n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32a\n",
    "    \n",
    "    #move input file to hdfs \n",
    "    !hdfs dfs -rm hw3/src/hw32_a_input.txt\n",
    "    !hdfs dfs -put hw32_a_input.txt  hw3/src/\n",
    "    \n",
    "    # run map reduce job. Here we have explicitly set the number of mappers and reducers\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.map.tasks=1 \\\n",
    "    -numReduceTasks 4 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/hw32_a_input.txt \\\n",
    "    -output hw3/output/hw32a\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"Word and its frequency\"\n",
    "    !hdfs dfs -cat hw3/output/hw32a/part*\n",
    "\n",
    "hw3_2_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output for first part\n",
    "\n",
    "<img src=\"./images/hw32a.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.b\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers). Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2b In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is issue which is token[3] and 1\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "sys.stderr.write(\"reporter:counter:HW_32b,num_mappers,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    #3rd token  contains text about issue\n",
    "    issue = re.findall(WORD_RE,token[3])\n",
    "    #now loop through all words in the issue and emit it \n",
    "    for word in issue:\n",
    "        print '\"%s\",%s' %(word.lower(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2b In this question, we will emit a counter for everytime the reducer is called. \n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32b,num_reducers,1\\n\")\n",
    "last_key = None #user for keeping track of which key is currently being aggregated\n",
    "word = None\n",
    "total_count = 0 #counter to increment frequency\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    word=token[0]\n",
    "    count = int(token[1])\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '%s\\t%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "if last_key == word:\n",
    "    print '%s\\t%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 18:10:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw32b\n",
      "16/02/03 18:10:56 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 18:10:56 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar6957153217036626704/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob2676896676320192719.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/02/03 18:11:18 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "action\t2964\n",
      "advance\t240\n",
      "advertising\t1193\n",
      "amount\t98\n",
      "amt\t71\n",
      "application\t8868\n",
      "apply\t118\n",
      "are\t3821\n",
      "atm\t2422\n",
      "attempts\t17972\n",
      "balance\t597\n",
      "bank\t202\n",
      "cancelling\t2795\n",
      "card\t4405\n",
      "charged\t976\n",
      "collect\t17972\n",
      "collection\t72394\n",
      "communication\t8671\n",
      "contact\t3710\n",
      "costs\t4350\n",
      "credited\t92\n",
      "customer\t2734\n",
      "day\t71\n",
      "dealing\t1944\n",
      "debit\t2422\n",
      "decision\t2774\n",
      "decrease\t1149\n",
      "didn't\t925\n",
      "disclosures\t64\n",
      "dispute\t904\n",
      "embezzlement\t3276\n",
      "fee\t3198\n",
      "forbearance\t350\n",
      "fraud\t3842\n",
      "funds\t5663\n",
      "get\t4357\n",
      "getting\t291\n",
      "identity\t4729\n",
      "illegal\t2964\n",
      "improper\t4966\n",
      "increase\t1149\n",
      "info\t3553\n",
      "information\t29069\n",
      "interest\t4238\n",
      "investigation\t4858\n",
      "issues\t538\n",
      "late\t1797\n",
      "lease\t6337\n",
      "lender\t2165\n",
      "line\t1732\n",
      "loan\t119630\n",
      "low\t5663\n",
      "managing\t5006\n",
      "marketing\t1193\n",
      "missing\t64\n",
      "modification\t70487\n",
      "money\t3639\n",
      "monitoring\t1453\n",
      "mortgage\t8625\n",
      "my\t10731\n",
      "opening\t16205\n",
      "originator\t8625\n",
      "other\t7886\n",
      "out\t1242\n",
      "pay\t3821\n",
      "payment\t92\n",
      "plans\t350\n",
      "practices\t1003\n",
      "privacy\t240\n",
      "problems\t9484\n",
      "rate\t3431\n",
      "receiving\t3226\n",
      "report\t34903\n",
      "reporting\t6559\n",
      "rewards\t1002\n",
      "scam\t566\n",
      "score\t4357\n",
      "sending\t3226\n",
      "servicing\t36767\n",
      "sharing\t3489\n",
      "shopping\t672\n",
      "statements\t3621\n",
      "stop\t131\n",
      "taking\t4206\n",
      "transaction\t1485\n",
      "underwriting\t2774\n",
      "using\t2422\n",
      "when\t4095\n",
      "with\t1944\n",
      "withdrawals\t10555\n",
      "a\t3503\n",
      "account\t57448\n",
      "acct\t163\n",
      "an\t2964\n",
      "and\t16448\n",
      "applied\t139\n",
      "apr\t3431\n",
      "arbitration\t168\n",
      "available\t274\n",
      "bankruptcy\t222\n",
      "being\t5663\n",
      "billing\t8158\n",
      "broker\t8625\n",
      "by\t5663\n",
      "can't\t1999\n",
      "cash\t240\n",
      "caused\t5663\n",
      "changes\t350\n",
      "charges\t131\n",
      "checks\t75\n",
      "closing\t19000\n",
      "company's\t4858\n",
      "cont'd\t17972\n",
      "convenience\t75\n",
      "credit\t55251\n",
      "debt\t27874\n",
      "delay\t243\n",
      "delinquent\t1061\n",
      "deposits\t10555\n",
      "determination\t1490\n",
      "did\t139\n",
      "disclosure\t7655\n",
      "disputes\t6938\n",
      "escrow\t36767\n",
      "expect\t807\n",
      "false\t3621\n",
      "fees\t807\n",
      "for\t929\n",
      "foreclosure\t70487\n",
      "i\t925\n",
      "incorrect\t29133\n",
      "issuance\t640\n",
      "issue\t1098\n",
      "making\t3226\n",
      "management\t16205\n",
      "not\t18477\n",
      "of\t13983\n",
      "on\t29069\n",
      "or\t40508\n",
      "overlimit\t127\n",
      "owed\t17972\n",
      "payments\t39993\n",
      "payoff\t1155\n",
      "process\t5505\n",
      "processing\t243\n",
      "promised\t274\n",
      "protection\t4139\n",
      "receive\t139\n",
      "received\t216\n",
      "relations\t1367\n",
      "repay\t1647\n",
      "repaying\t3844\n",
      "representation\t3621\n",
      "sale\t139\n",
      "service\t1518\n",
      "servicer\t1944\n",
      "settlement\t4350\n",
      "statement\t1220\n",
      "tactics\t8671\n",
      "terms\t350\n",
      "the\t6248\n",
      "theft\t3276\n",
      "threatening\t2964\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t8178\n",
      "unsolicited\t640\n",
      "use\t1477\n",
      "verification\t7655\n",
      "was\t274\n",
      "workout\t350\n",
      "wrong\t169\n",
      "you\t3821\n",
      "your\t3844\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_b():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32b\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.map.tasks=2 \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/Consumer_Complaints_mod.csv \\\n",
    "    -output hw3/output/hw32b\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw32b/part*\n",
    "\n",
    "hw3_2_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output for 32b of custome counters\n",
    "<img src=\"./images/hw32b.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.c\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is issue which is token[3] and 1\n",
    "import sys\n",
    "from csv import reader\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "sys.stderr.write(\"reporter:counter:HW_32c,num_mappers,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    #3rd token  contains text about issue\n",
    "    issue = re.findall(WORD_RE,token[3])\n",
    "    #now loop through all words in the issue and emit it \n",
    "    for word in issue:\n",
    "        print '\"%s\",%s' %(word.lower(),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c In this question, we will emit a counter for everytime the combiner is called. \n",
    "#the combiner will do intermediate aggregation of data and is similar to reducer in terms of logic\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32c,num_combiners,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    word=token[0]\n",
    "    count = int(token[1])\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '\"%s\",%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "if last_key == word:\n",
    "    print '\"%s\",%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c In this question, we will emit a counter for everytime the reducer is called. \n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32c,num_reducers,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    word=token[0]\n",
    "    count = int(token[1])\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '\"%s\",%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "# do not forget to output the last word\n",
    "if last_key == word:\n",
    "    print '\"%s\",%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 18:12:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw32c\n",
      "16/02/03 18:12:38 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 18:12:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar2932626776036871795/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob101464256127590978.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/02/03 18:13:01 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"action\",2964\t\n",
      "\"advance\",240\t\n",
      "\"advertising\",1193\t\n",
      "\"amount\",98\t\n",
      "\"amt\",71\t\n",
      "\"application\",8868\t\n",
      "\"apply\",118\t\n",
      "\"are\",3821\t\n",
      "\"atm\",2422\t\n",
      "\"attempts\",17972\t\n",
      "\"balance\",597\t\n",
      "\"bank\",202\t\n",
      "\"cancelling\",2795\t\n",
      "\"card\",4405\t\n",
      "\"charged\",976\t\n",
      "\"collect\",17972\t\n",
      "\"collection\",72394\t\n",
      "\"communication\",8671\t\n",
      "\"contact\",3710\t\n",
      "\"costs\",4350\t\n",
      "cat: Unable to write to output stream.\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_c():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32c\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.map.tasks=4 \\\n",
    "    -numReduceTasks 2 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file combiner.py \\\n",
    "    -combiner combiner.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/Consumer_Complaints_mod.csv \\\n",
    "    -output hw3/output/hw32c\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw32c/part* | head -20\n",
    "\n",
    "hw3_2_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of mapper, recucer and combiner counters\n",
    "<img src=\"./images/hw32c.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  part 2 of 3_2_c\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c_1 In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is frequency and issue \n",
    "# output of previous mapreduce job is used as input for this map reduce task\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper-counter,num_mappers,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    print '%s\\t\"%s\"' %(token[1],token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c_1 In this question, we will emit a counter for everytime the reducer is called. \n",
    "# In this case of mapreduce we are using secondary sort where in we first sort on the frequency (ascending) \n",
    "# and than sort on the issue in case of tie\n",
    "\n",
    "import sys, Queue\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer-counter,num_reducers,1\\n\")\n",
    "\n",
    "n_max = 50 # top n issues \n",
    "n_min = 10 # bottom n issues \n",
    "a_min = [] # list to hold the bototom issues \n",
    "q_max = Queue.Queue(n_max) # Queue to hold the top 50 issues\n",
    "total_count =0 \n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    freq = int(row[0])\n",
    "    key = row[1]\n",
    "    #increment the count\n",
    "    total_count += freq\n",
    "    \n",
    "    # add the lowest 10 words as we are getting data in sorted order already\n",
    "    if len(a_min) < n_min:\n",
    "        a_min.append((key,freq))\n",
    "    \n",
    "    #Now add the top 10 words . In this case we use queue as its FIFO which will always pop the lowest element\n",
    "    if q_max.full():\n",
    "        q_max.get()\n",
    "    q_max.put((key,freq))\n",
    "    \n",
    "print '\\n Output = Word , Frequency and Relative Frequency'\n",
    "\n",
    "print '\\n%d Bottom issues:' %n_min\n",
    "for record in a_min:\n",
    "    print record[0] +\"\\t\"+str(record[1])+\"\\t\"+str(1.0*record[1]/total_count)\n",
    "\n",
    "print '\\n%d Top issues:' %n_max\n",
    "for i in range(n_max):\n",
    "    record = q_max.get()\n",
    "    print record[0] +\"\\t\"+str(record[1])+\"\\t\"+str(1.0*record[1]/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 18:14:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw32c_2\n",
      "16/02/03 18:14:29 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 18:14:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar4136695429102537105/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob6409782510192040952.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/02/03 18:14:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      " Output = Word , Frequency and Relative Frequency\t\n",
      "\t\n",
      "10 Bottom issues:\t\n",
      "\"missing\"\t64\t4.74668992545e-05\n",
      "\"disclosures\"\t64\t4.74668992545e-05\n",
      "\"amt\"\t71\t5.26585913604e-05\n",
      "\"day\"\t71\t5.26585913604e-05\n",
      "\"checks\"\t75\t5.56252725638e-05\n",
      "\"convenience\"\t75\t5.56252725638e-05\n",
      "\"credited\"\t92\t6.82336676783e-05\n",
      "\"payment\"\t92\t6.82336676783e-05\n",
      "\"amount\"\t98\t7.26836894834e-05\n",
      "\"apply\"\t118\t8.75170955004e-05\n",
      "\t\n",
      "50 Top issues:\t\n",
      "\"low\"\t5663\t0.00420007891372\n",
      "\"caused\"\t5663\t0.00420007891372\n",
      "\"funds\"\t5663\t0.00420007891372\n",
      "\"being\"\t5663\t0.00420007891372\n",
      "\"by\"\t5663\t0.00420007891372\n",
      "\"the\"\t6248\t0.00463395603972\n",
      "\"lease\"\t6337\t0.00469996469649\n",
      "\"reporting\"\t6559\t0.00486461550328\n",
      "\"disputes\"\t6938\t0.00514570854731\n",
      "\"verification\"\t7655\t0.00567748615302\n",
      "\"disclosure\"\t7655\t0.00567748615302\n",
      "\"other\"\t7886\t0.00584881199251\n",
      "\"billing\"\t8158\t0.00605054631434\n",
      "\"unable\"\t8178\t0.00606537972036\n",
      "\"to\"\t8401\t0.00623077219745\n",
      "\"originator\"\t8625\t0.00639690634484\n",
      "\"broker\"\t8625\t0.00639690634484\n",
      "\"mortgage\"\t8625\t0.00639690634484\n",
      "\"tactics\"\t8671\t0.00643102317868\n",
      "\"communication\"\t8671\t0.00643102317868\n",
      "\"application\"\t8868\t0.00657713222795\n",
      "\"problems\"\t9484\t0.00703400113327\n",
      "\"deposits\"\t10555\t0.00782833002548\n",
      "\"withdrawals\"\t10555\t0.00782833002548\n",
      "\"my\"\t10731\t0.00795886399843\n",
      "\"of\"\t13983\t0.0103707758168\n",
      "\"management\"\t16205\t0.0120187672253\n",
      "\"opening\"\t16205\t0.0120187672253\n",
      "\"and\"\t16448\t0.0121989931084\n",
      "\"collect\"\t17972\t0.0133292986469\n",
      "\"cont'd\"\t17972\t0.0133292986469\n",
      "\"attempts\"\t17972\t0.0133292986469\n",
      "\"owed\"\t17972\t0.0133292986469\n",
      "\"not\"\t18477\t0.0137038421488\n",
      "\"closing\"\t19000\t0.0140917357162\n",
      "\"debt\"\t27874\t0.0206733179659\n",
      "\"on\"\t29069\t0.0215596139754\n",
      "\"information\"\t29069\t0.0215596139754\n",
      "\"incorrect\"\t29133\t0.0216070808747\n",
      "\"report\"\t34903\t0.0258865185106\n",
      "\"escrow\"\t36767\t0.0272689919514\n",
      "\"servicing\"\t36767\t0.0272689919514\n",
      "\"payments\"\t39993\t0.0296616203419\n",
      "\"or\"\t40508\t0.0300435805469\n",
      "\"credit\"\t55251\t0.0409780257923\n",
      "\"account\"\t57448\t0.0426074754433\n",
      "\"modification\"\t70487\t0.0522781144961\n",
      "\"foreclosure\"\t70487\t0.0522781144961\n",
      "\"collection\"\t72394\t0.0536924797598\n",
      "\"loan\"\t119630\t0.0887260180908\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_c():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32c_2\n",
    "    \n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1n -k2,2\" \\\n",
    "    -D mapred.map.tasks=4 \\\n",
    "    -numReduceTasks 1 \\\n",
    "    -mapper mapper.py \\\n",
    "    -file mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -file reducer.py \\\n",
    "    -input hw3/output/hw32c/part* \\\n",
    "    -output hw3/output/hw32c_2\n",
    "    \n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw32c_2/part*\n",
    "\n",
    "hw3_2_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.2.1 OPTIONAL \n",
    "Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### First create a job to generate the partition file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.2.1_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.2.1_1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "# The input is the result from hw3.2-part3, which has the format:\n",
    "# word<tab>count\n",
    "\n",
    "n = 0\n",
    "rate = 5 # sample one out of every five counts\n",
    "samples = []\n",
    "\n",
    "# We want to sample the count\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.strip().split(\",\")\n",
    "\n",
    "    if n % rate == 0:\n",
    "        samples.append(int(count))\n",
    "        \n",
    "    n += 1\n",
    "    \n",
    "# Now we have a sample of counts.  Let's find the 50% percentile, as we only have 2 reducers.\n",
    "p50 = np.percentile(samples, 50)\n",
    "\n",
    "print p50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.2.1_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.2.1_2.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"--partitionFile\", default=\"partitions.txt\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,mapper_called,1\\n')\n",
    "\n",
    "\n",
    "# The input is the result from hw3.2-part3, which has the format:\n",
    "# word<tab>count\n",
    "\n",
    "# First read the partition file and get the 50th percentile\n",
    "with open(args.partitionFile, 'r') as f:\n",
    "    p50 = float(f.readline())\n",
    "\n",
    "groups = [\"g\" + str(x) for x in range(2)] # names of the two groups\n",
    "\n",
    "for line in sys.stdin:\n",
    "    (word, count) = line.strip().split(\",\")\n",
    "    count = int(count)\n",
    "\n",
    "    # Assign it to different reducers based on the count value\n",
    "    if count >= p50:\n",
    "        group = groups[0]\n",
    "    else:\n",
    "        group = groups[1]\n",
    "        \n",
    "    print group + \"\\t\" + word+\"\\t\"+str(count)\n",
    "\n",
    "    # Use order inversion so that reducer can count the total word count in a single pass\n",
    "    # Need to send it to each group\n",
    "    for g in groups:\n",
    "        print g + \"\\t\" + str(count) + \"\\t\" + str(sys.maxint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 20:59:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "partitions.txt:\n",
      "3276.0\n",
      "\n",
      "reporter:counter:custom,mapper_called,1\n",
      "g0\t1149\t9223372036854775807\n",
      "g0\t118\t9223372036854775807\n",
      "g0\t1193\t9223372036854775807\n",
      "g0\t17972\t9223372036854775807\n",
      "g0\t17972\t9223372036854775807\n",
      "g0\t1944\t9223372036854775807\n",
      "g0\t202\t9223372036854775807\n",
      "g0\t240\t9223372036854775807\n",
      "g0\t2422\t9223372036854775807\n",
      "g0\t2422\t9223372036854775807\n",
      "g0\t2734\t9223372036854775807\n",
      "g0\t2774\t9223372036854775807\n",
      "g0\t2795\t9223372036854775807\n",
      "g0\t2964\t9223372036854775807\n",
      "g0\t3710\t9223372036854775807\n",
      "g0\t3821\t9223372036854775807\n",
      "g0\t4350\t9223372036854775807\n",
      "g0\t4405\t9223372036854775807\n",
      "g0\t597\t9223372036854775807\n",
      "g0\t64\t9223372036854775807\n",
      "g0\t71\t9223372036854775807\n",
      "g0\t71\t9223372036854775807\n",
      "g0\t72394\t9223372036854775807\n",
      "g0\t8671\t9223372036854775807\n",
      "g0\t8868\t9223372036854775807\n",
      "g0\t904\t9223372036854775807\n",
      "g0\t92\t9223372036854775807\n",
      "g0\t925\t9223372036854775807\n",
      "g0\t976\t9223372036854775807\n",
      "g0\t98\t9223372036854775807\n",
      "g0\t\"collection\"\t72394\n",
      "g0\t\"attempts\"\t17972\n",
      "g0\t\"collect\"\t17972\n",
      "g0\t\"application\"\t8868\n",
      "g0\t\"communication\"\t8671\n",
      "g0\t\"card\"\t4405\n",
      "g0\t\"costs\"\t4350\n",
      "g0\t\"are\"\t3821\n",
      "g0\t\"contact\"\t3710\n",
      "g1\t1149\t9223372036854775807\n",
      "g1\t118\t9223372036854775807\n",
      "g1\t1193\t9223372036854775807\n",
      "g1\t17972\t9223372036854775807\n",
      "g1\t17972\t9223372036854775807\n",
      "g1\t1944\t9223372036854775807\n",
      "g1\t202\t9223372036854775807\n",
      "g1\t240\t9223372036854775807\n",
      "g1\t2422\t9223372036854775807\n",
      "g1\t2422\t9223372036854775807\n",
      "g1\t2734\t9223372036854775807\n",
      "g1\t2774\t9223372036854775807\n",
      "g1\t2795\t9223372036854775807\n",
      "g1\t2964\t9223372036854775807\n",
      "g1\t3710\t9223372036854775807\n",
      "g1\t3821\t9223372036854775807\n",
      "g1\t4350\t9223372036854775807\n",
      "g1\t4405\t9223372036854775807\n",
      "g1\t597\t9223372036854775807\n",
      "g1\t64\t9223372036854775807\n",
      "g1\t71\t9223372036854775807\n",
      "g1\t71\t9223372036854775807\n",
      "g1\t72394\t9223372036854775807\n",
      "g1\t8671\t9223372036854775807\n",
      "g1\t8868\t9223372036854775807\n",
      "g1\t904\t9223372036854775807\n",
      "g1\t92\t9223372036854775807\n",
      "g1\t925\t9223372036854775807\n",
      "g1\t976\t9223372036854775807\n",
      "g1\t98\t9223372036854775807\n",
      "g1\t\"action\"\t2964\n",
      "g1\t\"cancelling\"\t2795\n",
      "g1\t\"decision\"\t2774\n",
      "g1\t\"customer\"\t2734\n",
      "g1\t\"atm\"\t2422\n",
      "g1\t\"debit\"\t2422\n",
      "g1\t\"dealing\"\t1944\n",
      "g1\t\"advertising\"\t1193\n",
      "g1\t\"decrease\"\t1149\n",
      "g1\t\"charged\"\t976\n",
      "g1\t\"didn't\"\t925\n",
      "g1\t\"dispute\"\t904\n",
      "g1\t\"balance\"\t597\n",
      "g1\t\"advance\"\t240\n",
      "g1\t\"bank\"\t202\n",
      "g1\t\"apply\"\t118\n",
      "g1\t\"amount\"\t98\n",
      "g1\t\"credited\"\t92\n",
      "g1\t\"amt\"\t71\n",
      "g1\t\"day\"\t71\n",
      "g1\t\"disclosures\"\t64\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "!hdfs dfs -cat hw3/output/hw32c/part* > t\n",
    "!cat t | python mapper_3.2.1_1.py > partitions.txt\n",
    "\n",
    "print \"partitions.txt:\"\n",
    "!cat partitions.txt\n",
    "print\n",
    "\n",
    "!head -n 30 t | \\\n",
    "python mapper_3.2.1_2.py | \\\n",
    "sort -k1,1 -k3,3nr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_3.2.1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.2.1.py\n",
    "#!/usr/bin/python\n",
    "from __future__ import division # Use Python 3-style division\n",
    "import sys, Queue, csv\n",
    "\n",
    "sys.stderr.write('reporter:counter:custom,reducer_called,1\\n')\n",
    "    \n",
    "wordCount = 0 # Count of each word\n",
    "totalCount = 0 # Total number of words\n",
    "curr = None # the current word\n",
    "\n",
    "# input format: \n",
    "# Total count: group \\t count \\t max.int\n",
    "# Word: group \\t word \\t count\n",
    "for fields in csv.reader(sys.stdin, delimiter='\\t'):\n",
    "    group = fields[0]\n",
    "    word = fields[1]\n",
    "    count = fields[2]\n",
    "    count = int(count)\n",
    "\n",
    "    # Find out the total word count.\n",
    "    # We use count == sys.maxint as the special key for order inversion.\n",
    "    if count == sys.maxint:\n",
    "        totalCount += int(word) # The word is the count\n",
    "        continue\n",
    "    \n",
    "    # If we have encountered a new word, output the answer of the current word\n",
    "    if curr != word:\n",
    "        if curr is not None:\n",
    "            print \"\\t\".join([curr, str(wordCount), str(wordCount/totalCount)])\n",
    "            wordCount = 0\n",
    "            \n",
    "    wordCount += count\n",
    "    curr = word\n",
    "\n",
    "# Handle the last word seen\n",
    "if curr is not None:\n",
    "    print \"\\t\".join([curr, str(wordCount), str(wordCount/totalCount)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 20:59:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "partitions.txt:\n",
      "3276.0\n",
      "\n",
      "reporter:counter:custom,reducer_called,1\n",
      "reporter:counter:custom,mapper_called,1\n",
      "collection\t72394\t0.433709965372\n",
      "attempts\t17972\t0.107669634192\n",
      "collect\t17972\t0.107669634192\n",
      "application\t8868\t0.0531278831522\n",
      "communication\t8671\t0.0519476629243\n",
      "card\t4405\t0.026390203573\n",
      "costs\t4350\t0.0260607004637\n",
      "are\t3821\t0.0228914796487\n",
      "contact\t3710\t0.0111132412322\n",
      "action\t2964\t0.00887861105453\n",
      "cancelling\t2795\t0.00837237445932\n",
      "decision\t2774\t0.00830946932027\n",
      "customer\t2734\t0.00818965000779\n",
      "atm\t2422\t0.00725505937047\n",
      "debit\t2422\t0.00725505937047\n",
      "dealing\t1944\t0.00582321858637\n",
      "advertising\t1193\t0.00357361099462\n",
      "decrease\t1149\t0.0034418097509\n",
      "charged\t976\t0.00292359122443\n",
      "didn't\t925\t0.00277082160103\n",
      "dispute\t904\t0.00270791646198\n",
      "balance\t597\t0.00178830323872\n",
      "advance\t240\t0.000718915874861\n",
      "bank\t202\t0.000605087528008\n",
      "apply\t118\t0.000353466971807\n",
      "amount\t98\t0.000293557315568\n",
      "credited\t92\t0.000275584418697\n",
      "amt\t71\t0.000212679279646\n",
      "day\t71\t0.000212679279646\n",
      "disclosures\t64\t0.000191710899963\n"
     ]
    }
   ],
   "source": [
    "# Quick test\n",
    "!hdfs dfs -cat hw3/output/hw32c/part* > t\n",
    "!cat t | python mapper_3.2.1_1.py > partitions.txt\n",
    "\n",
    "print \"partitions.txt:\"\n",
    "!cat partitions.txt\n",
    "print\n",
    "\n",
    "!head -n 30 t | \\\n",
    "python mapper_3.2.1_2.py | \\\n",
    "sort -k1,1 -k3,3nr | \\\n",
    "python reducer_3.2.1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 20:59:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2.1-part1\n",
      "16/02/03 20:59:53 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/03 20:59:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper_3.2.1_1.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar4442655755771471103/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob2084411711848654635.jar tmpDir=null\n",
      "16/02/03 21:00:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# First job - generate partitions file\n",
    "!hdfs dfs -rm -r hw3.2.1-part1\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-D mapreduce.job.name=\"hw3.2.1-part1\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=0 \\\n",
    "-file mapper_3.2.1_1.py -mapper mapper_3.2.1_1.py \\\n",
    "-input hw3/output/hw32c/part* \\\n",
    "-output hw3.2.1-part1\n",
    "\n",
    "! hdfs dfs -cat hw3.2.1-part1/part* > partitions.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/03 21:00:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3.2.1-part2\n",
      "16/02/03 21:00:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/03 21:00:11 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "packageJobJar: [mapper_3.2.1_2.py, reducer_3.2.1.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar455173474606119936/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob6654069116123968994.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r hw3.2.1-part2\n",
    "\n",
    "!hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "-files \"partitions.txt\" \\\n",
    "-D mapreduce.job.name=\"hw3.2.1-part2\" \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=2 \\\n",
    "-D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapred.text.key.comparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-file mapper_3.2.1_2.py -mapper mapper_3.2.1_2.py \\\n",
    "-file reducer_3.2.1.py -reducer reducer_3.2.1.py \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input hw3/output/hw32c/part* \\\n",
    "-output hw3.2.1-part2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 50 words:\n",
      "loan\t119630\t0.0887260180908\n",
      "collection\t72394\t0.0536924797598\n",
      "foreclosure\t70487\t0.0522781144961\n",
      "modification\t70487\t0.0522781144961\n",
      "account\t57448\t0.0426074754433\n",
      "credit\t55251\t0.0409780257923\n",
      "or\t40508\t0.0300435805469\n",
      "payments\t39993\t0.0296616203419\n",
      "escrow\t36767\t0.0272689919514\n",
      "servicing\t36767\t0.0272689919514\n",
      "report\t34903\t0.0258865185106\n",
      "incorrect\t29133\t0.0216070808747\n",
      "information\t29069\t0.0215596139754\n",
      "on\t29069\t0.0215596139754\n",
      "debt\t27874\t0.0206733179659\n",
      "closing\t19000\t0.0140917357162\n",
      "not\t18477\t0.0137038421488\n",
      "attempts\t17972\t0.0133292986469\n",
      "collect\t17972\t0.0133292986469\n",
      "cont'd\t17972\t0.0133292986469\n",
      "owed\t17972\t0.0133292986469\n",
      "and\t16448\t0.0121989931084\n",
      "management\t16205\t0.0120187672253\n",
      "opening\t16205\t0.0120187672253\n",
      "of\t13983\t0.0103707758168\n",
      "my\t10731\t0.00795886399843\n",
      "deposits\t10555\t0.00782833002548\n",
      "withdrawals\t10555\t0.00782833002548\n",
      "problems\t9484\t0.00703400113327\n",
      "application\t8868\t0.00657713222795\n",
      "communication\t8671\t0.00643102317868\n",
      "tactics\t8671\t0.00643102317868\n",
      "broker\t8625\t0.00639690634484\n",
      "mortgage\t8625\t0.00639690634484\n",
      "originator\t8625\t0.00639690634484\n",
      "to\t8401\t0.00623077219745\n",
      "unable\t8178\t0.00606537972036\n",
      "billing\t8158\t0.00605054631434\n",
      "other\t7886\t0.00584881199251\n",
      "disclosure\t7655\t0.00567748615302\n",
      "verification\t7655\t0.00567748615302\n",
      "disputes\t6938\t0.00514570854731\n",
      "reporting\t6559\t0.00486461550328\n",
      "lease\t6337\t0.00469996469649\n",
      "the\t6248\t0.00463395603972\n",
      "being\t5663\t0.00420007891372\n",
      "by\t5663\t0.00420007891372\n",
      "caused\t5663\t0.00420007891372\n",
      "funds\t5663\t0.00420007891372\n",
      "low\t5663\t0.00420007891372\n",
      "\n",
      "Bottom 10 words:\n",
      "apply\t118\t8.75170955004e-05\n",
      "amount\t98\t7.26836894834e-05\n",
      "credited\t92\t6.82336676783e-05\n",
      "payment\t92\t6.82336676783e-05\n",
      "checks\t75\t5.56252725638e-05\n",
      "convenience\t75\t5.56252725638e-05\n",
      "amt\t71\t5.26585913604e-05\n",
      "day\t71\t5.26585913604e-05\n",
      "disclosures\t64\t4.74668992545e-05\n",
      "missing\t64\t4.74668992545e-05\n"
     ]
    }
   ],
   "source": [
    "# Get the list of output files\n",
    "!hdfs dfs -ls hw3.2.1-part2/part* 2>/dev/null | \\\n",
    "tr -s ' ' | cut -d ' ' -f 8 > hw3.2.1_output_files\n",
    "\n",
    "# For each file name, append the word count taken from the 1st line in the file content\n",
    "!rm -f hw3.2.1_output_files_with_size\n",
    "!for f in `cat hw3.2.1_output_files`; do hdfs dfs -cat $f 2>/dev/null | \\\n",
    "head -n 1 | cut -d$'\\t' -f 2 | \\\n",
    "xargs echo \"$f\" >> hw3.2.1_output_files_with_size; done\n",
    "\n",
    "# Sort hw3.2.1_output_files_with_size by size, in descending order.\n",
    "# Then generate a sorted list of output files, based on the sorted size\n",
    "!sort -t ' ' -k2,2nr hw3.2.1_output_files_with_size | \\\n",
    "cut -d ' ' -f1 > hw3.2.1_output_files_sorted\n",
    "\n",
    "# Output the top 50\n",
    "print \"Top 50 words:\"\n",
    "!sed -n 1p hw3.2.1_output_files_sorted | xargs hdfs dfs -cat 2>/dev/null | head -n 50 \n",
    "\n",
    "print\n",
    "\n",
    "print \"Bottom 10 words:\"\n",
    "!sed -n 2p hw3.2.1_output_files_sorted | xargs hdfs dfs -cat 2>/dev/null | tail -n 10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 In this question, we will emit a counter for everytime the reducer is called. \n",
    "# Here the mapper reads each session information and emits product along with the count.\n",
    "# We also emit the number of products in each basket and basket count as for each line read.\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,num_mapper,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    count = 0 #used to determine basket length\n",
    "    # remove leading and trailing whitespace\n",
    "    for token in line.strip().split(\" \"):\n",
    "        print '%s\\t%s' % (token, 1)\n",
    "        count += 1\n",
    "    print '%s\\t%s'%('*BASKET',count) # total number of products in the basket\n",
    "    print '%s\\t%s'%('*NUM_OF_BASKET',1) # basket count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 In this question, we will emit a counter for everytime the combiner is called. \n",
    "# Here the combiner code does intermediate aggregation for product, number of baskets \n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,num_combiners,1\\n\")\n",
    "\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "basket_cnt = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = row[0]\n",
    "    count = int(row[1])\n",
    "    \n",
    "    #if its basket count determine which whether the basket is largest so far and continue\n",
    "    if (product =='*BASKET'):\n",
    "        if(count > max_basket):\n",
    "            max_basket = count\n",
    "        continue\n",
    "    #if the data is about number of baskets increment the count and continue\n",
    "    elif (product =='*NUM_OF_BASKET'):\n",
    "        basket_cnt += count\n",
    "        continue\n",
    "    #else aggregate the product data\n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            if (last_key):\n",
    "                print '%s\\t%s' %(last_key,total_count)\n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if last_key == product:\n",
    "    print '%s\\t%s' %(last_key,total_count)\n",
    "\n",
    "if(max_basket != -1):\n",
    "    print '%s\\t%s' %('*BASKET',max_basket) \n",
    "print '%s\\t%s' %('*NUM_OF_BASKET',basket_cnt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 In this question, we will emit a counter for everytime the reducer is called. \n",
    "#here we have used heapq to store the 50 top products frequently purchased. \n",
    "\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,num_reducers,1\\n\")\n",
    "\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "n_max = 100 #top n products purchased\n",
    "unique_cnt = 0 #number of unique products \n",
    "max_queue =  [] # hold top n products \n",
    "num_baskets = 0\n",
    "\n",
    "#function to add to queue.\n",
    "#we have used heapq here so we can remove the smallest element if the queue is full during insertion.\n",
    "def add_to_max_Q(max_q,product,cnt):\n",
    "    # add elements if q is not full\n",
    "    if (len(max_q) < n_max):\n",
    "        heapq.heappush(max_q, (cnt, product))\n",
    "    else:\n",
    "        # add new element and then remove smallest element\n",
    "        heapq.heappush(max_q, (cnt, product))\n",
    "        heapq.heappop(max_q)\n",
    "\n",
    "#function to print the queue data\n",
    "def print_topn(max_q,total_bsk):\n",
    "    cnt = len(max_q)\n",
    "    if(cnt > n_max):\n",
    "        cnt = n_max\n",
    "    s = heapq.nlargest(cnt, max_q)\n",
    "    for row in s:\n",
    "        print '%s\\t%s\\t%s'%(row[1] ,row[0],(1.0*row[0]/total_bsk))\n",
    "    \n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = row[0]\n",
    "    count = int(row[1])\n",
    "    \n",
    "    #if data is about basket length, determine which is largest and continue\n",
    "    if (product =='*BASKET'):\n",
    "        if(count > max_basket):\n",
    "            max_basket = count\n",
    "        continue\n",
    "    #if data is about number of baskets , than increment total count\n",
    "    elif (product == '*NUM_OF_BASKET'):\n",
    "        num_baskets += count\n",
    "        continue\n",
    "    #aggregate products \n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            if (last_key):\n",
    "                #increment unique product count and add to our queue \n",
    "                unique_cnt += 1\n",
    "                add_to_max_Q(max_queue,last_key,total_count) \n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if last_key == product:\n",
    "    unique_cnt += 1\n",
    "    #uncomment below line if you want use heapq\n",
    "    add_to_max_Q(max_queue,last_key,total_count)\n",
    "\n",
    "    \n",
    "print '%s\\t%s\\t%s' %('*LARGEST_BASKET',max_basket,1)\n",
    "print '%s\\t%s\\t%s' %('*UNIQUE_CNT',unique_cnt,1)\n",
    "print_topn(max_queue,num_baskets)\n",
    "\n",
    "# print \"\\n Largest Basket : %s\" %(max_basket)\n",
    "# print \"\\n Number of unique products: %s\" %(unique_cnt)\n",
    "# print \"\\n Top 50 products (product, frequency, relative frequency )\\n \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 Writing a second map reduce job to do sorting on the output from the previous job\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3b,num_mapper,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    token = line.strip().split('\\t')\n",
    "    print '%s\\t%s\\t%s' %(token[1],token[0],token[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 Writing a second  reduce job to print the 50 top products \n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3b,num_mapper,1\\n\")\n",
    "\n",
    "print \"\\n Top 50 products (product, frequency, relative frequency )\\n \"\n",
    "counter = 0\n",
    "max_basket=0\n",
    "unique_cnt =0\n",
    "n_max=50\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    token = re.split(r'\\t+',line.strip())\n",
    "    \n",
    "    if(token[1]=='*LARGEST_BASKET'):\n",
    "        max_basket = token[0]\n",
    "        continue\n",
    "    elif(token[1]=='*UNIQUE_CNT'):\n",
    "        unique_cnt = token[0]\n",
    "        continue;\n",
    "    else:\n",
    "        if(counter<n_max):\n",
    "            print '%s\\t%s\\t%s'%(token[1] ,token[0],token[2]) \n",
    "            counter += 1\n",
    "\n",
    "print \"\\n Largest Basket : %s\" %(max_basket)\n",
    "print \"\\n Number of unique products: %s\" %(unique_cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 21:22:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw33b\n",
      "16/02/01 21:22:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw33\n",
      "16/02/01 21:22:07 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/src/ProductPurchaseData.txt\n",
      "16/02/01 21:22:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/02/01 21:22:10 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 21:22:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper1.py, combiner.py, reducer1.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar4314244195036496904/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob5323640183787201909.jar tmpDir=null\n",
      "16/02/01 21:22:28 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 21:22:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper2.py, reducer2.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar5533945427769730269/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob3927560617081616870.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/02/01 21:22:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      " Top 50 products (product, frequency, relative frequency )\t\n",
      " \t\n",
      "DAI62779\t6667\t0.214366097553\n",
      "FRO40251\t3881\t0.124786984341\n",
      "ELE17451\t3875\t0.1245940645\n",
      "GRO73461\t3602\t0.115816211697\n",
      "SNA80324\t3044\t0.0978746664094\n",
      "ELE32164\t2851\t0.0916690781647\n",
      "DAI75645\t2736\t0.0879714478634\n",
      "SNA45677\t2455\t0.0789363686055\n",
      "FRO31317\t2330\t0.0749172052346\n",
      "DAI85309\t2293\t0.0737275328768\n",
      "ELE26917\t2292\t0.0736953795698\n",
      "FRO80039\t2233\t0.0717983344587\n",
      "GRO21487\t2115\t0.0680042442365\n",
      "SNA99873\t2083\t0.0669753384136\n",
      "GRO59710\t2004\t0.0644352271631\n",
      "GRO71621\t1920\t0.0617343493778\n",
      "FRO85978\t1918\t0.0616700427639\n",
      "GRO30386\t1840\t0.0591620848204\n",
      "ELE74009\t1816\t0.0583904054532\n",
      "GRO56726\t1784\t0.0573614996302\n",
      "DAI63921\t1773\t0.0570078132536\n",
      "GRO46854\t1756\t0.0564612070351\n",
      "ELE66600\t1713\t0.0550786148355\n",
      "DAI83733\t1712\t0.0550464615286\n",
      "FRO32293\t1702\t0.0547249284589\n",
      "ELE66810\t1697\t0.0545641619241\n",
      "SNA55762\t1646\t0.0529243432687\n",
      "DAI22177\t1627\t0.0523134304363\n",
      "FRO78087\t1531\t0.0492267129674\n",
      "ELE99737\t1516\t0.0487444133629\n",
      "ELE34057\t1489\t0.0478762740748\n",
      "GRO94758\t1489\t0.0478762740748\n",
      "FRO35904\t1436\t0.0461721488055\n",
      "FRO53271\t1420\t0.045657695894\n",
      "SNA93860\t1407\t0.0452397029034\n",
      "SNA90094\t1390\t0.044693096685\n",
      "GRO38814\t1352\t0.0434712710202\n",
      "ELE56788\t1345\t0.0432461978715\n",
      "GRO61133\t1321\t0.0424745185042\n",
      "DAI88807\t1316\t0.0423137519694\n",
      "ELE74482\t1316\t0.0423137519694\n",
      "ELE59935\t1311\t0.0421529854346\n",
      "SNA96271\t1295\t0.0416385325231\n",
      "DAI43223\t1290\t0.0414777659882\n",
      "ELE91337\t1289\t0.0414456126813\n",
      "GRO15017\t1275\t0.0409954663837\n",
      "DAI31081\t1261\t0.0405453200862\n",
      "GRO81087\t1220\t0.0392270345005\n",
      "DAI22896\t1219\t0.0391948811935\n",
      "GRO85051\t1214\t0.0390341146587\n",
      "\t\n",
      " Largest Basket : 37\t\n",
      "\t\n",
      " Number of unique products: 12591\t\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_3():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper1.py;chmod a+x reducer1.py;chmod a+x mapper2.py;chmod a+x reducer2.py;chmod a+x combiner.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw33b\n",
    "    !hdfs dfs -rm -r hw3/output/hw33\n",
    "    \n",
    "    #move input file to hdfs \n",
    "    !hdfs dfs -rm hw3/src/ProductPurchaseData.txt\n",
    "    !hdfs dfs -put ProductPurchaseData.txt  hw3/src/\n",
    "    \n",
    "    #run hadoop job with mapper1 and reducer1\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -file mapper1.py \\\n",
    "    -mapper mapper1.py \\\n",
    "    -file combiner.py \\\n",
    "    -combiner combiner.py \\\n",
    "    -file reducer1.py \\\n",
    "    -reducer reducer1.py \\\n",
    "    -input hw3/src/ProductPurchaseData.txt \\\n",
    "    -output hw3/output/hw33\n",
    "    \n",
    "    \n",
    "    #run hadoop job with mapper2 and reducer 2\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2\" \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -mapper mapper2.py \\\n",
    "    -file mapper2.py \\\n",
    "    -reducer reducer2.py \\\n",
    "    -file reducer2.py \\\n",
    "    -input hw3/output/hw33/part* \\\n",
    "    -output hw3/output/hw33b\n",
    "    \n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw33b/part*\n",
    "\n",
    "hw3_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "#HW3.4 In this question, we will emit a counter for everytime the mapper is called. \n",
    "# In this mapper code, we are first taking unique products for each basket ( if there are duplicates ) \n",
    "# and than sorting it. We have used python function of combinations to emit all tuples of length 2.\n",
    "# We also emit for everyline read a count for the number of baskets \n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,num_mapper,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # emit count for basket\n",
    "    print '%s\\t%s\\t%s'%('*','BASKET',1)\n",
    "    \n",
    "    # remove leading and trailing whitespace and tokenize\n",
    "    token  =  line.strip().split(\" \")\n",
    "    for subset in itertools.combinations(sorted(set(token)), 2):\n",
    "        print '%s\\t%s\\t%s' % (subset[0],subset[1], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the combiner is called. \n",
    "# In this combiner code we aggregate the basket count and products that occur together.\n",
    "# This is done for intermediate aggregation\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,num_combiner,1\\n\")\n",
    "last_key = None\n",
    "total_count = 0\n",
    "basket_cnt = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = (row[0],row[1]) #combine product1 and product2 as tuple to be used as key\n",
    "    count = int(row[2])\n",
    "    #increment total number of baskets if incoming data is for baskets \n",
    "    if (product == ('*','BASKET')):\n",
    "        basket_cnt += count\n",
    "        continue\n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            if (last_key):\n",
    "                print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if last_key == product:\n",
    "    print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "\n",
    "print '%s\\t%s\\t%s' %('*','BASKET',basket_cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the reducer is called. \n",
    "# In this reducer code we aggregate the basket count and products that occur together.\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,num_reducers,1\\n\")\n",
    "last_key = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "basket_cnt = 0\n",
    "sup_cnt = 100 \n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = (row[0],row[1]) #combine product1 and product2 as tuple to be used as key\n",
    "    count = int(row[2])\n",
    "    #increment total number of baskets if incoming data is for baskets \n",
    "    if (product ==('*','BASKET')):\n",
    "        basket_cnt += count\n",
    "        continue\n",
    "    #increment the count for products that occur together\n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            #emit pair of product if they are greater than 100\n",
    "            if (last_key and total_count > sup_cnt):\n",
    "                print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if (last_key == product and total_count > sup_cnt ):\n",
    "    print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "#emit the total number of baskets\n",
    "print '%s\\t%s\\t%s' %('*','NUM_OF_BASKET',basket_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing second mapreduce job to determine top 50 product pairs which coccur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the mapper is called is called. \n",
    "# In the mapper code we will emit the frequency first followed by the product pair\n",
    "# we are also going to tell hadoop to sort the key as numeric and in reverse order\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4b,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    #remove leading and trailing spaces \n",
    "    row = re.split(r'\\t',line.strip())\n",
    "    print '%s\\t%s\\t%s' %(row[2],row[0],row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the reducer is called is called. \n",
    "# As we are sorting the data in descending order based on the frequency, we will get the number of baskets on top.\n",
    "# after that we enter the top 50 product pair in the list and print the record\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4b,num_reducers,1\\n\")\n",
    "\n",
    "n_max = 50\n",
    "a_max = []\n",
    "num_baskets =0 \n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    freq = int(row[0])\n",
    "    key = (row[1],row[2])\n",
    "    \n",
    "    if(key==('*','NUM_OF_BASKET')):\n",
    "        num_baskets = freq\n",
    "        continue\n",
    "    else:\n",
    "        # add the lowest 10 words \n",
    "        if len(a_max) < n_max:\n",
    "            a_max.append((key,freq,1.0*freq/num_baskets))\n",
    "        \n",
    "for record in a_max:\n",
    "    print record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 22:20:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "rm: `hw3/output/hw34b': No such file or directory\n",
      "16/02/01 22:20:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw34a\n",
      "16/02/01 22:20:22 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 22:20:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper1.py, reducer1.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar2899922116860889051/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob241122676465733855.jar tmpDir=null\n",
      "16/02/01 22:20:54 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 22:20:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper2.py, reducer2.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar5795836211272193457/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob3883355704360858287.jar tmpDir=null\n",
      "\n",
      " Time taken to run this section of code in seconds: \n",
      "59.0430419445\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/02/01 22:21:21 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "(('DAI62779', 'ELE17451'), 1592, 0.05118806469245362)\t\n",
      "(('FRO40251', 'SNA80324'), 1412, 0.04540046943828173)\t\n",
      "(('DAI75645', 'FRO40251'), 1254, 0.04032024693739751)\t\n",
      "(('FRO40251', 'GRO85051'), 1213, 0.039001961351725026)\t\n",
      "(('DAI62779', 'GRO73461'), 1139, 0.03662261663612103)\t\n",
      "(('DAI75645', 'SNA80324'), 1130, 0.03633323687341243)\t\n",
      "(('DAI62779', 'FRO40251'), 1070, 0.03440403845535513)\t\n",
      "(('DAI62779', 'SNA80324'), 923, 0.029677502331114755)\t\n",
      "(('DAI62779', 'DAI85309'), 918, 0.029516735796276648)\t\n",
      "(('ELE32164', 'GRO59710'), 911, 0.029291662647503297)\t\n",
      "(('DAI62779', 'DAI75645'), 882, 0.02835921674544227)\t\n",
      "(('FRO40251', 'GRO73461'), 882, 0.02835921674544227)\t\n",
      "(('DAI62779', 'ELE92920'), 877, 0.02819845021060416)\t\n",
      "(('FRO40251', 'FRO92469'), 835, 0.026848011317964052)\t\n",
      "(('DAI62779', 'ELE32164'), 832, 0.026751551397061188)\t\n",
      "(('DAI75645', 'GRO73461'), 712, 0.022893154560946594)\t\n",
      "(('DAI43223', 'ELE32164'), 711, 0.022861001253978972)\t\n",
      "(('DAI62779', 'GRO30386'), 709, 0.02279669464004373)\t\n",
      "(('ELE17451', 'FRO40251'), 697, 0.022410854956432268)\t\n",
      "(('DAI85309', 'ELE99737'), 659, 0.021189029291662647)\t\n",
      "(('DAI62779', 'ELE26917'), 650, 0.020899649528954053)\t\n",
      "(('GRO21487', 'GRO73461'), 631, 0.02028873669656924)\t\n",
      "(('DAI62779', 'SNA45677'), 604, 0.019420597408443457)\t\n",
      "(('ELE17451', 'SNA80324'), 597, 0.019195524259670107)\t\n",
      "(('DAI62779', 'GRO71621'), 595, 0.019131217645734864)\t\n",
      "(('DAI62779', 'SNA55762'), 593, 0.01906691103179962)\t\n",
      "(('DAI62779', 'DAI83733'), 586, 0.01884183788302627)\t\n",
      "(('ELE17451', 'GRO73461'), 580, 0.018648918041220538)\t\n",
      "(('GRO73461', 'SNA80324'), 562, 0.01807015851580335)\t\n",
      "(('DAI62779', 'GRO59710'), 561, 0.01803800520883573)\t\n",
      "(('DAI62779', 'FRO80039'), 550, 0.01768431883219189)\t\n",
      "(('DAI75645', 'ELE17451'), 547, 0.017587858911289025)\t\n",
      "(('DAI62779', 'SNA93860'), 537, 0.01726632584161281)\t\n",
      "(('DAI55148', 'DAI62779'), 526, 0.016912639464968973)\t\n",
      "(('DAI43223', 'GRO59710'), 512, 0.01646249316742227)\t\n",
      "(('ELE17451', 'ELE32164'), 511, 0.016430339860454647)\t\n",
      "(('DAI62779', 'SNA18336'), 506, 0.01626957332561654)\t\n",
      "(('ELE32164', 'GRO73461'), 486, 0.015626507186264106)\t\n",
      "(('DAI62779', 'FRO78087'), 482, 0.01549789395839362)\t\n",
      "(('DAI85309', 'ELE17451'), 482, 0.01549789395839362)\t\n",
      "(('DAI62779', 'GRO94758'), 479, 0.015401434037490756)\t\n",
      "(('DAI62779', 'GRO21487'), 471, 0.015144207581749784)\t\n",
      "(('GRO85051', 'SNA80324'), 471, 0.015144207581749784)\t\n",
      "(('ELE17451', 'GRO30386'), 468, 0.015047747660846917)\t\n",
      "(('FRO85978', 'SNA95666'), 463, 0.01488698112600881)\t\n",
      "(('DAI62779', 'FRO19221'), 462, 0.014854827819041188)\t\n",
      "(('DAI62779', 'GRO46854'), 461, 0.014822674512073567)\t\n",
      "(('DAI43223', 'DAI62779'), 459, 0.014758367898138324)\t\n",
      "(('ELE92920', 'SNA18336'), 455, 0.014629754670267838)\t\n",
      "(('DAI88079', 'FRO40251'), 446, 0.014340374907559243)\t\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "import time \n",
    "def hw3_4():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper1.py;chmod a+x reducer1.py;chmod a+x mapper2.py;chmod a+x reducer2.py;chmod a+x combiner.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw34b\n",
    "    !hdfs dfs -rm -r hw3/output/hw34a\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #run hadoop job with mapper1 and reducer1\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -file mapper1.py \\\n",
    "    -mapper mapper1.py \\\n",
    "    -file reducer1.py \\\n",
    "    -reducer reducer1.py \\\n",
    "    -input hw3/src/ProductPurchaseData.txt \\\n",
    "    -output hw3/output/hw34a\n",
    "    \n",
    "    #run hadoop job with mapper2 and reducer2\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2 -k3,3\" \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -mapper mapper2.py \\\n",
    "    -file mapper2.py \\\n",
    "    -reducer reducer2.py \\\n",
    "    -file reducer2.py \\\n",
    "    -input hw3/output/hw34a/part* \\\n",
    "    -output hw3/output/hw34b\n",
    "    \n",
    "    print\"\\n Time taken to run this section of code in seconds: \"\n",
    "    print time.time() - start\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw34b/part*\n",
    "\n",
    "hw3_4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational setup used \n",
    "\n",
    "1. Single node haddop cluster , running hadoop version 2.7.1. This was setup on my mac (intel i7, 4 cores)\n",
    "2. Yarn is enabled and all mapreduce jobs are running in yarn mode\n",
    "3. Yarn configured with a single node manager / slave. This node manager has 8GB capacity. Yarn vcore /cpu support was not enabled \n",
    "4. Each map task is configured to use 1 GB resource (mapreduce.map.memory.mb = 1024, default value)\n",
    "5. Each reduce task is configured to use 1 GB resource (mapreduce.reduce.memory.mb = 1024, default value)\n",
    "6. Given this, it means that my yarn cluster could run at max 6 maps/reduce tasks concurrently (yarn.app.mapreduce.am.resource.mb = 1536, MR application master is using 2GB)  \n",
    "\n",
    "Total time taken = 59.0430419445 secs\n",
    "\n",
    "From Job a : \n",
    "\n",
    "Number of mapper = 2\n",
    "\n",
    "Number of reducers = 1\n",
    "\n",
    "Number of combiners = 0\n",
    "\n",
    "From Job b : \n",
    "\n",
    "Number of mapper = 2\n",
    "\n",
    "Number of reducers = 1\n",
    "\n",
    "Number of combiners = 0\n",
    "\n",
    "Image from Job a\n",
    "<img src=\"./images/hw34a.jpg\">\n",
    "\n",
    "Image from Job b\n",
    "<img src=\"./images/hw34b.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5a,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Emit * , BAKSET for counting total number of baskets \n",
    "    print '%s\\t%s\\t%s'%('*','BASKET',1)\n",
    "    # remove leading and trailing whitespace and tokenize\n",
    "    token  =  line.strip().split(\" \")\n",
    "    uniq_tokens = sorted(set(token))\n",
    "    for i in range (0,len(uniq_tokens)-1):\n",
    "        key = uniq_tokens[i]\n",
    "        value = \"\"\n",
    "        for j in range(i+1, len(uniq_tokens)):\n",
    "            value += uniq_tokens[j] + \"\\t1\\t\"\n",
    "        print '%s\\t%s' %(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "#H3.5 This reducer functions gets the total number of baskets as the first element from mapper due to \n",
    "# which we are able to use it to calculate relative frequncy. I have used heapq to insert top 100 products. \n",
    "# This code was written before we got instructions to use 2 map reduce jobs. I have not removed this piece of code. \n",
    "# Given that we need to sort data based  lexicographically the 2nd map reduce job helps with it.\n",
    "\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "import operator\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5a,num_reducers,1\\n\")\n",
    "\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "basket_cnt = 0 \n",
    "sup_cnt = 100 # variable to hold the minimum support count \n",
    "tmp_dict ={} # for aggregation\n",
    "n_max = 50 # top n product pairs \n",
    "max_q = []\n",
    "\n",
    "#function to add to queue.\n",
    "#we have used heapq here so we can remove the smallest element if the queue is full during insertion.\n",
    "def add_to_max_Q(p1,p2,cnt):\n",
    "    # add elements if q is not full\n",
    "    if (len(max_q) < n_max):\n",
    "        heapq.heappush(max_q, (cnt, (p1,p2)))\n",
    "    else:\n",
    "        # add new element and then remove smallest element\n",
    "        heapq.heappush(max_q, (cnt, (p1,p2)))\n",
    "        heapq.heappop(max_q)\n",
    "    \n",
    "#function to print the queue data\n",
    "def print_topn(total_bsk):\n",
    "    cnt = len(max_q)\n",
    "    if(cnt > n_max):\n",
    "        cnt = n_max\n",
    "    s = heapq.nlargest(cnt, max_q)\n",
    "    for row in s:\n",
    "        print '%s\\t%s\\t%s\\t%s'%(row[1][0] ,row[1][1],row[0],(1.0*row[0]/total_bsk))\n",
    "\n",
    "def merge_dict(merged,row):\n",
    "    for product in row:\n",
    "        if product in merged:\n",
    "            merged[product] += row[product]\n",
    "        else:\n",
    "            merged[product] = row[product]\n",
    "    return merged\n",
    "\n",
    "def emit_dict(key,merged):\n",
    "    output = key\n",
    "    cnt =0\n",
    "    merged_sorted = sorted(merged.items(), key=operator.itemgetter(0))\n",
    "    for tup in merged_sorted:\n",
    "        # for prd, value in merged_sort.iteritems():\n",
    "        prd = tup[0]\n",
    "        value = tup[1]\n",
    "        if(value < sup_cnt):\n",
    "            continue\n",
    "        cnt += 1\n",
    "        add_to_max_Q(key,prd,value)\n",
    "        output += \"\\t\" + prd + \"\\t\" + str(value)\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t',line.strip())\n",
    "    product = row[0]\n",
    "    #increment the total basket if we encounter * BASKET tuple\n",
    "    if(row[0]=='*' and row[1]=='BASKET'):\n",
    "        basket_cnt += int(row[2])\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    #increment the count for products that occur together. \n",
    "    else:\n",
    "        row_dict ={}\n",
    "        #parse the remaining data elements and increment counters by 2\n",
    "        for x in range (1,len(row),2):\n",
    "            prd = row[x]\n",
    "            prd_cnt = int(row[x+1])\n",
    "            row_dict[prd]=prd_cnt\n",
    "        \n",
    "        if(last_key == product):\n",
    "            #update dictionary                \n",
    "            tmp_dict=merge_dict(tmp_dict,row_dict)\n",
    "\n",
    "        #emit the data is we find new product key \n",
    "        else:\n",
    "            #emit pair of product if they are greater than 100\n",
    "            if (last_key):\n",
    "                #print dictionary\n",
    "                emit_dict(last_key,tmp_dict)\n",
    "            #create dictonary\n",
    "            tmp_dict=row_dict\n",
    "            last_key = product\n",
    "                        \n",
    "            \n",
    "if (last_key == product):\n",
    "    #print dictionary\n",
    "    emit_dict(last_key,tmp_dict)\n",
    "    \n",
    "#emit the total number of baskets\n",
    "print '%s\\t%s\\t%s\\t%s' %('*','NUM_OF_BASKET',basket_cnt,1)     \n",
    "# \"\\n %d Top product pairs that occur together\" %(n_max)\n",
    "print_topn(basket_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.5 In this question, we will emit a counter for everytime the mapper is called is called. \n",
    "# In the mapper code we will emit the same data out as from previous output\n",
    "# we are also going to tell hadoop to sort the key as numeric and in reverse order\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5b,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    #remove leading and trailing spaces \n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    print '%s\\t%s\\t%s\\t%s' %(row[0],row[1],row[2],row[3])\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/02/01 21:35:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw35b\n",
      "16/02/01 21:35:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw35a\n",
      "16/02/01 21:35:29 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 21:35:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper1.py, reducer1.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar6811419751842694770/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob2656764134585733354.jar tmpDir=null\n",
      "16/02/01 21:35:51 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/02/01 21:35:51 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper2.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar8493040502893937842/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob823579040201425842.jar tmpDir=null\n",
      "\n",
      " Time taken to run this section of code in seconds: \n",
      "41.5408499241\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/02/01 21:36:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "*\tNUM_OF_BASKET\t31101\t1\n",
      "DAI62779\tELE17451\t1592\t0.0511880646925\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\n",
      "DAI62779\tSNA80324\t923\t0.0296775023311\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\n",
      "DAI43223\tELE32164\t711\t0.022861001254\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\n",
      "DAI62779\tELE26917\t650\t0.020899649529\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\n",
      "DAI62779\tSNA45677\t604\t0.0194205974084\n",
      "ELE17451\tSNA80324\t597\t0.0191955242597\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\n",
      "DAI62779\tSNA55762\t593\t0.0190669110318\n",
      "DAI62779\tDAI83733\t586\t0.018841837883\n",
      "ELE17451\tGRO73461\t580\t0.0186489180412\n",
      "GRO73461\tSNA80324\t562\t0.0180701585158\n",
      "DAI62779\tGRO59710\t561\t0.0180380052088\n",
      "DAI62779\tFRO80039\t550\t0.0176843188322\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\n",
      "DAI62779\tSNA93860\t537\t0.0172663258416\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\n",
      "ELE17451\tELE32164\t511\t0.0164303398605\n",
      "DAI62779\tSNA18336\t506\t0.0162695733256\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\n",
      "DAI62779\tFRO78087\t482\t0.0154978939584\n",
      "DAI85309\tELE17451\t482\t0.0154978939584\n",
      "DAI62779\tGRO94758\t479\t0.0154014340375\n",
      "DAI62779\tGRO21487\t471\t0.0151442075817\n",
      "GRO85051\tSNA80324\t471\t0.0151442075817\n",
      "ELE17451\tGRO30386\t468\t0.0150477476608\n",
      "FRO85978\tSNA95666\t463\t0.014886981126\n",
      "DAI62779\tFRO19221\t462\t0.014854827819\n",
      "DAI62779\tGRO46854\t461\t0.0148226745121\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\n",
      "ELE92920\tSNA18336\t455\t0.0146297546703\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "import time \n",
    "def hw3_5_b():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper1.py;chmod a+x reducer1.py;chmod a+x mapper2.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw35b\n",
    "    !hdfs dfs -rm -r hw3/output/hw35a\n",
    "    \n",
    "    start = time.time()\n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -file mapper1.py \\\n",
    "    -mapper mapper1.py \\\n",
    "    -file reducer1.py \\\n",
    "    -reducer reducer1.py \\\n",
    "    -input hw3/src/ProductPurchaseData.txt \\\n",
    "    -output hw3/output/hw35a\n",
    "    \n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k3,3nr -k1,1 -k2,2\" \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -mapper mapper2.py \\\n",
    "    -file mapper2.py \\\n",
    "    -reducer mapper2.py \\\n",
    "    -input hw3/output/hw35a/part* \\\n",
    "    -output hw3/output/hw35b\n",
    "    \n",
    "    print\"\\n Time taken to run this section of code in seconds: \"\n",
    "    print time.time() - start\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw35b/part*\n",
    "\n",
    "hw3_5_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational setup used \n",
    "\n",
    "1. Single node haddop cluster , running hadoop version 2.7.1. This was setup on my mac (intel i7, 4 cores)\n",
    "2. Yarn is enabled and all mapreduce jobs are running in yarn mode\n",
    "3. Yarn configured with a single node manager / slave. This node manager has 8GB capacity. Yarn vcore /cpu support was not enabled \n",
    "4. Each map task is configured to use 1 GB resource (mapreduce.map.memory.mb = 1024, default value)\n",
    "5. Each reduce task is configured to use 1 GB resource (mapreduce.reduce.memory.mb = 1024, default value)\n",
    "6. Given this, it means that my yarn cluster could run at max 6 maps/reduce tasks concurrently (yarn.app.mapreduce.am.resource.mb = 1536, MR application master is using 2GB)  \n",
    "\n",
    "Total time taken = \n",
    "\n",
    "From Job a : \n",
    "\n",
    "a.Time Taken : 23.9189631939 secs\n",
    "\n",
    "b. Number of mapper = 2\n",
    "\n",
    "c. Number of reducers = 1\n",
    "\n",
    "d. Number of combiners = 0\n",
    "\n",
    "From Job b : \n",
    "\n",
    "a. Time Taken : 17.4585750103 secs\n",
    "\n",
    "b. Number of mapper = 2\n",
    "\n",
    "c. Number of reducers = 1\n",
    "\n",
    "d. Number of combiners = 0\n",
    "\n",
    "Image from Job a\n",
    "<img src=\"./images/hw35a.jpg\">\n",
    "\n",
    "Image from Job b\n",
    "<img src=\"./images/hw35b.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
