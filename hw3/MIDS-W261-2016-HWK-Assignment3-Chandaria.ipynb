{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #3=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "DATSCIW261 ASSIGNMENT #3\n",
    "\n",
    "Version 2016-01-27 (FINAL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hetal Chandaria (hetalchandaria@berkeley.edu)\n",
    "W261 - 2 , ASSIGNMENT #3\n",
    "\n",
    "Submission Date : add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.0.\n",
    "What is a merge sort? Where is it used in Hadoop?\n",
    "\n",
    "How is  a combiner function in the context of Hadoop?  Give an example where it can be used and justify why it should be used in the context of this problem.\n",
    "\n",
    "What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:blue;\">Answer</span>\n",
    "\n",
    "#### Merge Sort \n",
    "\n",
    "Merge sort is a sort algorithm that sorts a dataset by divide-and-conquer. The data set is split continuously into sub-sets until each sub-set is size of 1 and therefore considered automatically sorted. Now, sub-sets are merged together to form a final fully sorted data-set. Given n sorted-subsets, the complexity to merge these sorted sub-sets is proportional to the total no. of elements in all the sub-sets combined.\n",
    "\n",
    "Hadoop can be considered in some sense an implementation of merge sort as it first partitions the dataset into multiple map tasks, sorts each map output and eventually does a \"merge\" of each map's sorted output in the reducer. A lot of the heavy lifting of this implementation is done in the shuffle layer. \n",
    "\n",
    "#### Combiner Function\n",
    "\n",
    "Given that data comes to the mapper in no defined order, the mapper output can be quite verbose as it can potentially not make any optimizations. The combiner function is run on mapper output and can \"coalesce/combine\" the mapper output so that the verbose map output does not fully need to be sent to the reducer. The combiner pretty much does the same work as that of the reducer. It has to process data in the format of the mapper output but unlike the reducer, it also generate output that is consumable by the reducer. \n",
    "\n",
    "Let us consider a job that accepts the following input (word, priority) and the aim is to generate the avg. priority for each word. For a job without a combiner, the mapper could generate the following output (word, priority). This would be then averaged upon by the reducer. However, let's say that the word \"foo\" occurs a million times in the same mapper. In this scenario, without a combiner, 1 million rows will be sent to the reducer. A combiner if run could potentially optimize this into an key-value output such as (word, (sum(priority), frequency of word)). Using this tuple, the reducer can then calculate the avg. priority. Note, that the mapper output would also change to (word, (priority, 1)).\n",
    "\n",
    "Not all operations support a combiner. Only commutative and associative operations can work as the output of the reducer has to be the same regardless of whether the combiner ran or not. \n",
    "\n",
    "#### Hadoop shuffle\n",
    "\n",
    "The hadoop shuffle is the layer that transfers the data from the mappers to the reducers. Consider 4 maps and 2 reducers. Each mapper will generate data in 2 partitions considering that there are 2 reducers. This is governed by the partitioner function which can be overridden by the user. The shuffle layer will try to efficiently transfer the data from each mapper to a particular reducer, ensure that the data is sorted correctly by the configured keys ( by defining an approrpiate comparator used to compare keys when sorting ) and that data for a given key is provided in a sorted/grouped manner to the reducer. This is what enables us to write simple reducers without requiring us to do a lot of in-memory buffering. For example, consider the input data of format (word, priority) where the aim is to find max(priority) for each word. If the words came to the reducer in a random order, the reducer would need to keep a lookup table to track each word and its max priority given that there is no information on whether the word may or may not show up later. Given the sorted nature of input to the reducer, the simplistic reducer knows that as soon as a new word is seen, the previous word will never again show up and the max priority can then be generated for that word. This means no need to have a large lookup table for each seen word but rather just one object to track the data for the current word and its max priority.\n",
    "\n",
    "The diagram below depicts the various components of hadoop shuffle. \n",
    "\n",
    "<img src=\"./images/shuffle.jpg\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#### Create directories in HDFS for HW3\n",
    "!hdfs dfs -mkdir hw3\n",
    "!hdfs dfs -mkdir hw3/src\n",
    "!hdfs dfs -mkdir hw3/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.1 \n",
    "Use Counters to do EDA (exploratory data analysis and to monitor progress)\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "Here’s is the first few lines of the  of the Consumer Complaints  Dataset:\n",
    "\n",
    "Complaint ID,Product,Sub-product,Issue,Sub-issue,State,ZIP code,Submitted via,Date received,Date sent to company,Company,Company response,Timely response?,Consumer disputed?\n",
    "\n",
    "1114245,Debt collection,Medical,Disclosure verification of debt,Not given enough info to verify debt,FL,32219,Web,11/13/2014,11/13/2014,\"Choice Recovery, Inc.\",Closed with explanation,Yes,\n",
    "\n",
    "1114488,Debt collection,Medical,Disclosure verification of debt,Right to dispute notice not received,TX,75006,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "1114255,Bank account or service,Checking account,Deposits and withdrawals,,NY,11102,Web,11/13/2014,11/13/2014,\"FNIS (Fidelity National Information Services, Inc.)\",In progress,Yes,\n",
    "\n",
    "1115106,Debt collection,\"Other (phone, health club, etc.)\",Communication tactics,Frequent or repeated calls,GA,31721,Web,11/13/2014,11/13/2014,\"Expert Global Solutions, Inc.\",In progress,Yes,\n",
    "\n",
    "\n",
    "User-defined Counters\n",
    "\n",
    "Now, let’s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.1 In this question ,we will only have a mapper which emit counters based on the type of complaint.\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    key = None\n",
    "    if(token[1]=='Debt collection'):\n",
    "        sys.stderr.write(\"reporter:counter:Debt-counter,debt,1\\n\")\n",
    "    elif(token[1]=='Mortgage'):\n",
    "        sys.stderr.write(\"reporter:counter:Mortgage-counter,mortgage,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:Other-counter,other,1\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 16:18:31 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw31\n",
      "16/01/31 16:18:36 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/src/Consumer_Complaints_mod.csv\n",
      "16/01/31 16:18:37 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 16:18:39 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 16:18:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar6525908040360619174/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob381896063830971794.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_1():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw31\n",
    "    \n",
    "    #remove header line from Consumer_Complaints.csv\n",
    "    !tail -n $(($(wc -l < Consumer_Complaints.csv) - 1))  < Consumer_Complaints.csv > Consumer_Complaints_mod.csv\n",
    "    \n",
    "    #move input file to hdfs \n",
    "    !hdfs dfs -rm hw3/src/Consumer_Complaints_mod.csv\n",
    "    !hdfs dfs -put Consumer_Complaints_mod.csv  hw3/src/\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.reduce.tasks=0 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -input hw3/src/Consumer_Complaints_mod.csv \\\n",
    "    -output hw3/output/hw31 \\\n",
    "\n",
    "hw3_1()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of the above from job tracker\n",
    "\n",
    "<img src=\"./images/hw31.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters\n",
    "\n",
    "For this brief study the Input file will be one record (the next line only): \n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers).\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. \n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2a In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is word and 1\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32a,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    for token in line.strip().split(\" \"):\n",
    "        print '%s,%s' % (token, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2a In this question, we will emit a counter for everytime the reducer is called. \n",
    "\n",
    "import sys\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32a,num_reducers,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split(',', 1)\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '%s\\t%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "if last_key == word:\n",
    "    print '%s\\t%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 16:22:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw32a\n",
      "16/01/31 16:22:46 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/src/hw32_a_input.txt\n",
      "16/01/31 16:22:47 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 16:22:48 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 16:22:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar3923939505159964458/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob6363776760723693083.jar tmpDir=null\n",
      "\n",
      "\n",
      "Word and its frequency\n",
      "16/01/31 16:23:09 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "labs\t1\n",
      "quux\t2\n",
      "foo\t3\n",
      "bar\t1\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_a():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #create inpur file and move it to hdfs \n",
    "    !echo \"foo foo quux labs foo bar quux\" > hw32_a_input.txt \n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32a\n",
    "    \n",
    "    #move input file to hdfs \n",
    "    !hdfs dfs -rm hw3/src/hw32_a_input.txt\n",
    "    !hdfs dfs -put hw32_a_input.txt  hw3/src/\n",
    "    \n",
    "    # run map reduce job. Here we have explicitly set the number of mappers and reducers\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.map.tasks=1 \\\n",
    "    -Dmapred.reduce.tasks=4 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/hw32_a_input.txt \\\n",
    "    -output hw3/output/hw32a\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"Word and its frequency\"\n",
    "    !hdfs dfs -cat hw3/output/hw32a/part*\n",
    "\n",
    "hw3_2_a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output for first part\n",
    "\n",
    "<img src=\"./images/hw32a.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.b\n",
    "\n",
    "Please use mulitple mappers and reducers for these jobs (at least 2 mappers and 2 reducers). Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2b In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is issue which is token[3] and 1\n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32b,num_mappers,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    #emit only 3rd token as it contains text about issue\n",
    "    print '\"%s\",%s' %(token[3],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2b In this question, we will emit a counter for everytime the reducer is called. \n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32b,num_reducers,1\\n\")\n",
    "last_key = None #user for keeping track of which key is currently being aggregated\n",
    "word = None\n",
    "total_count = 0 #counter to increment frequency\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    word=token[0]\n",
    "    count = int(token[1])\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '%s\\t%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "if last_key == word:\n",
    "    print '%s\\t%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 16:26:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw32b\n",
      "16/01/31 16:26:45 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 16:26:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar6961344658254863619/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob5728141958360488655.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/01/31 16:27:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "APR or interest rate\t3431\n",
      "Account opening, closing, or management\t16205\n",
      "Account terms and changes\t350\n",
      "Application processing delay\t243\n",
      "Applied for loan/did not receive money\t139\n",
      "Billing disputes\t6938\n",
      "Billing statement\t1220\n",
      "Can't repay my loan\t1647\n",
      "Can't stop charges to bank account\t131\n",
      "Charged fees or interest I didn't expect\t807\n",
      "Collection practices\t1003\n",
      "Cont'd attempts collect debt not owed\t17972\n",
      "Convenience checks\t75\n",
      "Credit decision / Underwriting\t2774\n",
      "Credit determination\t1490\n",
      "Credit line increase/decrease\t1149\n",
      "Credit reporting company's investigation\t4858\n",
      "Dealing with my lender or servicer\t1944\n",
      "Delinquent account\t1061\n",
      "Deposits and withdrawals\t10555\n",
      "Disclosure verification of debt\t7655\n",
      "Forbearance / Workout plans\t350\n",
      "Improper contact or sharing of info\t3489\n",
      "Late fee\t1797\n",
      "Managing the loan or lease\t4560\n",
      "Money was not available when promised\t274\n",
      "Other fee\t1075\n",
      "Other transaction issues\t387\n",
      "Other\t6273\n",
      "Payoff process\t1155\n",
      "Privacy\t240\n",
      "Received a loan I didn't apply for\t118\n",
      "Repaying your loan\t3844\n",
      "Rewards\t1002\n",
      "Shopping for a line of credit\t137\n",
      "Taking out the loan or lease\t1242\n",
      "Unsolicited issuance of credit card\t640\n",
      "Using a debit or ATM card\t2422\n",
      "Advertising and marketing\t1193\n",
      "Application, originator, mortgage broker\t8625\n",
      "Arbitration\t168\n",
      "Balance transfer fee\t95\n",
      "Balance transfer\t502\n",
      "Bankruptcy\t222\n",
      "Can't contact lender\t221\n",
      "Cash advance fee\t104\n",
      "Cash advance\t136\n",
      "Charged bank acct wrong day or amt\t71\n",
      "Closing/Cancelling account\t2795\n",
      "Collection debt dispute\t904\n",
      "Communication tactics\t8671\n",
      "Credit card protection / Debt protection\t1343\n",
      "Credit monitoring or identity protection\t1453\n",
      "Credit reporting\t1701\n",
      "Customer service / Customer relations\t1367\n",
      "False statements or representation\t3621\n",
      "Fraud or scam\t566\n",
      "Getting a loan\t291\n",
      "Identity theft / Fraud / Embezzlement\t3276\n",
      "Improper use of my credit report\t1477\n",
      "Incorrect information on credit report\t29069\n",
      "Incorrect/missing disclosures or info\t64\n",
      "Loan modification,collection,foreclosure\t70487\n",
      "Loan servicing, payments, escrow account\t36767\n",
      "Making/receiving payments, sending money\t3226\n",
      "Managing the line of credit\t446\n",
      "Other service issues\t151\n",
      "Overlimit fee\t127\n",
      "Payment to acct not credited\t92\n",
      "Problems caused by my funds being low\t5663\n",
      "Problems when you are unable to pay\t3821\n",
      "Sale of account\t139\n",
      "Settlement process and costs\t4350\n",
      "Shopping for a loan or lease\t535\n",
      "Taking/threatening an illegal action\t2964\n",
      "Transaction issue\t1098\n",
      "Unable to get credit report/credit score\t4357\n",
      "Wrong amount charged or received\t98\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_b():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32b\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.map.tasks=2 \\\n",
    "    -Dmapred.reduce.tasks=2 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/Consumer_Complaints_mod.csv \\\n",
    "    -output hw3/output/hw32b\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw32b/part*\n",
    "\n",
    "hw3_2_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output for 32b of custome counters\n",
    "<img src=\"./images/hw32b.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2.c\n",
    "\n",
    "Perform a word count analysis of the Issue column of the Consumer Complaints Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is issue which is token[3] and 1\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32c,num_mappers,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    #emit issue related token\n",
    "    print '\"%s\",%s' %(token[3],1)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c In this question, we will emit a counter for everytime the combiner is called. \n",
    "#the combiner will do intermediate aggregation of data and is similar to reducer in terms of logic\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32c,num_combiners,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    word=token[0]\n",
    "    count = int(token[1])\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '\"%s\",%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "if last_key == word:\n",
    "    print '\"%s\",%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c In this question, we will emit a counter for everytime the reducer is called. \n",
    "\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW_32c,num_reducers,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    word=token[0]\n",
    "    count = int(token[1])\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '\"%s\",%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "# do not forget to output the last word\n",
    "if last_key == word:\n",
    "    print '\"%s\",%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 16:32:52 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw32c\n",
      "16/01/31 16:32:53 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 16:32:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar1327447845199348294/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob3379068482288296690.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/01/31 16:33:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\"APR or interest rate\",3431\t\n",
      "\"Account opening, closing, or management\",16205\t\n",
      "\"Account terms and changes\",350\t\n",
      "\"Application processing delay\",243\t\n",
      "\"Applied for loan/did not receive money\",139\t\n",
      "\"Billing disputes\",6938\t\n",
      "\"Billing statement\",1220\t\n",
      "\"Can't repay my loan\",1647\t\n",
      "\"Can't stop charges to bank account\",131\t\n",
      "\"Charged fees or interest I didn't expect\",807\t\n",
      "\"Collection practices\",1003\t\n",
      "\"Cont'd attempts collect debt not owed\",17972\t\n",
      "\"Convenience checks\",75\t\n",
      "\"Credit decision / Underwriting\",2774\t\n",
      "\"Credit determination\",1490\t\n",
      "\"Credit line increase/decrease\",1149\t\n",
      "\"Credit reporting company's investigation\",4858\t\n",
      "\"Dealing with my lender or servicer\",1944\t\n",
      "\"Delinquent account\",1061\t\n",
      "\"Deposits and withdrawals\",10555\t\n",
      "\"Disclosure verification of debt\",7655\t\n",
      "\"Forbearance / Workout plans\",350\t\n",
      "\"Improper contact or sharing of info\",3489\t\n",
      "\"Late fee\",1797\t\n",
      "\"Managing the loan or lease\",4560\t\n",
      "\"Money was not available when promised\",274\t\n",
      "\"Other fee\",1075\t\n",
      "\"Other transaction issues\",387\t\n",
      "\"Other\",6273\t\n",
      "\"Payoff process\",1155\t\n",
      "\"Privacy\",240\t\n",
      "\"Received a loan I didn't apply for\",118\t\n",
      "\"Repaying your loan\",3844\t\n",
      "\"Rewards\",1002\t\n",
      "\"Shopping for a line of credit\",137\t\n",
      "\"Taking out the loan or lease\",1242\t\n",
      "\"Unsolicited issuance of credit card\",640\t\n",
      "\"Using a debit or ATM card\",2422\t\n",
      "\"Advertising and marketing\",1193\t\n",
      "\"Application, originator, mortgage broker\",8625\t\n",
      "\"Arbitration\",168\t\n",
      "\"Balance transfer fee\",95\t\n",
      "\"Balance transfer\",502\t\n",
      "\"Bankruptcy\",222\t\n",
      "\"Can't contact lender\",221\t\n",
      "\"Cash advance fee\",104\t\n",
      "\"Cash advance\",136\t\n",
      "\"Charged bank acct wrong day or amt\",71\t\n",
      "\"Closing/Cancelling account\",2795\t\n",
      "\"Collection debt dispute\",904\t\n",
      "\"Communication tactics\",8671\t\n",
      "\"Credit card protection / Debt protection\",1343\t\n",
      "\"Credit monitoring or identity protection\",1453\t\n",
      "\"Credit reporting\",1701\t\n",
      "\"Customer service / Customer relations\",1367\t\n",
      "\"False statements or representation\",3621\t\n",
      "\"Fraud or scam\",566\t\n",
      "\"Getting a loan\",291\t\n",
      "\"Identity theft / Fraud / Embezzlement\",3276\t\n",
      "\"Improper use of my credit report\",1477\t\n",
      "\"Incorrect information on credit report\",29069\t\n",
      "\"Incorrect/missing disclosures or info\",64\t\n",
      "\"Loan modification,collection,foreclosure\",70487\t\n",
      "\"Loan servicing, payments, escrow account\",36767\t\n",
      "\"Making/receiving payments, sending money\",3226\t\n",
      "\"Managing the line of credit\",446\t\n",
      "\"Other service issues\",151\t\n",
      "\"Overlimit fee\",127\t\n",
      "\"Payment to acct not credited\",92\t\n",
      "\"Problems caused by my funds being low\",5663\t\n",
      "\"Problems when you are unable to pay\",3821\t\n",
      "\"Sale of account\",139\t\n",
      "\"Settlement process and costs\",4350\t\n",
      "\"Shopping for a loan or lease\",535\t\n",
      "\"Taking/threatening an illegal action\",2964\t\n",
      "\"Transaction issue\",1098\t\n",
      "\"Unable to get credit report/credit score\",4357\t\n",
      "\"Wrong amount charged or received\",98\t\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_c():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32c\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -Dmapred.map.tasks=4 \\\n",
    "    -Dmapred.reduce.tasks=2 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file combiner.py \\\n",
    "    -combiner combiner.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/Consumer_Complaints_mod.csv \\\n",
    "    -output hw3/output/hw32c\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw32c/part*\n",
    "\n",
    "hw3_2_c()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output of mapper, recucer and combiner counters\n",
    "<img src=\"./images/hw32c.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  part 2 of 3_2_c\n",
    "Using a single reducer: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c_1 In this question, we will emit a counter for everytime the mapper is called. \n",
    "# output of mapper is frequency and issue \n",
    "# output of previous mapreduce job is used as input for this map reduce task\n",
    "import sys\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Mapper-counter,num_mappers,1\\n\")\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for token in reader(sys.stdin):\n",
    "    print '%s\\t\"%s\"' %(token[1],token[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.2c_1 In this question, we will emit a counter for everytime the reducer is called. \n",
    "# In this case of mapreduce we are using secondary sort where in we first sort on the frequency (ascending) \n",
    "# and than sort on the issue in case of tie\n",
    "\n",
    "import sys, Queue\n",
    "import re\n",
    "from csv import reader\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:Reducer-counter,num_reducers,1\\n\")\n",
    "\n",
    "n_max = 50 # top n issues \n",
    "n_min = 10 # bottom n issues \n",
    "a_min = [] # list to hold the bototom issues \n",
    "q_max = Queue.Queue(n_max) # Queue to hold the top 50 issues\n",
    "total_count =0 \n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    freq = int(row[0])\n",
    "    key = row[1]\n",
    "    #increment the count\n",
    "    total_count += freq\n",
    "    \n",
    "    # add the lowest 10 words as we are getting data in sorted order already\n",
    "    if len(a_min) < n_min:\n",
    "        a_min.append((key,freq))\n",
    "    \n",
    "    #Now add the top 10 words . In this case we use queue as its FIFO which will always pop the lowest element\n",
    "    if q_max.full():\n",
    "        q_max.get()\n",
    "    q_max.put((key,freq))\n",
    "    \n",
    "print '\\n Output = Word , Frequency and Relative Frequency'\n",
    "\n",
    "print '\\n%d Bottom issues:' %n_min\n",
    "for record in a_min:\n",
    "    print record[0] +\"\\t\"+str(record[1])+\"\\t\"+str(1.0*record[1]/total_count)\n",
    "\n",
    "print '\\n%d Top issues:' %n_max\n",
    "for i in range(n_max):\n",
    "    record = q_max.get()\n",
    "    print record[0] +\"\\t\"+str(record[1])+\"\\t\"+str(1.0*record[1]/total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 16:40:02 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw32c_2\n",
      "16/01/31 16:40:04 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 16:40:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar8680464347089963391/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob7457856801864801774.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/01/31 16:40:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      " Output = Word , Frequency and Relative Frequency\t\n",
      "\t\n",
      "10 Bottom issues:\t\n",
      "\"Incorrect/missing disclosures or info\"\t64\t0.000204532961765\n",
      "\"Charged bank acct wrong day or amt\"\t71\t0.000226903754458\n",
      "\"Convenience checks\"\t75\t0.000239687064568\n",
      "\"Payment to acct not credited\"\t92\t0.000294016132537\n",
      "\"Balance transfer fee\"\t95\t0.00030360361512\n",
      "\"Wrong amount charged or received\"\t98\t0.000313191097703\n",
      "\"Cash advance fee\"\t104\t0.000332366062868\n",
      "\"Received a loan I didn't apply for\"\t118\t0.000377107648254\n",
      "\"Overlimit fee\"\t127\t0.000405870096003\n",
      "\"Can't stop charges to bank account\"\t131\t0.000418653406113\n",
      "\t\n",
      "50 Top issues:\t\n",
      "\"Fraud or scam\"\t566\t0.00180883838061\n",
      "\"Unsolicited issuance of credit card\"\t640\t0.00204532961765\n",
      "\"Charged fees or interest I didn't expect\"\t807\t0.00257903281476\n",
      "\"Collection debt dispute\"\t904\t0.00288902808493\n",
      "\"Rewards\"\t1002\t0.00320221918264\n",
      "\"Collection practices\"\t1003\t0.00320541501016\n",
      "\"Delinquent account\"\t1061\t0.00339077300676\n",
      "\"Other fee\"\t1075\t0.00343551459215\n",
      "\"Transaction issue\"\t1098\t0.00350901862528\n",
      "\"Credit line increase/decrease\"\t1149\t0.00367200582919\n",
      "\"Payoff process\"\t1155\t0.00369118079435\n",
      "\"Advertising and marketing\"\t1193\t0.0038126222404\n",
      "\"Billing statement\"\t1220\t0.00389890958365\n",
      "\"Taking out the loan or lease\"\t1242\t0.00396921778925\n",
      "\"Credit card protection / Debt protection\"\t1343\t0.00429199636954\n",
      "\"Customer service / Customer relations\"\t1367\t0.0043686962302\n",
      "\"Credit monitoring or identity protection\"\t1453\t0.00464353739757\n",
      "\"Improper use of my credit report\"\t1477\t0.00472023725824\n",
      "\"Credit determination\"\t1490\t0.00476178301609\n",
      "\"Can't repay my loan\"\t1647\t0.00526352793792\n",
      "\"Credit reporting\"\t1701\t0.00543610262441\n",
      "\"Late fee\"\t1797\t0.00574290206706\n",
      "\"Dealing with my lender or servicer\"\t1944\t0.00621268871362\n",
      "\"Using a debit or ATM card\"\t2422\t0.0077402942718\n",
      "\"Credit decision / Underwriting\"\t2774\t0.00886522556151\n",
      "\"Closing/Cancelling account\"\t2795\t0.00893233793959\n",
      "\"Taking/threatening an illegal action\"\t2964\t0.00947243279175\n",
      "\"Making/receiving payments, sending money\"\t3226\t0.010309739604\n",
      "\"Identity theft / Fraud / Embezzlement\"\t3276\t0.0104695309804\n",
      "\"APR or interest rate\"\t3431\t0.0109648842471\n",
      "\"Improper contact or sharing of info\"\t3489\t0.0111502422437\n",
      "\"False statements or representation\"\t3621\t0.0115720914774\n",
      "\"Problems when you are unable to pay\"\t3821\t0.0122112569829\n",
      "\"Repaying your loan\"\t3844\t0.012284761016\n",
      "\"Settlement process and costs\"\t4350\t0.013901849745\n",
      "\"Unable to get credit report/credit score\"\t4357\t0.0139242205377\n",
      "\"Managing the loan or lease\"\t4560\t0.0145729735258\n",
      "\"Credit reporting company's investigation\"\t4858\t0.015525330129\n",
      "\"Problems caused by my funds being low\"\t5663\t0.0180979712887\n",
      "\"Other\"\t6273\t0.0200474260805\n",
      "\"Billing disputes\"\t6938\t0.0221726513863\n",
      "\"Disclosure verification of debt\"\t7655\t0.0244640597236\n",
      "\"Application, originator, mortgage broker\"\t8625\t0.0275640124254\n",
      "\"Communication tactics\"\t8671\t0.0277110204916\n",
      "\"Deposits and withdrawals\"\t10555\t0.0337319595536\n",
      "\"Account opening, closing, or management\"\t16205\t0.0517883850844\n",
      "\"Cont'd attempts collect debt not owed\"\t17972\t0.0574354123257\n",
      "\"Incorrect information on credit report\"\t29069\t0.0928995103992\n",
      "\"Loan servicing, payments, escrow account\"\t36767\t0.117500990707\n",
      "\"Loan modification,collection,foreclosure\"\t70487\t0.225264294937\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_2_c():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw32c_2\n",
    "    \n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1n -k2,2\" \\\n",
    "    -D mapred.map.tasks=4 \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -mapper mapper.py \\\n",
    "    -file mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -file reducer.py \\\n",
    "    -input hw3/output/hw32c/part* \\\n",
    "    -output hw3/output/hw32c_2\n",
    "    \n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw32c_2/part*\n",
    "\n",
    "hw3_2_c()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 3.2.1 OPTIONAL \n",
    "Using 2 reducers: What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.3. Shopping Cart Analysis\n",
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 In this question, we will emit a counter for everytime the reducer is called. \n",
    "# Here the mapper reads each session information and emits product along with the count.\n",
    "# We also emit the number of products in each basket and basket count as for each line read.\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,num_mapper,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    count = 0 #used to determine basket length\n",
    "    # remove leading and trailing whitespace\n",
    "    for token in line.strip().split(\" \"):\n",
    "        print '%s\\t%s' % (token, 1)\n",
    "        count += 1\n",
    "    print '%s\\t%s'%('*BASKET',count) # total number of products in the basket\n",
    "    print '%s\\t%s'%('*NUM_OF_BASKET',1) # basket count "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 In this question, we will emit a counter for everytime the combiner is called. \n",
    "# Here the combiner code does intermediate aggregation for product, number of baskets \n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,num_combiners,1\\n\")\n",
    "\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "basket_cnt = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = row[0]\n",
    "    count = int(row[1])\n",
    "    \n",
    "    #if its basket count determine which whether the basket is largest so far and continue\n",
    "    if (product =='*BASKET'):\n",
    "        if(count > max_basket):\n",
    "            max_basket = count\n",
    "        continue\n",
    "    #if the data is about number of baskets increment the count and continue\n",
    "    elif (product =='*NUM_OF_BASKET'):\n",
    "        basket_cnt += count\n",
    "        continue\n",
    "    #else aggregate the product data\n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            if (last_key):\n",
    "                print '%s\\t%s' %(last_key,total_count)\n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if last_key == product:\n",
    "    print '%s\\t%s' %(last_key,total_count)\n",
    "\n",
    "if(max_basket != -1):\n",
    "    print '%s\\t%s' %('*BASKET',max_basket) \n",
    "print '%s\\t%s' %('*NUM_OF_BASKET',basket_cnt) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 In this question, we will emit a counter for everytime the reducer is called. \n",
    "#here we have used heapq to store the 50 top products frequently purchased. \n",
    "\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3,num_reducers,1\\n\")\n",
    "\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "n_max = 100 #top n products purchased\n",
    "unique_cnt = 0 #number of unique products \n",
    "max_queue =  [] # hold top n products \n",
    "num_baskets = 0\n",
    "\n",
    "#function to add to queue.\n",
    "#we have used heapq here so we can remove the smallest element if the queue is full during insertion.\n",
    "def add_to_max_Q(max_q,product,cnt):\n",
    "    # add elements if q is not full\n",
    "    if (len(max_q) < n_max):\n",
    "        heapq.heappush(max_q, (cnt, product))\n",
    "    else:\n",
    "        # add new element and then remove smallest element\n",
    "        heapq.heappush(max_q, (cnt, product))\n",
    "        heapq.heappop(max_q)\n",
    "\n",
    "#function to print the queue data\n",
    "def print_topn(max_q,total_bsk):\n",
    "    cnt = len(max_q)\n",
    "    if(cnt > n_max):\n",
    "        cnt = n_max\n",
    "    s = heapq.nlargest(cnt, max_q)\n",
    "    for row in s:\n",
    "        print '%s\\t%s\\t%s'%(row[1] ,row[0],(1.0*row[0]/total_bsk))\n",
    "    \n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = row[0]\n",
    "    count = int(row[1])\n",
    "    \n",
    "    #if data is about basket length, determine which is largest and continue\n",
    "    if (product =='*BASKET'):\n",
    "        if(count > max_basket):\n",
    "            max_basket = count\n",
    "        continue\n",
    "    #if data is about number of baskets , than increment total count\n",
    "    elif (product == '*NUM_OF_BASKET'):\n",
    "        num_baskets += count\n",
    "        continue\n",
    "    #aggregate products \n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            if (last_key):\n",
    "                #increment unique product count and add to our queue \n",
    "                unique_cnt += 1\n",
    "                add_to_max_Q(max_queue,last_key,total_count) \n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if last_key == product:\n",
    "    unique_cnt += 1\n",
    "    #uncomment below line if you want use heapq\n",
    "    add_to_max_Q(max_queue,last_key,total_count)\n",
    "\n",
    "    \n",
    "print '%s\\t%s\\t%s' %('*LARGEST_BASKET',max_basket,1)\n",
    "print '%s\\t%s\\t%s' %('*UNIQUE_CNT',unique_cnt,1)\n",
    "print_topn(max_queue,num_baskets)\n",
    "\n",
    "# print \"\\n Largest Basket : %s\" %(max_basket)\n",
    "# print \"\\n Number of unique products: %s\" %(unique_cnt)\n",
    "# print \"\\n Top 50 products (product, frequency, relative frequency )\\n \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 22:06:12 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw33\n",
      "16/01/31 22:06:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/src/ProductPurchaseData.txt\n",
      "16/01/31 22:06:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/31 22:06:16 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 22:06:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar1866844824180613390/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob2321349398462098251.jar tmpDir=null\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_3():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw33\n",
    "    \n",
    "    #move input file to hdfs \n",
    "    !hdfs dfs -rm hw3/src/ProductPurchaseData.txt\n",
    "    !hdfs dfs -put ProductPurchaseData.txt  hw3/src/\n",
    "    \n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file combiner.py \\\n",
    "    -combiner combiner.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/ProductPurchaseData.txt \\\n",
    "    -output hw3/output/hw33\n",
    "    \n",
    "#     print \"\\n\"\n",
    "#     !echo \"HDFS Output\"\n",
    "#     !hdfs dfs -cat hw3/output/hw33/part*\n",
    "\n",
    "hw3_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 Writing a second map reduce job to do sorting on the output from the previous job\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3b,num_mapper,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    token = line.strip().split('\\t')\n",
    "    print '%s\\t%s\\t%s' %(token[1],token[0],token[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "#HW3.3 Writing a second  reduce job to print the 50 top products \n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_3b,num_mapper,1\\n\")\n",
    "\n",
    "print \"\\n Top 50 products (product, frequency, relative frequency )\\n \"\n",
    "counter = 0\n",
    "max_basket=0\n",
    "unique_cnt =0\n",
    "n_max=50\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    token = re.split(r'\\t+',line.strip())\n",
    "    \n",
    "    if(token[1]=='*LARGEST_BASKET'):\n",
    "        max_basket = token[0]\n",
    "        continue\n",
    "    elif(token[1]=='*UNIQUE_CNT'):\n",
    "        unique_cnt = token[0]\n",
    "        continue;\n",
    "    else:\n",
    "        if(counter<n_max):\n",
    "            print '%s\\t%s\\t%s'%(token[1] ,token[0],token[2]) \n",
    "            counter += 1\n",
    "\n",
    "print \"\\n Largest Basket : %s\" %(max_basket)\n",
    "print \"\\n Number of unique products: %s\" %(unique_cnt)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 22:06:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw33b\n",
      "16/01/31 22:06:49 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 22:06:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar5123181696967325995/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob6697234280007067738.jar tmpDir=null\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/01/31 22:07:06 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\t\n",
      " Top 50 products (product, frequency, relative frequency )\t\n",
      " \t\n",
      "DAI62779\t6667\t0.214366097553\n",
      "FRO40251\t3881\t0.124786984341\n",
      "ELE17451\t3875\t0.1245940645\n",
      "GRO73461\t3602\t0.115816211697\n",
      "SNA80324\t3044\t0.0978746664094\n",
      "ELE32164\t2851\t0.0916690781647\n",
      "DAI75645\t2736\t0.0879714478634\n",
      "SNA45677\t2455\t0.0789363686055\n",
      "FRO31317\t2330\t0.0749172052346\n",
      "DAI85309\t2293\t0.0737275328768\n",
      "ELE26917\t2292\t0.0736953795698\n",
      "FRO80039\t2233\t0.0717983344587\n",
      "GRO21487\t2115\t0.0680042442365\n",
      "SNA99873\t2083\t0.0669753384136\n",
      "GRO59710\t2004\t0.0644352271631\n",
      "GRO71621\t1920\t0.0617343493778\n",
      "FRO85978\t1918\t0.0616700427639\n",
      "GRO30386\t1840\t0.0591620848204\n",
      "ELE74009\t1816\t0.0583904054532\n",
      "GRO56726\t1784\t0.0573614996302\n",
      "DAI63921\t1773\t0.0570078132536\n",
      "GRO46854\t1756\t0.0564612070351\n",
      "ELE66600\t1713\t0.0550786148355\n",
      "DAI83733\t1712\t0.0550464615286\n",
      "FRO32293\t1702\t0.0547249284589\n",
      "ELE66810\t1697\t0.0545641619241\n",
      "SNA55762\t1646\t0.0529243432687\n",
      "DAI22177\t1627\t0.0523134304363\n",
      "FRO78087\t1531\t0.0492267129674\n",
      "ELE99737\t1516\t0.0487444133629\n",
      "ELE34057\t1489\t0.0478762740748\n",
      "GRO94758\t1489\t0.0478762740748\n",
      "FRO35904\t1436\t0.0461721488055\n",
      "FRO53271\t1420\t0.045657695894\n",
      "SNA93860\t1407\t0.0452397029034\n",
      "SNA90094\t1390\t0.044693096685\n",
      "GRO38814\t1352\t0.0434712710202\n",
      "ELE56788\t1345\t0.0432461978715\n",
      "GRO61133\t1321\t0.0424745185042\n",
      "DAI88807\t1316\t0.0423137519694\n",
      "ELE74482\t1316\t0.0423137519694\n",
      "ELE59935\t1311\t0.0421529854346\n",
      "SNA96271\t1295\t0.0416385325231\n",
      "DAI43223\t1290\t0.0414777659882\n",
      "ELE91337\t1289\t0.0414456126813\n",
      "GRO15017\t1275\t0.0409954663837\n",
      "DAI31081\t1261\t0.0405453200862\n",
      "GRO81087\t1220\t0.0392270345005\n",
      "DAI22896\t1219\t0.0391948811935\n",
      "GRO85051\t1214\t0.0390341146587\n",
      "\t\n",
      " Largest Basket : 37\t\n",
      "\t\n",
      " Number of unique products: 12591\t\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "def hw3_3_b():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw33b\n",
    "    \n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2\" \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -mapper mapper.py \\\n",
    "    -file mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -file reducer.py \\\n",
    "    -input hw3/output/hw33/part* \\\n",
    "    -output hw3/output/hw33b\n",
    "    \n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw33b/part*\n",
    "\n",
    "hw3_3_b()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3.1 OPTIONAL \n",
    "Using 2 reducers:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.4. (Computationally prohibitive but then again Hadoop can handle this) Pairs\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support\n",
    "\n",
    "\n",
    "\n",
    "Fix the ordering of the pairs lexicographically (left to right), \n",
    "and break ties in support (between pairs, if any exist) \n",
    "by taking the first ones in lexicographically increasing order. \n",
    "\n",
    "Report  the compute time for the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "#HW3.4 In this question, we will emit a counter for everytime the mapper is called. \n",
    "# In this mapper code, we are first taking unique products for each basket ( if there are duplicates ) \n",
    "# and than sorting it. We have used python function of combinations to emit all tuples of length 2.\n",
    "# We also emit for everyline read a count for the number of baskets \n",
    "\n",
    "import sys\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,num_mapper,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # emit count for basket\n",
    "    print '%s\\t%s\\t%s'%('*','BASKET',1)\n",
    "    \n",
    "    # remove leading and trailing whitespace and tokenize\n",
    "    token  =  line.strip().split(\" \")\n",
    "    for subset in itertools.combinations(sorted(set(token)), 2):\n",
    "        print '%s\\t%s\\t%s' % (subset[0],subset[1], 1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the combiner is called. \n",
    "# In this combiner code we aggregate the basket count and products that occur together.\n",
    "# This is done for intermediate aggregation\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,num_combiner,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "basket_cnt = 0\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = (row[0],row[1]) #combine product1 and product2 as tuple to be used as key\n",
    "    count = int(row[2])\n",
    "    #increment total number of baskets if incoming data is for baskets \n",
    "    if (product ==('*','BASKET')):\n",
    "        basket_cnt += count\n",
    "        continue\n",
    "\n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            if (last_key):\n",
    "                print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if last_key == product:\n",
    "    print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "\n",
    "print '%s\\t%s\\t%s' %('*','BASKET',basket_cnt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the reducer is called. \n",
    "# In this reducer code we aggregate the basket count and products that occur together.\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4,num_reducers,1\\n\")\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "basket_cnt = 0\n",
    "sup_cnt = 100 \n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    product = (row[0],row[1]) #combine product1 and product2 as tuple to be used as key\n",
    "    count = int(row[2])\n",
    "    #increment total number of baskets if incoming data is for baskets \n",
    "    if (product ==('*','BASKET')):\n",
    "        basket_cnt += count\n",
    "        continue\n",
    "    #increment the count for products that occur together\n",
    "    else:    \n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == product):\n",
    "            total_count += count\n",
    "        else:\n",
    "            #emit pair of product if they are greater than 100\n",
    "            if (last_key and total_count > sup_cnt):\n",
    "                print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "            total_count = count\n",
    "            last_key = product\n",
    "if (last_key == product and total_count > sup_cnt ):\n",
    "    print '%s\\t%s\\t%s' %(last_key[0],last_key[1],total_count)\n",
    "#emit the total number of baskets\n",
    "print '%s\\t%s\\t%s' %('*','NUM_OF_BASKET',basket_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 22:11:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw34a\n",
      "16/01/31 22:11:43 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 22:11:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, combiner.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar4466300746263274902/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob4216056481942220430.jar tmpDir=null\n",
      "\n",
      " Time taken to run this section of code in seconds: \n",
      "31.7277860641\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "# function to run the hadoop job \n",
    "def hw3_4a():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw34a\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=2 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1 -k2,2\" \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file combiner.py \\\n",
    "    -combiner combiner.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/ProductPurchaseData.txt \\\n",
    "    -output hw3/output/hw34a\n",
    "     \n",
    "    print\"\\n Time taken to run this section of code in seconds: \"\n",
    "    print time.time() - start\n",
    "#     print \"\\n\"\n",
    "#     !echo \"HDFS Output\"\n",
    "#     !hdfs dfs -cat hw3/output/hw34a/part*\n",
    "\n",
    "hw3_4a()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing second mapreduce job to determine top 50 product pairs which coccur "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the mapper is called is called. \n",
    "# In the mapper code we will emit the frequency first followed by the product pair\n",
    "# we are also going to tell hadoop to sort the key as numeric and in reverse order\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4b,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    #remove leading and trailing spaces \n",
    "    row = re.split(r'\\t',line.strip())\n",
    "    print '%s\\t%s\\t%s' %(row[2],row[0],row[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#HW3.4 In this question, we will emit a counter for everytime the reducer is called is called. \n",
    "# As we are sorting the data in descending order based on the frequency, we will get the number of baskets on top.\n",
    "# after that we enter the top 50 product pair in the list and print the record\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_4b,num_reducers,1\\n\")\n",
    "\n",
    "n_max = 50\n",
    "a_max = []\n",
    "num_baskets =0 \n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t+',line.strip())\n",
    "    freq = int(row[0])\n",
    "    key = (row[1],row[2])\n",
    "    \n",
    "    if(key==('*','NUM_OF_BASKET')):\n",
    "        num_baskets = freq\n",
    "        continue\n",
    "    else:\n",
    "        # add the lowest 10 words \n",
    "        if len(a_max) < n_max:\n",
    "            a_max.append((key,freq,1.0*freq/num_baskets))\n",
    "        \n",
    "for record in a_max:\n",
    "    print record"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 22:14:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw34b\n",
      "16/01/31 22:14:27 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 22:14:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar759612420876421916/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob9181965335011797423.jar tmpDir=null\n",
      "\n",
      " Time taken to run this section of code in seconds: \n",
      "16.7769880295\n",
      "\n",
      "\n",
      "HDFS Output\n",
      "16/01/31 22:14:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "(('DAI62779', 'ELE17451'), 902, 0.0290022828847947)\t\n",
      "(('FRO40251', 'GRO85051'), 859, 0.027619690685186972)\t\n",
      "(('FRO40251', 'SNA80324'), 846, 0.02720169769460789)\t\n",
      "(('DAI62779', 'GRO73461'), 844, 0.027137391080672646)\t\n",
      "(('DAI62779', 'ELE92920'), 789, 0.02536895919745346)\t\n",
      "(('DAI75645', 'FRO40251'), 729, 0.02343976077939616)\t\n",
      "(('DAI62779', 'ELE17451'), 690, 0.022185781807658917)\t\n",
      "(('DAI75645', 'SNA80324'), 663, 0.021317642519533133)\t\n",
      "(('DAI62779', 'FRO40251'), 658, 0.021156875984695026)\t\n",
      "(('DAI62779', 'DAI85309'), 614, 0.019742130478119676)\t\n",
      "(('DAI62779', 'SNA80324'), 598, 0.01922767756663773)\t\n",
      "(('DAI85309', 'ELE99737'), 594, 0.019099064338767242)\t\n",
      "(('FRO40251', 'GRO73461'), 569, 0.0182952316645767)\t\n",
      "(('FRO40251', 'SNA80324'), 566, 0.018198771743673837)\t\n",
      "(('DAI62779', 'DAI75645'), 560, 0.018005851901868108)\t\n",
      "(('ELE32164', 'GRO59710'), 541, 0.017394939069483296)\t\n",
      "(('DAI75645', 'FRO40251'), 525, 0.01688048615800135)\t\n",
      "(('DAI75645', 'GRO73461'), 504, 0.016205266711681297)\t\n",
      "(('DAI75645', 'SNA80324'), 467, 0.015015594353879296)\t\n",
      "(('DAI62779', 'SNA55762'), 463, 0.01488698112600881)\t\n",
      "(('ELE17451', 'FRO40251'), 453, 0.014565448056332593)\t\n",
      "(('DAI62779', 'FRO19221'), 449, 0.014436834828462107)\t\n",
      "(('ELE17451', 'SNA80324'), 428, 0.013761615382142054)\t\n",
      "(('DAI62779', 'ELE32164'), 426, 0.01369730876820681)\t\n",
      "(('FRO40251', 'FRO92469'), 426, 0.01369730876820681)\t\n",
      "(('GRO73461', 'SNA80324'), 424, 0.013633002154271566)\t\n",
      "(('DAI62779', 'FRO40251'), 412, 0.013247162470660108)\t\n",
      "(('ELE17451', 'GRO73461'), 409, 0.013150702549757242)\t\n",
      "(('FRO40251', 'FRO92469'), 409, 0.013150702549757242)\t\n",
      "(('DAI62779', 'ELE32164'), 406, 0.013054242628854377)\t\n",
      "(('DAI62779', 'SNA18336'), 394, 0.012668402945242917)\t\n",
      "(('DAI62779', 'SNA45677'), 392, 0.012604096331307674)\t\n",
      "(('DAI62779', 'GRO30386'), 389, 0.01250763641040481)\t\n",
      "(('DAI62779', 'ELE26917'), 371, 0.011928876884987621)\t\n",
      "(('DAI75645', 'ELE17451'), 370, 0.01189672357802)\t\n",
      "(('ELE32164', 'GRO59710'), 370, 0.01189672357802)\t\n",
      "(('ELE92920', 'SNA18336'), 369, 0.011864570271052378)\t\n",
      "(('DAI62779', 'DAI83733'), 363, 0.011671650429246647)\t\n",
      "(('DAI62779', 'GRO71621'), 357, 0.011478730587440918)\t\n",
      "(('DAI43223', 'ELE32164'), 356, 0.011446577280473297)\t\n",
      "(('DAI43223', 'ELE32164'), 355, 0.011414423973505675)\t\n",
      "(('FRO40251', 'GRO85051'), 354, 0.011382270666538054)\t\n",
      "(('FRO92469', 'SNA80324'), 352, 0.01131796405260281)\t\n",
      "(('FRO85978', 'SNA95666'), 346, 0.01112504421079708)\t\n",
      "(('ELE17451', 'ELE92920'), 344, 0.011060737596861837)\t\n",
      "(('GRO85051', 'SNA80324'), 339, 0.01089997106202373)\t\n",
      "(('GRO21487', 'GRO73461'), 335, 0.010771357834153244)\t\n",
      "(('GRO46854', 'GRO73461'), 328, 0.010546284685379891)\t\n",
      "(('DAI62779', 'FRO80039'), 327, 0.01051413137841227)\t\n",
      "(('DAI55148', 'DAI62779'), 325, 0.010449824764477027)\t\n"
     ]
    }
   ],
   "source": [
    "# function to run the hadoop job \n",
    "import time \n",
    "def hw3_4_b():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw34b\n",
    "    \n",
    "    start = time.time()\n",
    "    \n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapreduce.lib.partition.KeyFieldBasedComparator \\\n",
    "    -D stream.num.map.output.key.fields=3 \\\n",
    "    -D mapreduce.partition.keypartitioner.options=-k1,1 \\\n",
    "    -D mapreduce.partition.keycomparator.options=\"-k1,1nr -k2,2 -k3,3\" \\\n",
    "    -D mapred.reduce.tasks=1 \\\n",
    "    -mapper mapper.py \\\n",
    "    -file mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -file reducer.py \\\n",
    "    -input hw3/output/hw34a/part* \\\n",
    "    -output hw3/output/hw34b\n",
    "    \n",
    "    print\"\\n Time taken to run this section of code in seconds: \"\n",
    "    print time.time() - start\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"HDFS Output\"\n",
    "    !hdfs dfs -cat hw3/output/hw34b/part*\n",
    "\n",
    "hw3_4_b()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computational setup used \n",
    "\n",
    "1. Single node haddop cluster , running hadoop version 2.7.1. This was setup on my mac (intel i7, 4 cores)\n",
    "2. Yarn is enabled and all mapreduce jobs are running in yarn mode\n",
    "3. Yarn configured with a single node manager / slave. This node manager has 8GB capacity. Yarn vcore /cpu support was not enabled \n",
    "4. Each map task is configured to use 1 GB resource (mapreduce.map.memory.mb = 1024, default value)\n",
    "5. Each reduce task is configured to use 1 GB resource (mapreduce.reduce.memory.mb = 1024, default value)\n",
    "6. Given this, it means that my yarn cluster could run at max 6 maps/reduce tasks concurrently (yarn.app.mapreduce.am.resource.mb = 1536, MR application master is using 2GB)  \n",
    "\n",
    "Total time taken = \n",
    "From Job a : \n",
    "Time Taken : 31.7277860641 secs\n",
    "Number of mapper = 2\n",
    "Number of reducers = 1\n",
    "Number of combiners = 2\n",
    "\n",
    "From Job b : \n",
    "Time Taken : 16.7769880295 secs\n",
    "Number of mapper = 2\n",
    "Number of reducers = 1\n",
    "Number of combiners = 0\n",
    "\n",
    "Image from Job a\n",
    "<img src=\"./images/hw34a.jpg\">\n",
    "\n",
    "Image from Job b\n",
    "<img src=\"./images/hw34b.jpg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.5: Stripes\n",
    "Repeat 3.4 using the stripes design pattern for finding cooccuring pairs.\n",
    "\n",
    "Report  the compute times for stripes job versus the Pairs job. Describe the computational setup used (E.g., single computer; dual core; linux, number of mappers, number of reducers)\n",
    "\n",
    "Instrument your mapper, combiner, and reducer to count how many times each is called using Counters and report these counts. Discuss the differences in these counts between the Pairs and Stripes jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5a,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # Emit * , BAKSET for counting total number of baskets \n",
    "    print '%s\\t%s\\t%s'%('*','BASKET',1)\n",
    "    # remove leading and trailing whitespace and tokenize\n",
    "    token  =  line.strip().split(\" \")\n",
    "    uniq_tokens = sorted(set(token))\n",
    "    for i in range (0,len(uniq_tokens)-1):\n",
    "        key = uniq_tokens[i]\n",
    "        value = \"\"\n",
    "        for j in range(i+1, len(uniq_tokens)):\n",
    "            value += uniq_tokens[j] + \"\\t1\\t\"\n",
    "        print '%s\\t%s' %(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import heapq\n",
    "import operator\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5a,num_reducers,1\\n\")\n",
    "\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "max_basket = -1\n",
    "basket_cnt = 0 \n",
    "sup_cnt = 100 # variable to hold the minimum support count \n",
    "tmp_dict ={} # for aggregation\n",
    "n_max = 50 # top n product pairs \n",
    "max_q = []\n",
    "\n",
    "#function to add to queue.\n",
    "#we have used heapq here so we can remove the smallest element if the queue is full during insertion.\n",
    "def add_to_max_Q(p1,p2,cnt):\n",
    "    # add elements if q is not full\n",
    "    if (len(max_q) < n_max):\n",
    "        heapq.heappush(max_q, (cnt, (p1,p2)))\n",
    "    else:\n",
    "        # add new element and then remove smallest element\n",
    "        heapq.heappush(max_q, (cnt, (p1,p2)))\n",
    "        heapq.heappop(max_q)\n",
    "    \n",
    "#function to print the queue data\n",
    "def print_topn(total_bsk):\n",
    "    cnt = len(max_q)\n",
    "    if(cnt > n_max):\n",
    "        cnt = n_max\n",
    "    s = heapq.nlargest(cnt, max_q)\n",
    "    for row in s:\n",
    "        print '%s\\t%s\\t%s\\t%s'%(row[1][0] ,row[1][1],row[0],(1.0*row[0]/total_bsk))\n",
    "\n",
    "def merge_dict(merged,row):\n",
    "    for product in row:\n",
    "        if product in merged:\n",
    "            merged[product] += row[product]\n",
    "        else:\n",
    "            merged[product] = row[product]\n",
    "    return merged\n",
    "\n",
    "def emit_dict(key,merged):\n",
    "    output = key\n",
    "    cnt =0\n",
    "    merged_sorted = sorted(merged.items(), key=operator.itemgetter(0))\n",
    "    for tup in merged_sorted:\n",
    "        # for prd, value in merged_sort.iteritems():\n",
    "        prd = tup[0]\n",
    "        value = tup[1]\n",
    "        if(value < sup_cnt):\n",
    "            continue\n",
    "        cnt += 1\n",
    "        add_to_max_Q(key,prd,value)\n",
    "        output += \"\\t\" + prd + \"\\t\" + str(value)\n",
    "\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    row = re.split(r'\\t',line.strip())\n",
    "    product = row[0]\n",
    "    #increment the total basket if we encounter * BASKET tuple\n",
    "    if(row[0]=='*' and row[1]=='BASKET'):\n",
    "        basket_cnt += int(row[2])\n",
    "        \n",
    "        continue\n",
    "        \n",
    "    #increment the count for products that occur together. \n",
    "    else:\n",
    "        row_dict ={}\n",
    "        #parse the remaining data elements and increment counters by 2\n",
    "        for x in range (1,len(row),2):\n",
    "            prd = row[x]\n",
    "            prd_cnt = int(row[x+1])\n",
    "            row_dict[prd]=prd_cnt\n",
    "        \n",
    "        if(last_key == product):\n",
    "            #update dictionary                \n",
    "            tmp_dict=merge_dict(tmp_dict,row_dict)\n",
    "\n",
    "        #emit the data is we find new product key \n",
    "        else:\n",
    "            #emit pair of product if they are greater than 100\n",
    "            if (last_key):\n",
    "                #print dictionary\n",
    "                emit_dict(last_key,tmp_dict)\n",
    "            #create dictonary\n",
    "            tmp_dict=row_dict\n",
    "            last_key = product\n",
    "                        \n",
    "            \n",
    "if (last_key == product):\n",
    "    #print dictionary\n",
    "    emit_dict(last_key,tmp_dict)\n",
    "    \n",
    "#emit the total number of baskets\n",
    "print '%s\\t%s\\t%s' %('*','NUM_OF_BASKET',basket_cnt)     \n",
    "print \"\\n %d Top product pairs that occur together\" %(n_max)\n",
    "print_topn(basket_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/31 19:10:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw3/output/hw35a\n",
      "16/01/31 19:10:16 WARN streaming.StreamJob: -file option is deprecated, please use generic option -files instead.\n",
      "16/01/31 19:10:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "packageJobJar: [mapper.py, reducer.py, /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/hadoop-unjar839828248141599493/] [] /var/folders/91/cjfxt7ys6958qll6vjtgwwfw0000gn/T/streamjob4969713235079721198.jar tmpDir=null\n",
      "\n",
      " Time taken to run this section of code in seconds: \n",
      "22.3631811142\n",
      "\n",
      "\n",
      "16/01/31 19:10:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "*\tNUM_OF_BASKET\t31101\n",
      "\t\n",
      " 50 Top product pairs that occur together\t\n",
      "DAI62779\tELE17451\t1592\t0.0511880646925\n",
      "FRO40251\tSNA80324\t1412\t0.0454004694383\n",
      "DAI75645\tFRO40251\t1254\t0.0403202469374\n",
      "FRO40251\tGRO85051\t1213\t0.0390019613517\n",
      "DAI62779\tGRO73461\t1139\t0.0366226166361\n",
      "DAI75645\tSNA80324\t1130\t0.0363332368734\n",
      "DAI62779\tFRO40251\t1070\t0.0344040384554\n",
      "DAI62779\tSNA80324\t923\t0.0296775023311\n",
      "DAI62779\tDAI85309\t918\t0.0295167357963\n",
      "ELE32164\tGRO59710\t911\t0.0292916626475\n",
      "FRO40251\tGRO73461\t882\t0.0283592167454\n",
      "DAI62779\tDAI75645\t882\t0.0283592167454\n",
      "DAI62779\tELE92920\t877\t0.0281984502106\n",
      "FRO40251\tFRO92469\t835\t0.026848011318\n",
      "DAI62779\tELE32164\t832\t0.0267515513971\n",
      "DAI75645\tGRO73461\t712\t0.0228931545609\n",
      "DAI43223\tELE32164\t711\t0.022861001254\n",
      "DAI62779\tGRO30386\t709\t0.02279669464\n",
      "ELE17451\tFRO40251\t697\t0.0224108549564\n",
      "DAI85309\tELE99737\t659\t0.0211890292917\n",
      "DAI62779\tELE26917\t650\t0.020899649529\n",
      "GRO21487\tGRO73461\t631\t0.0202887366966\n",
      "DAI62779\tSNA45677\t604\t0.0194205974084\n",
      "ELE17451\tSNA80324\t597\t0.0191955242597\n",
      "DAI62779\tGRO71621\t595\t0.0191312176457\n",
      "DAI62779\tSNA55762\t593\t0.0190669110318\n",
      "DAI62779\tDAI83733\t586\t0.018841837883\n",
      "ELE17451\tGRO73461\t580\t0.0186489180412\n",
      "GRO73461\tSNA80324\t562\t0.0180701585158\n",
      "DAI62779\tGRO59710\t561\t0.0180380052088\n",
      "DAI62779\tFRO80039\t550\t0.0176843188322\n",
      "DAI75645\tELE17451\t547\t0.0175878589113\n",
      "DAI62779\tSNA93860\t537\t0.0172663258416\n",
      "DAI55148\tDAI62779\t526\t0.016912639465\n",
      "DAI43223\tGRO59710\t512\t0.0164624931674\n",
      "ELE17451\tELE32164\t511\t0.0164303398605\n",
      "DAI62779\tSNA18336\t506\t0.0162695733256\n",
      "ELE32164\tGRO73461\t486\t0.0156265071863\n",
      "DAI85309\tELE17451\t482\t0.0154978939584\n",
      "DAI62779\tFRO78087\t482\t0.0154978939584\n",
      "DAI62779\tGRO94758\t479\t0.0154014340375\n",
      "GRO85051\tSNA80324\t471\t0.0151442075817\n",
      "DAI62779\tGRO21487\t471\t0.0151442075817\n",
      "ELE17451\tGRO30386\t468\t0.0150477476608\n",
      "FRO85978\tSNA95666\t463\t0.014886981126\n",
      "DAI62779\tFRO19221\t462\t0.014854827819\n",
      "DAI62779\tGRO46854\t461\t0.0148226745121\n",
      "DAI43223\tDAI62779\t459\t0.0147583678981\n",
      "ELE92920\tSNA18336\t455\t0.0146297546703\n",
      "DAI88079\tFRO40251\t446\t0.0143403749076\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "# function to run the hadoop job \n",
    "def hw3_5a():\n",
    "    \n",
    "    #change properties of mapper.py \n",
    "    !chmod a+x mapper.py;chmod a+x reducer.py\n",
    "    \n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw3/output/hw35a\n",
    "    \n",
    "    start = time.time()\n",
    "    #run hadoop job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar  \\\n",
    "    -file mapper.py \\\n",
    "    -mapper mapper.py \\\n",
    "    -file reducer.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw3/src/ProductPurchaseData.txt \\\n",
    "    -output hw3/output/hw35a\n",
    "    \n",
    "    print\"\\n Time taken to run this section of code in seconds: \"\n",
    "    print time.time() - start\n",
    "    print \"\\n\"\n",
    "    !hdfs dfs -cat hw3/output/hw35a/part*\n",
    "    \n",
    "hw3_5a()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#hw3.5b \n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5b,num_mappers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace and tokenize\n",
    "    token  =  line.strip().split(\"\\t\")\n",
    "    for i in range (0,len(uniq_tokens)-1):\n",
    "        key = uniq_tokens[i]\n",
    "        value = \"\"\n",
    "        for j in range(i+1, len(uniq_tokens)):\n",
    "            value += uniq_tokens[j] + \"\\t1\\t\"\n",
    "        print '%s\\t%s' %(key,value)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#hw3.5b \n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "sys.stderr.write(\"reporter:counter:HW3_5b,num_reducers,1\\n\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace and tokenize\n",
    "    token  =  line.strip().split(\"\\t\")\n",
    "    for i in range (0,len(uniq_tokens)-1):\n",
    "        key = uniq_tokens[i]\n",
    "        value = \"\"\n",
    "        for j in range(i+1, len(uniq_tokens)):\n",
    "            value += uniq_tokens[j] + \"\\t1\\t\"\n",
    "        print '%s\\t%s' %(key,value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
