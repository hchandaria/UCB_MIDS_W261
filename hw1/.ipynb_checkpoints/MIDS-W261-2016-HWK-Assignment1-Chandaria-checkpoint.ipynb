{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #1=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #1  (version 2016-01-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hetal Chandaria (hetalchandaria@berkeley.edu)\n",
    "\n",
    "\n",
    "W261 - 2 , Assignment 01\n",
    "\n",
    "\n",
    "Submission Date : 01/18/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n",
    "\\cat $data.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        # Correct approach is to increment the count based on the number of occurences found.\n",
    "        # but shell script example provided only increments once per line matched.\n",
    "        count += len(indices)\n",
    "print count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            #Please insert your code\n",
    "            #convert to int and increment the sum \n",
    "            sum += int(line)   \n",
    "print \"Found \",sum,\" occurrences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  9  occurrences\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        # Correct approach is to increment the count based on the number of occurences found.\n",
    "        # but shell script example provided only increments once per line matched.\n",
    "        findword_count = len(indices)\n",
    "        total_doc_count = len(result)\n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+ findword\n",
    "        output += \"\\t\" + str(findword_count) + \"\\t\" + str(total_doc_count)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "total_spam_findword = 0\n",
    "total_nonspam_findword = 0\n",
    "\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            findword = content[2]\n",
    "            findword_freq = int(content[3])\n",
    "            total_doc_word_cnt = int(content[4])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_findword += findword_freq\n",
    "                total_spam_words += total_doc_word_cnt\n",
    "                \n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "                total_nonspam_findword += findword_freq\n",
    "                total_nonspam_words += total_doc_word_cnt\n",
    "                            \n",
    "prior_spam = (1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt )\n",
    "prior_ham = (1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt )\n",
    "# Probability of word given email class spam \n",
    "pr_findword_spam = (1.0)*(total_spam_findword)/total_spam_words\n",
    "pr_findword_ham = (1.0)*(total_nonspam_findword)/total_nonspam_words\n",
    "\n",
    "\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            findword_freq = int(content[3])\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = (1.0) * (prior_spam)*(pr_findword_spam**findword_freq)\n",
    "            pr_ham_doc = (1.0) * (prior_ham)*(pr_findword_ham**findword_freq)\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            print output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findwords ={}\n",
    "vocab_len = len(sys.argv[2:])\n",
    "for word in sys.argv[2:]:\n",
    "    findwords[word] = 1\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for key in result:\n",
    "            if key not in findwords:\n",
    "                continue\n",
    "            if key in vocab:\n",
    "                vocab[key] += 1\n",
    "            else:\n",
    "                vocab[key] = 1\n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+str(len(result))+\"\\t\"+str(vocab_len)\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            doc_word_cnt = int(content[2])\n",
    "            unique_word_cnt = int(content[3])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_words += doc_word_cnt\n",
    "            else:\n",
    "                non_spam_email_cnt += 1\n",
    "                total_nonspam_words += doc_word_cnt\n",
    "            if(len(content) > 4 ):\n",
    "                for x in range(4,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    if (true_class == 1):\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                        if word not in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] = 0\n",
    "                    else:\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "                        if word not in spam_words_freq:\n",
    "                            spam_words_freq[word] = 0\n",
    "                            \n",
    "prior_spam = (1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt )\n",
    "prior_ham = (1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt )\n",
    "# Probability of word given email class spam \n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(spam_words_freq[word])/ (total_spam_words + unique_word_cnt)\n",
    "    pr_word_spam[word] = pr        \n",
    "for word in not_spam_words_freq:\n",
    "     # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(not_spam_words_freq[word])/ (total_nonspam_words + unique_word_cnt)\n",
    "    pr_word_ham[word] = pr\n",
    "\n",
    "\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            if(len(content) > 4 ):\n",
    "                for x in range(4,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = 1.0\n",
    "            pr_ham_doc = 1.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                pr_spam_doc = pr_spam_doc * (pr_word_spam[key]**value)\n",
    "                pr_ham_doc = pr_ham_doc * (pr_word_ham[key]**value)                 \n",
    "            pr_spam_doc = prior_spam * pr_spam_doc\n",
    "            pr_ham_doc = prior_ham * pr_ham_doc\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            print output     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 assistance valium enlargementWithATypo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for key in result:\n",
    "            if key in vocab:\n",
    "                vocab[key] += 1\n",
    "            else:\n",
    "                vocab[key] = 1\n",
    "        output =content[0]+ \"\\t\" + content[1]\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "            if(len(content) > 2 ):\n",
    "                for x in range(2,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    if (not(word in spam_words_freq or word in not_spam_words_freq)):\n",
    "                        unique_word_cnt += 1;\n",
    "                    if (true_class == 1):\n",
    "                        total_spam_words += freq;\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                        if word not in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] = 0\n",
    "                    else:\n",
    "                        total_nonspam_words += freq;\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "                        if word not in spam_words_freq:\n",
    "                            spam_words_freq[word] = 0\n",
    "                            \n",
    "prior_spam = (1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt )\n",
    "prior_ham = (1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt )\n",
    "# Probability of word given email class spam \n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(spam_words_freq[word]+1)/ (total_spam_words + unique_word_cnt)\n",
    "    pr_word_spam[word] = pr        \n",
    "for word in not_spam_words_freq:\n",
    "     # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(not_spam_words_freq[word]+1)/ (total_nonspam_words + unique_word_cnt)\n",
    "    pr_word_ham[word] = pr\n",
    "\n",
    "\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            if(len(content) > 2 ):\n",
    "                for x in range(2,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = 1.0\n",
    "            pr_ham_doc = 1.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                pr_spam_doc = pr_spam_doc * (pr_word_spam[key]**value)\n",
    "                pr_ham_doc = pr_ham_doc * (pr_word_ham[key]**value)                 \n",
    "            pr_spam_doc = prior_spam * pr_spam_doc\n",
    "            pr_ham_doc = prior_ham * pr_ham_doc\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            print output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 2 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
