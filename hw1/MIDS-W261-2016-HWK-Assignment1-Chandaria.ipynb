{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #1=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #1  (version 2016-01-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hetal Chandaria (hetalchandaria@berkeley.edu)\n",
    "\n",
    "\n",
    "W261 - 2 , Assignment 01\n",
    "\n",
    "\n",
    "Submission Date : 01/18/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.1. Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n",
    "\\cat $data.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        # Correct approach is to increment the count based on the number of occurences found.\n",
    "        # but shell script example provided only increments once per line matched.\n",
    "        count += len(indices)\n",
    "print count        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "sum = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            #Please insert your code\n",
    "            #convert to int and increment the sum \n",
    "            sum += int(line)   \n",
    "print \"Found \",sum,\" occurrences\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found  10  occurrences\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        # Correct approach is to increment the count based on the number of occurences found.\n",
    "        # but shell script example provided only increments once per line matched.\n",
    "        findword_count = len(indices)\n",
    "        total_doc_count = len(result)\n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+ findword\n",
    "        output += \"\\t\" + str(findword_count) + \"\\t\" + str(total_doc_count)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "sum = 0\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "total_spam_findword = 0\n",
    "total_nonspam_findword = 0\n",
    "\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            findword = content[2]\n",
    "            findword_freq = int(content[3])\n",
    "            total_doc_word_cnt = int(content[4])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_findword += findword_freq\n",
    "                total_spam_words += total_doc_word_cnt\n",
    "                \n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "                total_nonspam_findword += findword_freq\n",
    "                total_nonspam_words += total_doc_word_cnt\n",
    "                            \n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "# Probability of word given email class spam \n",
    "pr_findword_spam = math.log((1.0)*(total_spam_findword)/total_spam_words)\n",
    "pr_findword_ham = math.log((1.0)*(total_nonspam_findword)/total_nonspam_words)\n",
    "\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            findword_freq = int(content[3])\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = prior_spam + (pr_findword_spam*findword_freq)\n",
    "            pr_ham_doc = prior_ham + (pr_findword_ham*findword_freq)\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            predicted_class =0 \n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class)==predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output\n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy of the model: 60.00\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findwords ={}\n",
    "local_findwords = re.split(\" \",sys.argv[2].lower())\n",
    "vocab_len = len(sys.argv[2:])\n",
    "for word in local_findwords:\n",
    "    findwords[word] = 1\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for word in local_findwords:\n",
    "            vocab[word] = 0\n",
    "        for key in result:\n",
    "            if key not in findwords:\n",
    "                continue\n",
    "            if key in vocab:\n",
    "                vocab[key] += 1\n",
    "            else:\n",
    "                vocab[key] = 1\n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+str(len(result))+\"\\t\"+str(vocab_len)\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math \n",
    "sum = 0\n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            doc_word_cnt = int(content[2])\n",
    "            unique_word_cnt = int(content[3])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_words += doc_word_cnt\n",
    "            else:\n",
    "                non_spam_email_cnt += 1\n",
    "                total_nonspam_words += doc_word_cnt\n",
    "            if(len(content) > 4 ):\n",
    "                for x in range(4,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    if (true_class == 1):\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                    else:\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "                            \n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "# Probability of word given email class spam \n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    if(spam_words_freq[word] > 0 ):\n",
    "        pr_word_spam[word] = math.log((1.0)*(spam_words_freq[word])/ (total_spam_words))\n",
    "    else:\n",
    "        pr_word_spam[word] = float('-inf')\n",
    "for word in not_spam_words_freq:\n",
    "    if(not_spam_words_freq[word] > 0):\n",
    "        pr_word_ham[word] = math.log((1.0)*(not_spam_words_freq[word])/(total_nonspam_words))\n",
    "    else:\n",
    "        pr_word_ham[word] = float('-inf')\n",
    "\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            if(len(content) > 4 ):\n",
    "                for x in range(4,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = 0.0\n",
    "            pr_ham_doc = 0.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                if (pr_word_spam[key] == float('-inf')):\n",
    "                    if(value !=0):\n",
    "                        pr_spam_doc += float('-inf')\n",
    "                    else :\n",
    "                        pr_spam_doc += 0\n",
    "                else:   \n",
    "                    pr_spam_doc += (pr_word_spam[key]*value)\n",
    "                if(pr_word_ham[key] == float('-inf')):\n",
    "                    if(value !=0):\n",
    "                        pr_ham_doc += float('-inf')\n",
    "                    else :\n",
    "                        pr_ham_doc += 0\n",
    "                else :\n",
    "                    pr_ham_doc += (pr_word_ham[key]*value)                 \n",
    "            pr_spam_doc = prior_spam + pr_spam_doc\n",
    "            pr_ham_doc = prior_ham + pr_ham_doc\n",
    "            output =  docId + \"\\t\" + true_class + \"\\t\"\n",
    "            predicted_class = 0\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class) == predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output \n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy of the model: 63.00\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.5. Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by all words present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for key in result:\n",
    "            if key in vocab:\n",
    "                vocab[key] += 1\n",
    "            else:\n",
    "                vocab[key] = 1\n",
    "        output =content[0]+ \"\\t\" + content[1]\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "sum = 0\n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "            if(len(content) > 2 ):\n",
    "                for x in range(2,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    if (not(word in spam_words_freq or word in not_spam_words_freq)):\n",
    "                        unique_word_cnt += 1;\n",
    "                    if (true_class == 1):\n",
    "                        total_spam_words += freq;\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                        if word not in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] = 0\n",
    "                    else:\n",
    "                        total_nonspam_words += freq;\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "                        if word not in spam_words_freq:\n",
    "                            spam_words_freq[word] = 0\n",
    "                            \n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "# Probability of word given email class spam \n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(spam_words_freq[word]+1)/ (total_spam_words + unique_word_cnt)\n",
    "    pr_word_spam[word] = pr        \n",
    "for word in not_spam_words_freq:\n",
    "     # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(not_spam_words_freq[word]+1)/ (total_nonspam_words + unique_word_cnt)\n",
    "    pr_word_ham[word] = pr\n",
    "\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            if(len(content) > 2 ):\n",
    "                for x in range(2,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = 0.0\n",
    "            pr_ham_doc = 0.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                pr_spam_doc +=  math.log(pr_word_spam[key])*value\n",
    "                pr_ham_doc += math.log(pr_word_ham[key])*value                 \n",
    "            pr_spam_doc = prior_spam + pr_spam_doc\n",
    "            pr_ham_doc = prior_ham + pr_ham_doc\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            predicted_class = 0\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class) == predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output \n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.6 Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "alpha = 1\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = \"enronemail_1h.txt\"  \n",
    "train_data = []\n",
    "train_label =[]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        true_class = int(content[1])\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        text = re.sub(r'[\\W]+', ' ', text)\n",
    "#        text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "        train_data.append(text)\n",
    "        train_label.append(true_class)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "text_matrix = count_vectorizer.fit_transform(train_data)\n",
    "feature_names = count_vectorizer.get_feature_names()\n",
    "# print feature_names[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "nb =  MultinomialNB(alpha=alpha)\n",
    "nb.fit(text_matrix, train_label)\n",
    "print \"Trained the model on emails\"\n",
    "\n",
    "# Compute accuracy on the test data.\n",
    "print \"Using our Multinomial classifier\"\n",
    "print 'sklearn accuracy: %3.2f' %(nb.score(text_matrix, train_label)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compare to sklearn's implementation.\n",
    "print \"Using sklearn's NB classifier\"\n",
    "clf = BernoulliNB(alpha=alpha)\n",
    "clf.fit(text_matrix, train_label)\n",
    "print 'sklearn accuracy: %3.2f' %(clf.score(text_matrix, train_label)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
