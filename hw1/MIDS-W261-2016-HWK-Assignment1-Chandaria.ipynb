{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #1=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #1  (version 2016-01-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hetal Chandaria (hetalchandaria@berkeley.edu)\n",
    "\n",
    "\n",
    "W261 - 2 , Assignment 01\n",
    "\n",
    "\n",
    "Submission Date : 01/18/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0. Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer :\n",
    "\n",
    "Big data is a term for data sets that are so large that traditional computing systems are not sufficient for processing them. They are usually characterised by the 4 V's :\n",
    "\n",
    "1. Volume : Amount of data generated  \n",
    "2. Velocity: How frequently is the data being generated \n",
    "3. Variety : Different forms of data\n",
    "4. Veracity : How certain are we about the data\n",
    "\n",
    "Example of big data:\n",
    "\n",
    "Consider eBay as an example. eBay has millions of users browsing through its website searching for, buying and selling products in 100s of categories. Trying to understand users and what their interests can be considered a big data challenge.\n",
    "\n",
    "One can consider tracking the activity of all unique users across all pages/categories/product pages of eBay on all devices. These events would need to be tracked, aggregated on a per user basis and historically tracked to try and create some form of a profile of a user in terms of which categories/products the user is interested in. Additionally, present/future behaviour can be compared to models learnt on past user behaviour. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1.In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer: \n",
    "\n",
    "\n",
    "Consider f(x) to be the true relationship function and g(x) be an estimator of f(x). \n",
    "Assume g(x) estimator to be polynomial regression model of degree 1,2,3,4,5 which is fit over the test dataset T.\n",
    "\n",
    "Based on this \n",
    "\n",
    "1. Variance of the estimator is defined as E[(g(x) - E[g(x)])^2]. Variance basically helps us understand how much the preictions for a given data point vary between different realizations of the same model.\n",
    "2. Bias of the estimator is defined as E[g(x)]-f(x). Bias measures the difference between the avergaes of the estimated value and the true values.  \n",
    "3. Irreducible Error is defined as variance in the data itself \n",
    "\n",
    "As model complexity increases ( i.e high order polynomials) we tend to find that the estimator fits the data well which means that the estimator variance increases while bias decreases. Models that are simple, have low variance but hgh bias. While selecting a model we should pick a model that will have both low bias and variance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.1. \n",
    "Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note\n",
    "enronemail_1h.txt file had to be fixed to remove mac-style line endings for correct processing. Below are the commands that were run to fix the issue.\n",
    "\n",
    "\n",
    "cat enronemail_1h.txt | tr '^M' '\\n' > ./foo\n",
    "\n",
    "mv foo enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*\n",
    "\\cat $data.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        # Correct approach is to increment the count based on the number of occurences found.\n",
    "        # but shell script example provided only increments once per line matched.\n",
    "        count += len(indices)\n",
    "output = findword+\"\\t\"+str(count)\n",
    "print output        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            #Please insert your code\n",
    "            #convert to int and increment the sum\n",
    "            content = re.split(r'\\t+', line)\n",
    "            sum += int(content[1])   \n",
    "print content[0]+\"\\t\"+str(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        # Correct approach is to increment the count based on the number of occurences found.\n",
    "        # but shell script example provided only increments once per line matched.\n",
    "        findword_count = len(indices)\n",
    "        total_doc_count = len(result)\n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+ findword\n",
    "        output += \"\\t\" + str(findword_count) + \"\\t\" + str(total_doc_count)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "sum = 0\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "total_spam_findword = 0\n",
    "total_nonspam_findword = 0\n",
    "\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            findword = content[2]\n",
    "            findword_freq = int(content[3])\n",
    "            total_doc_word_cnt = int(content[4])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_findword += findword_freq\n",
    "                total_spam_words += total_doc_word_cnt\n",
    "                \n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "                total_nonspam_findword += findword_freq\n",
    "                total_nonspam_words += total_doc_word_cnt\n",
    "                            \n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "# Probability of word given email class spam \n",
    "pr_findword_spam = math.log((1.0)*(total_spam_findword)/total_spam_words)\n",
    "pr_findword_ham = math.log((1.0)*(total_nonspam_findword)/total_nonspam_words)\n",
    "\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            findword_freq = int(content[3])\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = prior_spam + (pr_findword_spam*findword_freq)\n",
    "            pr_ham_doc = prior_ham + (pr_findword_ham*findword_freq)\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            predicted_class =0 \n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class)==predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output\n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy of the model: 60.00\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findwords ={}\n",
    "local_findwords = re.split(\" \",sys.argv[2].lower())\n",
    "vocab_len = len(sys.argv[2:])\n",
    "for word in local_findwords:\n",
    "    findwords[word] = 1\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for word in local_findwords:\n",
    "            vocab[word] = 0\n",
    "        for key in result:\n",
    "            if key not in findwords:\n",
    "                continue\n",
    "            if key in vocab:\n",
    "                vocab[key] += 1\n",
    "            else:\n",
    "                vocab[key] = 1\n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+str(len(result))+\"\\t\"+str(vocab_len)\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math \n",
    "sum = 0\n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            doc_word_cnt = int(content[2])\n",
    "            unique_word_cnt = int(content[3])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_words += doc_word_cnt\n",
    "            else:\n",
    "                non_spam_email_cnt += 1\n",
    "                total_nonspam_words += doc_word_cnt\n",
    "            if(len(content) > 4 ):\n",
    "                for x in range(4,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    if (true_class == 1):\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                    else:\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "                            \n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "# Probability of word given email class spam \n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    if(spam_words_freq[word] > 0 ):\n",
    "        pr_word_spam[word] = math.log((1.0)*(spam_words_freq[word])/ (total_spam_words))\n",
    "    else:\n",
    "        pr_word_spam[word] = float('-inf')\n",
    "for word in not_spam_words_freq:\n",
    "    if(not_spam_words_freq[word] > 0):\n",
    "        pr_word_ham[word] = math.log((1.0)*(not_spam_words_freq[word])/(total_nonspam_words))\n",
    "    else:\n",
    "        pr_word_ham[word] = float('-inf')\n",
    "\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            if(len(content) > 4 ):\n",
    "                for x in range(4,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = 0.0\n",
    "            pr_ham_doc = 0.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                if (pr_word_spam[key] == float('-inf')):\n",
    "                    if(value !=0):\n",
    "                        pr_spam_doc += float('-inf')\n",
    "                    else :\n",
    "                        pr_spam_doc += 0\n",
    "                else:   \n",
    "                    pr_spam_doc += (pr_word_spam[key]*value)\n",
    "                if(pr_word_ham[key] == float('-inf')):\n",
    "                    if(value !=0):\n",
    "                        pr_ham_doc += float('-inf')\n",
    "                    else :\n",
    "                        pr_ham_doc += 0\n",
    "                else :\n",
    "                    pr_ham_doc += (pr_word_ham[key]*value)                 \n",
    "            pr_spam_doc = prior_spam + pr_spam_doc\n",
    "            pr_ham_doc = prior_ham + pr_ham_doc\n",
    "            output =  docId + \"\\t\" + true_class + \"\\t\"\n",
    "            predicted_class = 0\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class) == predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output \n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy of the model: 63.00\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.5. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "\n",
    "#####  This question was already solved before the email for ignoring it came."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for key in result:\n",
    "            if key in vocab:\n",
    "                vocab[key] += 1\n",
    "            else:\n",
    "                vocab[key] = 1\n",
    "        output =content[0]+ \"\\t\" + content[1]\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "sum = 0\n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "            if(len(content) > 2 ):\n",
    "                for x in range(2,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    if (not(word in spam_words_freq or word in not_spam_words_freq)):\n",
    "                        unique_word_cnt += 1;\n",
    "                    if (true_class == 1):\n",
    "                        total_spam_words += freq;\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                        if word not in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] = 0\n",
    "                    else:\n",
    "                        total_nonspam_words += freq;\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "                        if word not in spam_words_freq:\n",
    "                            spam_words_freq[word] = 0\n",
    "                            \n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "# Probability of word given email class spam \n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(spam_words_freq[word]+1)/ (total_spam_words + unique_word_cnt)\n",
    "    pr_word_spam[word] = pr        \n",
    "for word in not_spam_words_freq:\n",
    "     # 1 is added for laplace smoothing \n",
    "    pr = (1.0)*(not_spam_words_freq[word]+1)/ (total_nonspam_words + unique_word_cnt)\n",
    "    pr_word_ham[word] = pr\n",
    "\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            if(len(content) > 2 ):\n",
    "                for x in range(2,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham\n",
    "            pr_spam_doc = 0.0\n",
    "            pr_ham_doc = 0.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                pr_spam_doc +=  math.log(pr_word_spam[key])*value\n",
    "                pr_ham_doc += math.log(pr_word_ham[key])*value                 \n",
    "            pr_spam_doc = prior_spam + pr_spam_doc\n",
    "            pr_ham_doc = prior_ham + pr_ham_doc\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            predicted_class = 0\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class) == predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output \n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.6 Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes\n",
    "##### Parts of this question were already solved before the email for ignoring it came"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "alpha = 1\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = \"enronemail_1h.txt\"  \n",
    "train_data = []\n",
    "train_label =[]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        true_class = int(content[1])\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        text = re.sub(r'[\\W]+', ' ', text)\n",
    "#        text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "        train_data.append(text)\n",
    "        train_label.append(true_class)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "text_matrix = count_vectorizer.fit_transform(train_data)\n",
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using our Multinomial classifier\n",
      "sklearn accuracy: 100.00\n",
      "sk learn training error 0.00\n"
     ]
    }
   ],
   "source": [
    "# nb =  MultinomialNB(alpha=alpha)\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(text_matrix, train_label)\n",
    "\n",
    "# Compute accuracy on the test data.\n",
    "print \"Using our Multinomial classifier\"\n",
    "accuracy = nb.score(text_matrix, train_label)*100\n",
    "tr_error = 100-accuracy\n",
    "print 'sklearn accuracy: %3.2f' %accuracy\n",
    "print 'sk learn training error %3.2f' %tr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sklearn's NB classifier\n",
      "sklearn accuracy: 84.00\n",
      "sk learn training error 16.00\n"
     ]
    }
   ],
   "source": [
    "# Compare to sklearn's implementation.\n",
    "print \"Using sklearn's NB classifier\"\n",
    "# clf = BernoulliNB(alpha=alpha)\n",
    "clf = BernoulliNB()\n",
    "clf.fit(text_matrix, train_label)\n",
    "accuracy = clf.score(text_matrix, train_label)*100\n",
    "tr_error = 100 - accuracy\n",
    "print 'sklearn accuracy: %3.2f' %accuracy\n",
    "print 'sk learn training error %3.2f' %tr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
