{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #1=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "DATSCIW261 ASSIGNMENT #1  (version 2016-01-14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hetal Chandaria (hetalchandaria@berkeley.edu)\n",
    "\n",
    "\n",
    "W261 - 2 , Assignment 01\n",
    "\n",
    "\n",
    "Submission Date : 01/18/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.0. \n",
    "#### Define big data. Provide an example of a big data problem in your domain of expertise. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Answer :\n",
    "\n",
    "Big data is a term for data sets that are so large that traditional computing systems are not sufficient for processing them. Imagine trying to run Machine learning algorithms on a training set which is 10GB in size on a traditonal laptop.  \n",
    "\n",
    "Big data is usually characterised by the 4 V's :\n",
    "\n",
    "1. Volume : Amount of data generated  \n",
    "2. Velocity: How frequently is the data being generated \n",
    "3. Variety : Different forms of data\n",
    "4. Veracity : How certain are we about the data\n",
    "\n",
    "Example of big data:\n",
    "\n",
    "Consider eBay as an example. eBay has millions of users browsing through its website searching for, buying and selling products in 100s of categories. Trying to understand users and what their interests can be considered a big data challenge.\n",
    "\n",
    "One can consider tracking the activity of all unique users across all pages/categories/product pages of eBay on all devices. These events would need to be tracked, aggregated on a per user basis and historically tracked to try and create some form of a profile of a user in terms of which categories/products the user is interested in. Additionally, present/future behaviour can be compared to models learnt on past user behaviour. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.0.1.\n",
    "#### In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Let $f(x)$ be the true relationship function such that $ y = f(x) + \\epsilon $, where $\\epsilon $ is normally distributed with a mean of 0. \n",
    "\n",
    "Let g(x) be the estimator of f(x) using polynomial regression. We will fit polynomial regression models of degree 1,2,3,4 and 5 to our test dataset T. \n",
    "\n",
    "To determine bias and variance, we will simulate multiple training sets by bootstrap replicates T’ = {x | x is drawn at random with replacement from T} and |T’| = |T|.\n",
    "\n",
    "\n",
    "Bias : tells us how well the model can fit the true relationship between x and y. Here we will fit the 5 different polynomial regression model we have to the training dataset and determine which model fits the best.\n",
    "\n",
    "In the diagram below ,we can see that for the test data, 3 different models for varying degree being fit. Model with degree 1 underfits the data while model with degree 10 overfits the data. Thus as model complexity increases bias decreases.\n",
    "<img src=\"./images/bias.jpg\">\n",
    "\n",
    "Similary we can determine Variance.\n",
    "Variance : Variance  tell us how different can a specific fit to given dataset be different from one another as we are looking at different datasets. \n",
    "\n",
    "For example, looking at the diagram below, we can see that as model complexity increases ( degree=10) variance increases. For model with degree 3, variance is low.\n",
    "<img src=\"./images/variance.jpg\">\n",
    "\n",
    "Mathematically, expected squared predition error at a point x is ,\n",
    "$Err(x) = E[(y-g(x))]^2$\n",
    "\n",
    "$=E[y^2-2yg(x)+g(x)^2]$\n",
    "\n",
    "$= E[y^2]+E[g(x)^2]-2E[y]E[g(x)]$\n",
    "\n",
    "$=E[(y-f(x))^2]+f(x)^2+E[(g(x)-\\overline{g}(x))^2]+\\overline{g}(x)^2-2f(x)\\overline{g}(x)$\n",
    "\n",
    "$=E[(y-f(x))^2] + E[(g(x)-\\overline{g}(x))^2] + f(x)^2 - 2f(x)\\overline{g}(x) +\\overline{g}(x)^2$\n",
    "\n",
    "$=\\underbrace{E[(y-f(x))^2]} + \\underbrace{E[(g(x)-\\overline{g}(x))^2]} + \\underbrace{(\\overline{g}(x)-f(x))^2}$\n",
    "\n",
    "$=noise   + variance +    bias^2$\n",
    "          \n",
    "\n",
    "Thus the expected prediction error on new data can be used as a quantitative criterion for selecting the best model from a candidate set of estimators. \n",
    "\n",
    "1. Variance of the estimator is defined as $E[(g(x) - E[g(x)])^2]$.\n",
    "\n",
    "2. Bias of the estimator is defined as $[\\overline{g}(x)-f(x)]$. \n",
    "\n",
    "3. Irreducible Error is defined as variance in the data itself.\n",
    "\n",
    "\n",
    "As model complexity increases ( i.e high order polynomials) we tend to find that the estimator fits the data well. Models that are simple, have low variance but high bias. While selecting a model we should pick a model that will have both low bias and variance. As shown in the diagram below, model with degree 3 gives us a good tradeoff between bias and variance.\n",
    "\n",
    "<img src=\"./images/model_complexity.jpg\">\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ \\sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.1. \n",
    "Read through the provided control script (pNaiveBayes.sh)\n",
    "   and all of its comments. When you are comfortable with their\n",
    "   purpose and function, respond to the remaining homework questions below. \n",
    "   A simple cell in the notebook with a print statmement with  a \"done\" string will suffice here. (dont forget to include the Question Number and the quesition in the cell as a multiline comment!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### #### <span style=\"color:red;\">Note</span>  \n",
    "enronemail_1h.txt file had to be fixed to remove mac-style line endings for correct processing. Below are the commands that were run to fix the issue.\n",
    "\n",
    "\n",
    "cat enronemail_1h.txt | tr '^M' '\\n' > ./foo\n",
    "\n",
    "mv foo enronemail_1h.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print 'done'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    "\n",
    "## a test set data of 100 messages\n",
    "data=\"enronemail_1h.txt\" \n",
    "\n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    "\n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    "\n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.2. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Mapper</span> \n",
    "output of mapper is in the format \n",
    "\n",
    "```\n",
    "\"findword\"   \"count\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        #tokenize email and subject\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        count += len(indices)\n",
    "output = findword+\"\\t\"+str(count)\n",
    "print output        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Reducer</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "sum = 0\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            #convert to int and increment the sum\n",
    "            content = re.split(r'\\t+', line)\n",
    "            sum += int(content[1])   \n",
    "print content[0]+\"\\t\"+str(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "! cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.3. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Mapper</span> \n",
    "Output of Mapper is in the format \n",
    "\n",
    "```\n",
    "\"email_id\"   \"class_of_email\"   \"findword\"   \"count_of_findword\"   \"total_numberof_word_for_email\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "findword = sys.argv[2]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #combine email subject and body \n",
    "        text = content[2] + ' ' + content[3]\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #Now find index of each matching instance of the word for that email\n",
    "        #lower is used to do case insensitive search\n",
    "        indices = [i for i,x in enumerate(result) if x.lower() == findword.lower()]\n",
    "        #find out the count of findword in the email\n",
    "        findword_count = len(indices)\n",
    "        #find total number of words in email \n",
    "        total_doc_count = len(result)\n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+ findword\n",
    "        output += \"\\t\" + str(findword_count) + \"\\t\" + str(total_doc_count)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Reducer</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "\n",
    "#Total count of findword in all spam emails \n",
    "total_spam_findword = 0\n",
    "#Total count of findword in all non spam emails \n",
    "total_nonspam_findword = 0\n",
    "\n",
    "#loop through the input files\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            #get the document Id\n",
    "            docId = content[0]\n",
    "            #Get the true class of the email \n",
    "            true_class = int(content[1])\n",
    "            #get the find word and its count\n",
    "            findword = content[2]\n",
    "            findword_freq = int(content[3])\n",
    "            #get total word count for an email\n",
    "            total_doc_word_cnt = int(content[4])\n",
    "            \n",
    "            #if email is spam, increment the spam email count, spam findword count and spam word count\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_findword += findword_freq\n",
    "                total_spam_words += total_doc_word_cnt\n",
    "            #if email is spam, increment the spam email count, spam findword count and spam word count   \n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "                total_nonspam_findword += findword_freq\n",
    "                total_nonspam_words += total_doc_word_cnt\n",
    "\n",
    "#Now calculate prior probabilities for spam and non spam\n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "\n",
    "# Calculate conditional probability of findword given email class spam and non spam\n",
    "pr_findword_spam = math.log((1.0)*(total_spam_findword)/total_spam_words)\n",
    "pr_findword_ham = math.log((1.0)*(total_nonspam_findword)/total_nonspam_words)\n",
    "\n",
    "#below counts are used to calculate accuracy\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "\n",
    "###### Classification ########## \n",
    "#loop through files again \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            #get the findword freq \n",
    "            findword_freq = int(content[3])\n",
    "            # calculate prob for the email being spam & ham\n",
    "            pr_spam_doc = prior_spam + (pr_findword_spam*findword_freq)\n",
    "            pr_ham_doc = prior_ham + (pr_findword_ham*findword_freq)\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            predicted_class =0 \n",
    "            #determine what is the predicted class based on which probability is high \n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class)==predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output\n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 2 assistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy of the model: 60.00\r\n"
     ]
    }
   ],
   "source": [
    "! cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.4. \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh\n",
    "   will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Mapper</span> \n",
    "Output of mapper is in the format\n",
    "\n",
    "```\n",
    "\"email id\"   \"class of email\"   <\"findword\"    \"count of findword\"> repeat this based on how many input findwords are present\n",
    "e.x.\n",
    "123   1   assistance   1   vallium   3   hero   0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "#get the findwords\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "vocab_len = len(findwords)\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #combine email subject and body\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        #tokenize the text\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of findwords and initialise to 0\n",
    "        vocab ={}\n",
    "        for word in findwords:\n",
    "            vocab[word] = 0\n",
    "        #now loop through the email text and get frequency of each count word. \n",
    "        #If findword is not present the frequency will be 0 as we have initialized it above\n",
    "        for key in result:\n",
    "            if key not in findwords:\n",
    "                continue\n",
    "            vocab[key] += 1\n",
    "        #prepare output \n",
    "        output =content[0]+ \"\\t\" + content[1]+\"\\t\"+str(len(result))\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Reducer</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math \n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "\n",
    "#loop through the files\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            #get the doc Id\n",
    "            docId = content[0]\n",
    "            #get the class of the email\n",
    "            true_class = int(content[1])\n",
    "            #get the total doc word count\n",
    "            doc_word_cnt = int(content[2])\n",
    "            \n",
    "            #if email is for spam class increment spam email count, total spam words count\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "                total_spam_words += doc_word_cnt\n",
    "            #if email is for non spam class increment non spam email count, total non spam words count\n",
    "            else:\n",
    "                non_spam_email_cnt += 1\n",
    "                total_nonspam_words += doc_word_cnt\n",
    "            \n",
    "            # if we have more data for email \n",
    "            if(len(content) > 3 ):\n",
    "                # loop through the rest of the data in increments of 2 \n",
    "                for x in range(3,len(content),2):\n",
    "                    #get the findword and its frequency for the email\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    #Determine the class of email and increment the spam word frequency \n",
    "                    # or non spam word frewquency accordingly\n",
    "                    if (true_class == 1):\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                    else:\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "\n",
    "#calculate prior probabilities for spam and ham\n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "\n",
    "# Conditional Probability of findwords given email class  \n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    #if findword frequency is greater than 0 than calculate probability else set the value to smallest possible value\n",
    "    if(spam_words_freq[word] > 0 ):\n",
    "        pr_word_spam[word] = math.log((1.0)*(spam_words_freq[word])/ (total_spam_words))\n",
    "    else:\n",
    "        #setting value to -infinity here for frequency = 0\n",
    "        pr_word_spam[word] = float('-inf')\n",
    "for word in not_spam_words_freq:\n",
    "    if(not_spam_words_freq[word] > 0):\n",
    "        pr_word_ham[word] = math.log((1.0)*(not_spam_words_freq[word])/(total_nonspam_words))\n",
    "    else:\n",
    "        pr_word_ham[word] = float('-inf')\n",
    "\n",
    "#Counts used for accuracy \n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "###### Classification ########## \n",
    "#loop through the files again \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            # Build email vocab for findwords along with their frequency \n",
    "            if(len(content) > 3 ):\n",
    "                for x in range(3,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham for each email \n",
    "            pr_spam_doc = 0.0\n",
    "            pr_ham_doc = 0.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                #if probability of find word is -infinity than check what is the frequency of findword\n",
    "                # if frequency is 0 add 0 to probability else add -infinity . \n",
    "                # We are adding 0 here as infinity x 0 is not defined \n",
    "                if (pr_word_spam[key] == float('-inf')):\n",
    "                    if(value !=0):\n",
    "                        pr_spam_doc += float('-inf')\n",
    "                    else :\n",
    "                        pr_spam_doc += 0\n",
    "                # if probability of findowrd is not infinity than multiply the probability by frequency of find word\n",
    "                else:   \n",
    "                    pr_spam_doc += (pr_word_spam[key]*value)\n",
    "                #follow same rule as spam for calculating probability \n",
    "                if(pr_word_ham[key] == float('-inf')):\n",
    "                    if(value !=0):\n",
    "                        pr_ham_doc += float('-inf')\n",
    "                    else :\n",
    "                        pr_ham_doc += 0\n",
    "                else :\n",
    "                    pr_ham_doc += (pr_word_ham[key]*value) \n",
    "            #add the prior probabilities for spam and ham\n",
    "            pr_spam_doc = prior_spam + pr_spam_doc\n",
    "            pr_ham_doc = prior_ham + pr_ham_doc\n",
    "            output =  docId + \"\\t\" + true_class + \"\\t\"\n",
    "            predicted_class = 0\n",
    "            #determine which probability is higher \n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class) == predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output \n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t0\r\n",
      "0002.2003-12-18.GP\t1\t0\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t0\r\n",
      "0003.2004-08-01.BG\t1\t0\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0004.2004-08-01.BG\t1\t0\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t0\r\n",
      "0005.2003-12-18.GP\t1\t0\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0006.2003-12-18.GP\t1\t0\r\n",
      "0006.2004-08-01.BG\t1\t0\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t0\r\n",
      "0007.2004-08-01.BG\t1\t0\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t0\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t0\r\n",
      "0008.2003-12-18.GP\t1\t0\r\n",
      "0008.2004-08-01.BG\t1\t0\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t0\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t0\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t0\r\n",
      "0011.2003-12-18.GP\t1\t0\r\n",
      "0011.2004-08-01.BG\t1\t0\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t0\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t0\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t0\r\n",
      "0014.2003-12-19.GP\t1\t0\r\n",
      "0014.2004-08-01.BG\t1\t0\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0015.2003-12-19.GP\t1\t0\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t0\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t0\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t0\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t0\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t0\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy of the model: 63.00\r\n"
     ]
    }
   ],
   "source": [
    "! cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.5.  Please ignore \n",
    "Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.\n",
    "\n",
    "#####  This question was already solved before the email for ignoring it came."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Mapper</span> \n",
    "Output of mapper is the format\n",
    "\n",
    "```\n",
    "\"email id\"   \"class\"  <\"word\" \"frequency\"> <repeated based on how many words are in the email>\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = sys.argv[1]\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #combine email subject and body\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        #tokenize text\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for key in result:\n",
    "            if key in vocab:\n",
    "                vocab[key] += 1\n",
    "            else:\n",
    "                vocab[key] = 1\n",
    "        output =content[0]+ \"\\t\" + content[1]\n",
    "        for key, value in vocab.iteritems():\n",
    "            output += \"\\t\" + key + \"\\t\" + str(value)\n",
    "        \n",
    "        print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Reducer</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "sum = 0\n",
    "# Dictionary to store overall frequency of words for spam emails \n",
    "spam_words_freq = {}\n",
    "# Dictionary to store overall frequency of words for non spam emails \n",
    "not_spam_words_freq ={}\n",
    "# Total count of spam emails \n",
    "spam_email_cnt  = 0\n",
    "# Total count of non spam emails \n",
    "non_spam_email_cnt = 0\n",
    "# Unique vocab length\n",
    "unique_word_cnt = 0\n",
    "#Total count of words in all spam emails \n",
    "total_spam_words = 0\n",
    "# Total count of words in all non spam emails \n",
    "total_nonspam_words = 0\n",
    "\n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            #get doc id and class of email\n",
    "            docId = content[0]\n",
    "            true_class = int(content[1])\n",
    "            # based on class of email increment spam / non spam count\n",
    "            if (true_class == 1):\n",
    "                spam_email_cnt += 1\n",
    "            else:\n",
    "                non_spam_email_cnt += 1;\n",
    "            #if email has content \n",
    "            if(len(content) > 2 ):\n",
    "                #loop through rest of mapper output in increments of 2\n",
    "                for x in range(2,len(content),2):\n",
    "                    #get the word and its frequency\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    #Determine unique word count for laplace smoothing \n",
    "                    if (not(word in spam_words_freq or word in not_spam_words_freq)):\n",
    "                        unique_word_cnt += 1;\n",
    "                    #increment spam words, total spam words,ham words, total ham words  based on class of email\n",
    "                    #\n",
    "                    if (true_class == 1):\n",
    "                        total_spam_words += freq;\n",
    "                        if word in spam_words_freq:\n",
    "                            spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            spam_words_freq[word] = freq\n",
    "                        if word not in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] = 0\n",
    "                    else:\n",
    "                        total_nonspam_words += freq;\n",
    "                        if word in not_spam_words_freq:\n",
    "                            not_spam_words_freq[word] += freq\n",
    "                        else:\n",
    "                            not_spam_words_freq[word] = freq\n",
    "                        if word not in spam_words_freq:\n",
    "                            spam_words_freq[word] = 0\n",
    "\n",
    "#Calculate prior probabilites for spam and ham\n",
    "prior_spam = math.log((1.0)*spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "prior_ham = math.log((1.0)*non_spam_email_cnt / (spam_email_cnt + non_spam_email_cnt ))\n",
    "# Condirtional Probability of word given email class spam and ham\n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in spam_words_freq:\n",
    "    # 1 is added for laplace smoothing \n",
    "    pr_word_spam[word] = pr = math.log((1.0)*(spam_words_freq[word]+1)/ (total_spam_words + unique_word_cnt))\n",
    "            \n",
    "for word in not_spam_words_freq:\n",
    "     # 1 is added for laplace smoothing \n",
    "    pr_word_ham[word] = math.log((1.0)*(not_spam_words_freq[word]+1)/ (total_nonspam_words + unique_word_cnt))\n",
    "\n",
    "#counts for accuracy\n",
    "correct_match_cnt = 0\n",
    "total_match = 0\n",
    "\n",
    "###### Classification ########## \n",
    "for x in range(1,len(sys.argv)):\n",
    "    with open (sys.argv[x], \"r\") as myfile:\n",
    "        for line in myfile.readlines():\n",
    "            # Split the line by <TAB> delimiter\n",
    "            content = re.split(r'\\t+', line)\n",
    "            docId = content[0]\n",
    "            true_class = content[1]\n",
    "            doc_vocab = {}\n",
    "            if(len(content) > 2 ):\n",
    "                for x in range(2,len(content),2):\n",
    "                    word = content[x]\n",
    "                    freq = int(content[x+1])\n",
    "                    doc_vocab[word] = freq\n",
    "            # calculate prob for spam , ham for each email \n",
    "            pr_spam_doc = 0.0\n",
    "            pr_ham_doc = 0.0\n",
    "            for key,value in doc_vocab.iteritems():\n",
    "                pr_spam_doc +=  (pr_word_spam[key]*value)\n",
    "                pr_ham_doc += (pr_word_ham[key]*value)\n",
    "            #now add prior probabilities for spam and ham\n",
    "            pr_spam_doc = prior_spam + pr_spam_doc\n",
    "            pr_ham_doc = prior_ham + pr_ham_doc\n",
    "            output =  docId+\"\\t\"+true_class+\"\\t\"\n",
    "            predicted_class = 0\n",
    "            #Determine the predicted class\n",
    "            if(pr_spam_doc > pr_ham_doc) :\n",
    "                predicted_class = 1\n",
    "                output += \"1\"\n",
    "            else:\n",
    "                output += \"0\"\n",
    "            if(int(true_class) == predicted_class):\n",
    "                correct_match_cnt += 1\n",
    "            total_match += 1\n",
    "            print output \n",
    "print \"Accuracy of the model: %3.2f\" %(correct_match_cnt*100.0/total_match) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!./pNaiveBayes.sh 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t0\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t1\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accuracy of the model: 100.00\r\n"
     ]
    }
   ],
   "source": [
    "! cat enronemail_1h.txt.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW1.6 Please ignore\n",
    "Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes\n",
    "##### Parts of this question were already solved before the email for ignoring it came"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import re\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "alpha = 1\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "filename = \"enronemail_1h.txt\"  \n",
    "train_data = []\n",
    "train_label =[]\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        #Tokenize each line\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        true_class = int(content[1])\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        text = re.sub(r'[\\W]+', ' ', text)\n",
    "#        text = re.sub('[^0-9a-zA-Z]+', ' ', text)\n",
    "        train_data.append(text)\n",
    "        train_label.append(true_class)\n",
    "\n",
    "count_vectorizer = CountVectorizer()\n",
    "text_matrix = count_vectorizer.fit_transform(train_data)\n",
    "feature_names = count_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using our Multinomial classifier\n",
      "sklearn accuracy: 100.00\n",
      "sk learn training error 0.00\n"
     ]
    }
   ],
   "source": [
    "# nb =  MultinomialNB(alpha=alpha)\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(text_matrix, train_label)\n",
    "\n",
    "# Compute accuracy on the test data.\n",
    "print \"Using our Multinomial classifier\"\n",
    "accuracy = nb.score(text_matrix, train_label)*100\n",
    "tr_error = 100-accuracy\n",
    "print 'sklearn accuracy: %3.2f' %accuracy\n",
    "print 'sk learn training error %3.2f' %tr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sklearn's NB classifier\n",
      "sklearn accuracy: 84.00\n",
      "sk learn training error 16.00\n"
     ]
    }
   ],
   "source": [
    "# Compare to sklearn's implementation.\n",
    "print \"Using sklearn's NB classifier\"\n",
    "# clf = BernoulliNB(alpha=alpha)\n",
    "clf = BernoulliNB()\n",
    "clf.fit(text_matrix, train_label)\n",
    "accuracy = clf.score(text_matrix, train_label)*100\n",
    "tr_error = 100 - accuracy\n",
    "print 'sklearn accuracy: %3.2f' %accuracy\n",
    "print 'sk learn training error %3.2f' %tr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "— Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
