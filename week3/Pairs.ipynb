{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#DATASCI W261: Machine Learning at Scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#This notebook shows a Hadoop MapReduce job for counting paris of words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Creat four documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc1.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile doc1.txt\n",
    "A B C A C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc2.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile doc2.txt\n",
    "D A C B E D E A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc3.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile doc3.txt\n",
    "B A C E A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting doc4.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile doc4.txt\n",
    "D D B A C"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = line.split()\n",
    "    # increase counters\n",
    "    for word1 in words:\n",
    "        for word2 in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        #\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "            if word1!=word2:\n",
    "                print '%s\\t%s\\t%s' % (word1, word2, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Reducer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "current_words = None\n",
    "current_count = 0\n",
    "word = None\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word1, word2, count = line.split('\\t')\n",
    "    # convert count (currently a string) to int\n",
    "    words = (word1, word2)\n",
    "    try:\n",
    "        count = int(count)\n",
    "    except ValueError:\n",
    "        # count was not a number, so silently\n",
    "        # ignore/discard this line\n",
    "        continue\n",
    "    # this IF-switch only works because Hadoop sorts map output\n",
    "    # by key (here: word) before it is passed to the reducer\n",
    "    if current_words == words:\n",
    "        current_count += count\n",
    "    else:\n",
    "        if current_words:\n",
    "            # write result to STDOUT\n",
    "            print '%s\\t%s' % (current_words, current_count)\n",
    "        current_count = count\n",
    "        current_words = words\n",
    "\n",
    "# do not forget to output the last word if needed!\n",
    "if current_words == words:\n",
    "    print '%s\\t%s' % (current_words, current_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Run the code in hadoop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###start yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-liang-resourcemanager-ldai.out\n",
      "localhost: starting nodemanager, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/yarn-liang-nodemanager-ldai.out\n",
      "15/08/21 08:20:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-liang-namenode-ldai.out\n",
      "localhost: starting datanode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-liang-datanode-ldai.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /usr/local/Cellar/hadoop/2.6.0/libexec/logs/hadoop-liang-secondarynamenode-ldai.out\n",
      "15/08/21 08:20:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/start-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/start-dfs.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###create a new folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/08/21 08:23:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -mkdir pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###upload files to hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/08/21 08:23:08 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -put doc*.txt pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hadoop streaming command"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<pre>\n",
    "hadoop jar hadoopstreamingjarfile \\\n",
    "    -D stream.num.map.output.key.fields=n \\\n",
    "    -mapper mapperfile \\\n",
    "    -reducer reducerfile \\\n",
    "    -input inputfile \\\n",
    "    -output outputfile</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**By default, stream.num.map.output.key.fields is 1, which means the data of the first column is the key. In pairs example, the first two columns are the key. so stream.num.map.output.key.fields should be 2.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hadoop streaming jar file can be found in your hadoop folder or downloaded from\n",
    "http://mvnrepository.com/artifact/org.apache.hadoop/hadoop-streaming/2.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/08/21 08:23:13 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/08/21 08:23:15 INFO Configuration.deprecation: session.id is deprecated. Instead, use dfs.metrics.session-id\n",
      "15/08/21 08:23:15 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=\n",
      "15/08/21 08:23:15 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized\n",
      "15/08/21 08:23:15 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "15/08/21 08:23:15 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "15/08/21 08:23:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_local705544776_0001\n",
      "15/08/21 08:23:16 INFO mapreduce.Job: The url to track the job: http://localhost:8080/\n",
      "15/08/21 08:23:16 INFO mapred.LocalJobRunner: OutputCommitter set in config null\n",
      "15/08/21 08:23:16 INFO mapreduce.Job: Running job: job_local705544776_0001\n",
      "15/08/21 08:23:16 INFO mapred.LocalJobRunner: OutputCommitter is org.apache.hadoop.mapred.FileOutputCommitter\n",
      "15/08/21 08:23:16 INFO mapred.LocalJobRunner: Waiting for map tasks\n",
      "15/08/21 08:23:16 INFO mapred.LocalJobRunner: Starting task: attempt_local705544776_0001_m_000000_0\n",
      "15/08/21 08:23:16 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/08/21 08:23:16 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/liang/pairs/doc2.txt:0+15\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/08/21 08:23:17 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/08/21 08:23:17 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/liang/Documents/PythonNotebook/hadoop/./mapper.py]\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.local.dir is deprecated. Instead, use mapreduce.cluster.local.dir\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: map.input.file is deprecated. Instead, use mapreduce.map.input.file\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: map.input.length is deprecated. Instead, use mapreduce.map.input.length\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.work.output.dir is deprecated. Instead, use mapreduce.task.output.dir\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: map.input.start is deprecated. Instead, use mapreduce.map.input.start\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "15/08/21 08:23:17 INFO Configuration.deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition\n",
      "15/08/21 08:23:17 INFO mapreduce.Job: Job job_local705544776_0001 running in uber mode : false\n",
      "15/08/21 08:23:17 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: \n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Starting flush of map output\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Spilling map output\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: bufstart = 0; bufend = 300; bufvoid = 104857600\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214200(104856800); length = 197/6553600\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Finished spill 0\n",
      "15/08/21 08:23:18 INFO mapred.Task: Task:attempt_local705544776_0001_m_000000_0 is done. And is in the process of committing\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "15/08/21 08:23:18 INFO mapred.Task: Task 'attempt_local705544776_0001_m_000000_0' done.\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local705544776_0001_m_000000_0\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: Starting task: attempt_local705544776_0001_m_000001_0\n",
      "15/08/21 08:23:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/08/21 08:23:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/liang/pairs/doc1.txt:0+9\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/liang/Documents/PythonNotebook/hadoop/./mapper.py]\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "15/08/21 08:23:18 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: \n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Starting flush of map output\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Spilling map output\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: bufstart = 0; bufend = 96; bufvoid = 104857600\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214336(104857344); length = 61/6553600\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Finished spill 0\n",
      "15/08/21 08:23:18 INFO mapred.Task: Task:attempt_local705544776_0001_m_000001_0 is done. And is in the process of committing\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "15/08/21 08:23:18 INFO mapred.Task: Task 'attempt_local705544776_0001_m_000001_0' done.\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: Finishing task: attempt_local705544776_0001_m_000001_0\n",
      "15/08/21 08:23:18 INFO mapred.LocalJobRunner: Starting task: attempt_local705544776_0001_m_000002_0\n",
      "15/08/21 08:23:18 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/08/21 08:23:18 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/liang/pairs/doc3.txt:0+9\n",
      "15/08/21 08:23:18 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "15/08/21 08:23:18 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/liang/Documents/PythonNotebook/hadoop/./mapper.py]\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: \n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Starting flush of map output\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Spilling map output\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: bufstart = 0; bufend = 108; bufvoid = 104857600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214328(104857312); length = 69/6553600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Finished spill 0\n",
      "15/08/21 08:23:19 INFO mapred.Task: Task:attempt_local705544776_0001_m_000002_0 is done. And is in the process of committing\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "15/08/21 08:23:19 INFO mapred.Task: Task 'attempt_local705544776_0001_m_000002_0' done.\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local705544776_0001_m_000002_0\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: Starting task: attempt_local705544776_0001_m_000003_0\n",
      "15/08/21 08:23:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/08/21 08:23:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Processing split: hdfs://localhost:9000/user/liang/pairs/doc4.txt:0+9\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: numReduceTasks: 1\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: (EQUATOR) 0 kvi 26214396(104857584)\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: mapreduce.task.io.sort.mb: 100\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: soft limit at 83886080\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: bufstart = 0; bufvoid = 104857600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: kvstart = 26214396; length = 6553600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/liang/Documents/PythonNotebook/hadoop/./mapper.py]\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: Records R/W=1/1\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: \n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Starting flush of map output\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Spilling map output\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: bufstart = 0; bufend = 108; bufvoid = 104857600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: kvstart = 26214396(104857584); kvend = 26214328(104857312); length = 69/6553600\n",
      "15/08/21 08:23:19 INFO mapred.MapTask: Finished spill 0\n",
      "15/08/21 08:23:19 INFO mapred.Task: Task:attempt_local705544776_0001_m_000003_0 is done. And is in the process of committing\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: Records R/W=1/1\n",
      "15/08/21 08:23:19 INFO mapred.Task: Task 'attempt_local705544776_0001_m_000003_0' done.\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: Finishing task: attempt_local705544776_0001_m_000003_0\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: map task executor complete.\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: Waiting for reduce tasks\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: Starting task: attempt_local705544776_0001_r_000000_0\n",
      "15/08/21 08:23:19 INFO util.ProcfsBasedProcessTree: ProcfsBasedProcessTree currently is supported only on Linux.\n",
      "15/08/21 08:23:19 INFO mapred.Task:  Using ResourceCalculatorProcessTree : null\n",
      "15/08/21 08:23:19 INFO mapred.ReduceTask: Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@f84b5e9\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: MergerManager: memoryLimit=338375456, maxSingleShuffleLimit=84593864, mergeThreshold=223327808, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "15/08/21 08:23:19 INFO reduce.EventFetcher: attempt_local705544776_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "15/08/21 08:23:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local705544776_0001_m_000001_0 decomp: 130 len: 134 to MEMORY\n",
      "15/08/21 08:23:19 INFO reduce.InMemoryMapOutput: Read 130 bytes from map-output for attempt_local705544776_0001_m_000001_0\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 130, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->130\n",
      "15/08/21 08:23:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local705544776_0001_m_000002_0 decomp: 146 len: 150 to MEMORY\n",
      "15/08/21 08:23:19 INFO reduce.InMemoryMapOutput: Read 146 bytes from map-output for attempt_local705544776_0001_m_000002_0\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 146, inMemoryMapOutputs.size() -> 2, commitMemory -> 130, usedMemory ->276\n",
      "15/08/21 08:23:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local705544776_0001_m_000003_0 decomp: 146 len: 150 to MEMORY\n",
      "15/08/21 08:23:19 INFO reduce.InMemoryMapOutput: Read 146 bytes from map-output for attempt_local705544776_0001_m_000003_0\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 146, inMemoryMapOutputs.size() -> 3, commitMemory -> 276, usedMemory ->422\n",
      "15/08/21 08:23:19 INFO reduce.LocalFetcher: localfetcher#1 about to shuffle output of map attempt_local705544776_0001_m_000000_0 decomp: 402 len: 406 to MEMORY\n",
      "15/08/21 08:23:19 INFO reduce.InMemoryMapOutput: Read 402 bytes from map-output for attempt_local705544776_0001_m_000000_0\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: closeInMemoryFile -> map-output of size: 402, inMemoryMapOutputs.size() -> 4, commitMemory -> 422, usedMemory ->824\n",
      "15/08/21 08:23:19 INFO reduce.EventFetcher: EventFetcher is interrupted.. Returning\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: finalMerge called with 4 in-memory map-outputs and 0 on-disk map-outputs\n",
      "15/08/21 08:23:19 INFO mapred.Merger: Merging 4 sorted segments\n",
      "15/08/21 08:23:19 INFO mapred.Merger: Down to the last merge-pass, with 4 segments left of total size: 800 bytes\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: Merged 4 segments, 824 bytes to disk to satisfy reduce memory limit\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: Merging 1 files, 822 bytes from disk\n",
      "15/08/21 08:23:19 INFO reduce.MergeManagerImpl: Merging 0 segments, 0 bytes from memory into reduce\n",
      "15/08/21 08:23:19 INFO mapred.Merger: Merging 1 sorted segments\n",
      "15/08/21 08:23:19 INFO mapred.Merger: Down to the last merge-pass, with 1 segments left of total size: 812 bytes\n",
      "15/08/21 08:23:19 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: PipeMapRed exec [/Users/liang/Documents/PythonNotebook/hadoop/./reducer.py]\n",
      "15/08/21 08:23:19 INFO Configuration.deprecation: mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "15/08/21 08:23:19 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: R/W/S=1/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: R/W/S=10/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: R/W/S=100/0/0 in:NA [rec/s] out:NA [rec/s]\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: MRErrorThread done\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: Records R/W=102/1\n",
      "15/08/21 08:23:19 INFO streaming.PipeMapRed: mapRedFinished\n",
      "15/08/21 08:23:20 INFO mapred.Task: Task:attempt_local705544776_0001_r_000000_0 is done. And is in the process of committing\n",
      "15/08/21 08:23:20 INFO mapred.LocalJobRunner: 4 / 4 copied.\n",
      "15/08/21 08:23:20 INFO mapred.Task: Task attempt_local705544776_0001_r_000000_0 is allowed to commit now\n",
      "15/08/21 08:23:20 INFO output.FileOutputCommitter: Saved output of task 'attempt_local705544776_0001_r_000000_0' to hdfs://localhost:9000/user/liang/pairsoutput/_temporary/0/task_local705544776_0001_r_000000\n",
      "15/08/21 08:23:20 INFO mapred.LocalJobRunner: Records R/W=102/1 > reduce\n",
      "15/08/21 08:23:20 INFO mapred.Task: Task 'attempt_local705544776_0001_r_000000_0' done.\n",
      "15/08/21 08:23:20 INFO mapred.LocalJobRunner: Finishing task: attempt_local705544776_0001_r_000000_0\n",
      "15/08/21 08:23:20 INFO mapred.LocalJobRunner: reduce task executor complete.\n",
      "15/08/21 08:23:20 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "15/08/21 08:23:20 INFO mapreduce.Job: Job job_local705544776_0001 completed successfully\n",
      "15/08/21 08:23:20 INFO mapreduce.Job: Counters: 35\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=532860\n",
      "\t\tFILE: Number of bytes written=1866051\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=156\n",
      "\t\tHDFS: Number of bytes written=260\n",
      "\t\tHDFS: Number of read operations=51\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=7\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=4\n",
      "\t\tMap output records=102\n",
      "\t\tMap output bytes=612\n",
      "\t\tMap output materialized bytes=840\n",
      "\t\tInput split bytes=396\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=20\n",
      "\t\tReduce shuffle bytes=840\n",
      "\t\tReduce input records=102\n",
      "\t\tReduce output records=20\n",
      "\t\tSpilled Records=204\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=45\n",
      "\t\tTotal committed heap usage (bytes)=1941438464\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=42\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=260\n",
      "15/08/21 08:23:20 INFO streaming.StreamJob: Output directory: pairsoutput\n"
     ]
    }
   ],
   "source": [
    "!hadoop jar hadoop-*streaming*.jar -D stream.num.map.output.key.fields=2 -mapper mapper.py -reducer reducer.py -input pairs -output pairsoutput"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###show the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/08/21 08:23:22 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "('A', 'B')\t7\n",
      "('A', 'C')\t9\n",
      "('A', 'D')\t6\n",
      "('A', 'E')\t6\n",
      "('B', 'A')\t7\n",
      "('B', 'C')\t5\n",
      "('B', 'D')\t4\n",
      "('B', 'E')\t3\n",
      "('C', 'A')\t9\n",
      "('C', 'B')\t5\n",
      "('C', 'D')\t4\n",
      "('C', 'E')\t3\n",
      "('D', 'A')\t6\n",
      "('D', 'B')\t4\n",
      "('D', 'C')\t4\n",
      "('D', 'E')\t4\n",
      "('E', 'A')\t6\n",
      "('E', 'B')\t3\n",
      "('E', 'C')\t3\n",
      "('E', 'D')\t4\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat pairsoutput/part-00000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Remove the folder created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/08/21 08:23:26 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/08/21 08:23:27 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted pairsoutput\n",
      "15/08/21 08:23:29 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "15/08/21 08:23:30 INFO fs.TrashPolicyDefault: Namenode trash configuration: Deletion interval = 0 minutes, Emptier interval = 0 minutes.\n",
      "Deleted pairs\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r pairsoutput\n",
    "!hdfs dfs -rm -r pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###stop yarn and hdfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stopping yarn daemons\n",
      "stopping resourcemanager\n",
      "localhost: stopping nodemanager\n",
      "no proxyserver to stop\n",
      "15/08/21 08:23:43 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Stopping namenodes on [localhost]\n",
      "localhost: stopping namenode\n",
      "localhost: stopping datanode\n",
      "Stopping secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: stopping secondarynamenode\n",
      "15/08/21 08:24:04 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-yarn.sh\n",
    "!/usr/local/Cellar/hadoop/2.6.0/sbin/stop-dfs.sh"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
