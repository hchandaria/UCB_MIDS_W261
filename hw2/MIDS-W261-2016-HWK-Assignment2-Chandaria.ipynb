{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====DATSCIW261 ASSIGNMENT #2=====\n",
    "\n",
    "MIDS UC Berkeley, Machine Learning at Scale\n",
    "\n",
    "DATSCIW261 ASSIGNMENT #2\n",
    "\n",
    "Version 2016-01-22"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hetal Chandaria (hetalchandaria@berkeley.edu)\n",
    "\n",
    "\n",
    "W261 - 2 , Assignment 01\n",
    "\n",
    "\n",
    "Submission Date : 01/18/2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.0.  \n",
    "What is a race condition in the context of parallel computation? Give an example.\n",
    "What is MapReduce?\n",
    "How does it differ from Hadoop?\n",
    "Which programming paradigm is Hadoop based on? Explain and give a simple example in code and show the code running."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Race Condition \n",
    "A race condition is a bug which occurs when two or more threads are trying to access the same shared memory location and update it , and the final result depends on the execution sequence of those threads.\n",
    "\n",
    "#### Race Condition example \n",
    "Consider a shared variable X\n",
    "1. Thread A \n",
    "\n",
    "    a. Read Variable X\n",
    "    \n",
    "    b. Compute X*2\n",
    "    \n",
    "    c. Assign to X\n",
    "    \n",
    "\n",
    "2. Thread B\n",
    "\n",
    "    a. Read Variable X\n",
    "\n",
    "    b. Compute X*2\n",
    "\n",
    "    c. Assign to X\n",
    "    \n",
    "Depending on the order of the threads the final value of X can be different\n",
    "\n",
    "1. A & B both read the value of X and final of X is same\n",
    "Final Result : X * 2\n",
    "\n",
    "2. A reads and update X first and than B reads and updates X. \n",
    "Final Result : X * 2 * 2\n",
    "\n",
    "\n",
    "#### Map Reduce \n",
    "MapReduce is a framework processing parallelizable problems across huge data sets, using a large number of computers (nodes); cluster or grid. User specify a map fnction that processes a key/value pair to generate intermediate key/value pairs and a reduce function that merges all values associated with the same intermediate key. \n",
    "\n",
    "Mapreduce is a programming model while the mapreduce library inside Hadoop is an implementation of MapReduce programming framework.\n",
    "\n",
    "Hadoop is built on the mapreduce and distributed file system concepts pioneered by Google.\n",
    "\n",
    "Below is a simple map reduce program that takes input a string of words and counts the frequency of each word. We sort the output before sending it to the reducer ( to simulate the hadoop mapreduce functionality). Mapper counts the frequency of each word and emits the output as < word, value> and the reducer generates the final frequency for each word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "import sys\n",
    "import re\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "    # split the line into words\n",
    "    words = re.findall(WORD_RE,line)\n",
    "    # increase counters\n",
    "    for key in words:\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        # tab-delimited; the trivial word count is 1\n",
    "        value = 1\n",
    "        print '%s\\t%s' % (key, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "last_key = None\n",
    "word = None\n",
    "total_count = 0\n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    word, count = line.split('\\t', 1)\n",
    "    \n",
    "    #if current key is same as last_key than increment count \n",
    "    if(last_key == word):\n",
    "        total_count += int(count)\n",
    "    else:\n",
    "        if (last_key):\n",
    "            print '%s\\t%s' %(last_key,total_count)\n",
    "        total_count = int(count)\n",
    "        last_key = word\n",
    "if last_key == word:\n",
    "    print '%s\\t%s' %(last_key,total_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "be\t1\r\n",
      "does\t2\r\n",
      "even\t1\r\n",
      "it\t1\r\n",
      "may\t1\r\n",
      "this\t1\r\n",
      "work\t2\r\n"
     ]
    }
   ],
   "source": [
    "!echo \"does this even work may be it does work\" | python mapper.py | sort -k1,1 | python reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 21:36:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `hw2': File exists\n",
      "16/01/25 21:36:19 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `hw2/src': File exists\n",
      "16/01/25 21:36:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "mkdir: `hw2/output': File exists\n"
     ]
    }
   ],
   "source": [
    "#### Create directories in HDFS for below jobs\n",
    "!hdfs dfs -mkdir hw2\n",
    "!hdfs dfs -mkdir hw2/src\n",
    "!hdfs dfs -mkdir hw2/output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.1. \n",
    "Sort in Hadoop MapReduce\n",
    "\n",
    "Given as input: Records of the form $<integer, “NA”>$, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form $<integer, “NA”>$ in decreasing order; what happens if you have multiple reducers? Do you need additional steps? Explain.\n",
    "\n",
    "Write code to generate N  random records of the form $<integer, “NA”>$. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Number Generator</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "N = 10000\n",
    "numbers = np.random.randint(N, size=N)\n",
    "f = open('numbers_input.txt', 'w')\n",
    "for number in numbers:\n",
    "    s = str(number)+\",\"+\"NA\"+\"\\n\"\n",
    "    f.write(s)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/23 22:06:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/src/numbers_input.txt\n",
      "16/01/23 22:06:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#move input file into hdfs\n",
    "!hdfs dfs -rm hw2/src/numbers_input.txt\n",
    "!hdfs dfs -put numbers_input.txt hw2/src/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Mapper for sort</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "        \n",
    "def main():\n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        print line\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        num = re.split(',',line)\n",
    "        print num[0]\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py; chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Function for running the hadoop job\n",
    "\n",
    "##### Note on descending sorting of mapper\n",
    "\n",
    "Hadoop has a library class, KeyFieldBasedComparator, that is useful for many applications. This class provides a subset of features provided by the Unix/GNU Sort.\n",
    "\n",
    "We have added below 2 options to the hadoop command\n",
    "-D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator should be added to streaming command\n",
    "\n",
    "-D mapred.text.key.comparator.options=-nr\n",
    "\n",
    "Here, -n specifies that the sorting is numerical sorting and -r specifies that the result should be reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:03:28 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/SortOutput\n",
      "16/01/25 22:03:30 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 22:03:30 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "\n",
      "Top 10 biggest number\n",
      "16/01/25 22:03:33 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "9999\t\n",
      "9999\t\n",
      "9998\t\n",
      "9998\t\n",
      "9993\t\n",
      "9989\t\n",
      "9988\t\n",
      "9988\t\n",
      "9987\t\n",
      "9987\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "\n",
      "\n",
      "Top 10 smallest number\n",
      "16/01/25 22:03:34 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "8\t\n",
      "7\t\n",
      "7\t\n",
      "5\t\n",
      "5\t\n",
      "3\t\n",
      "1\t\n",
      "1\t\n",
      "0\t\n",
      "0\t\n"
     ]
    }
   ],
   "source": [
    "def hw2_1():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/SortOutput\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.text.key.comparator.options=-nr \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw2/src/numbers_input.txt \\\n",
    "    -output hw2/output/SortOutput \\\n",
    "    -jobconf mapred.map.tasks=10 \\\n",
    "    -jobconf mapred.reduce.tasks=1 \\\n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"Top 10 biggest number\"\n",
    "    !hdfs dfs -cat hw2/output/SortOutput/part* | head -10\n",
    "    print \"\\n\\n\"\n",
    "    !echo \"Top 10 smallest number\"\n",
    "    !hdfs dfs -cat hw2/output/SortOutput/part* | tail -10\n",
    "\n",
    "\n",
    "hw2_1()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "########### \n",
    "\n",
    "\n",
    "If we run the Job using multiple reducers than we need to do a final merge sort on the combined output of all reducers to get a final single ordered output. Another option would be to use a custom partioner if the data range is known and partition it such that subsets of the data range are passed to a given reducer. This can be used for ensuring the reducers generate their output into a defined totally sorted output.\n",
    "\n",
    "The code below runs using 2 reducers. In this case there are two output files which are individually sorted but need a final merge sort. A simple option for doing this sort would be to run a second MapReduce job with a single reducer to sort the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:03:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/SortOutput2\n",
      "16/01/25 22:03:40 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 22:03:41 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def hw2_1_2():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/SortOutput2\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.text.key.comparator.options=-nr \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw2/src/numbers_input.txt \\\n",
    "    -output hw2/output/SortOutput2 \\\n",
    "    -jobconf mapred.map.tasks=10 \\\n",
    "    -jobconf mapred.reduce.tasks=2 \\\n",
    "    \n",
    "hw2_1_2()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import re\n",
    "\n",
    "\n",
    "def main():\n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        print line\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 529,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:03:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/SortOutput3\n",
      "16/01/25 22:03:50 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 22:03:50 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "\n",
      "Top 10 biggest number\n",
      "16/01/25 22:03:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "9999\t\n",
      "9999\t\n",
      "9998\t\n",
      "9998\t\n",
      "9993\t\n",
      "9989\t\n",
      "9988\t\n",
      "9988\t\n",
      "9987\t\n",
      "9987\t\n",
      "cat: Unable to write to output stream.\n",
      "\n",
      "\n",
      "\n",
      "Top 10 smallest number\n",
      "16/01/25 22:03:54 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "8\t\n",
      "7\t\n",
      "7\t\n",
      "5\t\n",
      "5\t\n",
      "3\t\n",
      "1\t\n",
      "1\t\n",
      "0\t\n",
      "0\t\n"
     ]
    }
   ],
   "source": [
    "#Rerunning the above job but with only reducer will do the job \n",
    "def hw2_1_3():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/SortOutput3\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -D mapred.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "    -D mapred.text.key.comparator.options=-nr \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw2/output/SortOutput2/ \\\n",
    "    -output hw2/output/SortOutput3 \\\n",
    "    -jobconf mapred.map.tasks=10 \\\n",
    "    -jobconf mapred.reduce.tasks=1 \n",
    "    \n",
    "    print \"\\n\"\n",
    "    !echo \"Top 10 biggest number\"\n",
    "    !hdfs dfs -cat hw2/output/SortOutput3/part-00000 | head\n",
    "    print \"\\n\\n\"\n",
    "    !echo \"Top 10 smallest number\"\n",
    "    !hdfs dfs -cat hw2/output/SortOutput3/part-00000 | tail\n",
    "    \n",
    "hw2_1_3()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.  \n",
    "WORDCOUNT\n",
    "\n",
    "Using the Enron data from HW1 and Hadoop MapReduce streaming, write the mapper/reducer job that  will determine the word count (number of occurrences) of each white-space delimitted token (assume spaces, fullstops, comma as delimiters). Examine the word “assistance” and report its word count results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 530,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:04:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/src/enronemail_1h.txt\n",
      "16/01/25 22:05:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#move input file into hdfs\n",
    "!hdfs dfs -rm hw2/src/enronemail_1h.txt\n",
    "!hdfs dfs -put enronemail_1h.txt hw2/src/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "       \n",
    "def main():\n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #Combine email subject and body\n",
    "        text = content[2] + ' ' + content[3]\n",
    "        #tokenize email and subject\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        # write the results to STDOUT (standard output);\n",
    "        # what we output here will be the input for the\n",
    "        # Reduce step, i.e. the input for reducer.py\n",
    "        for key in result:\n",
    "            print '%s\\t%s' % (key.lower(), 1)\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "\n",
    "def main():\n",
    "    last_key = None\n",
    "    word = None\n",
    "    total_count = 0\n",
    "    # input comes from STDIN (standard input)\n",
    "    for line in sys.stdin:\n",
    "        # remove leading and trailing whitespace\n",
    "        line = line.strip()\n",
    "\n",
    "        # parse the input we got from mapper.py\n",
    "        word, count = line.split('\\t', 1)\n",
    "\n",
    "        #if current key is same as last_key than increment count \n",
    "        if(last_key == word):\n",
    "            total_count += int(count)\n",
    "        else:\n",
    "            if (last_key):\n",
    "                # write result to STDOUT\n",
    "                print '%s\\t%s' %(last_key,total_count)\n",
    "            total_count = int(count)\n",
    "            last_key = word\n",
    "    if last_key == word:\n",
    "        print '%s\\t%s' %(last_key,total_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:05:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/WordCountOutput\n",
      "16/01/25 22:05:17 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def hw2_2():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/WordCountOutput\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -mapper mapper.py \\\n",
    "    -reducer reducer.py \\\n",
    "    -input hw2/src/enronemail_1h.txt \\\n",
    "    -output hw2/output/WordCountOutput \n",
    "    \n",
    "\n",
    "hw2_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:05:23 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat hw2/output/WordCountOutput/part* > WC_22.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 23:35:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Frequency of :assistance\n",
      "            key  frequency\n",
      "728  assistance         10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hw22_output(findword):\n",
    "    #move the output file from HDFS to local file system\n",
    "    !hdfs dfs -cat hw2/output/WordCountOutput/part* > WC_22.txt\n",
    "    df = pd.read_csv(\"WC_22.txt\",sep='\\t',header=None)\n",
    "    df.columns = ['key', 'frequency']\n",
    "    frequency = df[df.key == findword]\n",
    "    print \"Frequency of :\"+findword\n",
    "    print frequency\n",
    "    \n",
    "hw22_output('assistance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.2.1  \n",
    "Using Hadoop MapReduce and your wordcount job (from HW2.2) determine the top-10 occurring tokens (most frequent tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 23:35:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Top 10 fequently occuring tokens\n",
      "       key  frequency\n",
      "2182   for        366\n",
      "1801   ect        382\n",
      "5412  your        391\n",
      "2649    in        411\n",
      "5405   you        432\n",
      "413      a        531\n",
      "3497    of        554\n",
      "614    and        638\n",
      "4934    to        954\n",
      "4863   the       1225\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def hw221_output():\n",
    "    #move the output file from HDFS to local file system\n",
    "    !hdfs dfs -cat hw2/output/WordCountOutput/part* > WC_22.txt\n",
    "    df = pd.read_csv(\"WC_22.txt\",sep='\\t',header=None)\n",
    "    df.columns = ['key', 'frequency']\n",
    "    df=df.sort_values(['frequency'])\n",
    "    print \"Top 10 fequently occuring tokens\"\n",
    "    print df.tail(10)\n",
    "    \n",
    "hw221_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "\n",
    "HW2.3. Multinomial NAIVE BAYES with NO Smoothing\n",
    "Using the Enron data from HW1 and Hadoop MapReduce, write  a mapper/reducer job(s) that\n",
    "   will both learn  Naive Bayes classifier and classify the Enron email messages using the learnt Naive Bayes classifier. Use all white-space delimitted tokens as independent input variables (assume spaces, fullstops, commas as delimiters). Note: for multinomial Naive Bayes, the Pr(X=“assistance”|Y=SPAM) is calculated as follows:\n",
    "\n",
    "   the number of times “assistance” occurs in SPAM labeled documents / the number of words in documents labeled SPAM \n",
    "\n",
    "   E.g.,   “assistance” occurs 5 times in all of the documents Labeled SPAM, and the length in terms of the number of words in all documents labeled as SPAM (when concatenated) is 1,000. Then Pr(X=“assistance”|Y=SPAM) = 5/1000. Note this is a multinomial estimation of the class conditional for a Naive Bayes Classifier. No smoothing is needed in this HW. Multiplying lots of probabilities, which are between 0 and 1, can result in floating-point underflow. Since log(xy) = log(x) + log(y), it is better to perform all computations by summing logs of probabilities rather than multiplying probabilities. Please pay attention to probabilites that are zero! They will need special attention. Count up how many times you need to process a zero probabilty for each class and report. \n",
    "\n",
    "   Report the performance of your learnt classifier in terms of misclassifcation error rate of your multinomial Naive Bayes Classifier. Plot a histogram of the  posterior probabilities (i.e., Pr(Class|Doc)) for each class over the training set. Summarize what you see. \n",
    "\n",
    "   Error Rate = misclassification rate with respect to a provided set (say training set in this case). It is more formally defined here:\n",
    "\n",
    "Let DF represent the evalution set in the following:\n",
    "Err(Model, DF) = |{(X, c(X)) ∈ DF : c(X) != Model(x)}|   / |DF|\n",
    "\n",
    "Where || denotes set cardinality; c(X) denotes the class of the tuple X in DF; and Model(X) denotes the class inferred by the Model “Model”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Mapper 1</span>\n",
    "\n",
    "In this implementation of Multinomial Naive Bayes we will use 2 mapreduce jobs instead of one.\n",
    "\n",
    "Mapper 1 Output :\n",
    "```\n",
    "For each email, mapper outputs overall document class and than emits each word along with its frequency\n",
    "\"DOC_CLASS\" 1 or 0\n",
    "\"word\" \"Spam or Ham : 1 or 0\" \"frequency of word\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper1.py\n",
    "#!/usr/bin/python\n",
    "#Mapper HW 2.3\n",
    "# This mapper will go through each email and emit the count of each word alng with document class\n",
    "import sys\n",
    "import re\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    " \n",
    "# input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #combine email subject and body  and remove leading and trailing spacers\n",
    "        text = \" \".join(content[-2:]).strip()\n",
    "        # Find all words\n",
    "        result = re.findall(WORD_RE,text)\n",
    "\n",
    "        #build a vocabluary of words \n",
    "        vocab ={}\n",
    "        for w in result:\n",
    "            word = w.lower()\n",
    "            if word in vocab:\n",
    "                vocab[word] += 1\n",
    "            else:\n",
    "                vocab[word] = 1\n",
    "                \n",
    "        doc_class = content[1]       \n",
    "        for key, value in vocab.iteritems():\n",
    "            print key + \",\" +str(doc_class)+\"\\t\" +str(value)\n",
    "            \n",
    "        #emit SPAM or HAM for document counts \n",
    "        print \"DOC_CLASS,\"+str(doc_class) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Reducer 1</span>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "spam_email_cnt  = 0 # Total count of spam emails \n",
    "ham_email_cnt = 0 #Total count of non spam emails \n",
    "total_spam_words = 0 #Total count of words in all spam emails \n",
    "total_ham_words = 0 # Total count of words in all non spam emails \n",
    "spam_words_freq = {} # Dictionary to store overall frequency of words for spam emails\n",
    "ham_words_freq ={} # Dictionary to store overall frequency of words for non spam emails\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    key, value = line.split(',', 1)\n",
    "    \n",
    "    if(key ==\"DOC_CLASS\"):\n",
    "        #increment spam or ham count \n",
    "        spam_email_cnt += int(value)\n",
    "        ham_email_cnt += not(int(value))\n",
    "        continue\n",
    "    \n",
    "    # Split the value by <TAB> delimiter\n",
    "    values = re.split(r'\\t+', value)\n",
    "\n",
    "    #Parse the values\n",
    "    true_class,freq = int(values[0]), int(values[1])\n",
    "    word = key\n",
    "             \n",
    "    #increment spam words, total spam words,ham words, total ham words  based on class of email\n",
    "    if (true_class == 1):\n",
    "        total_spam_words += freq;\n",
    "        if word in spam_words_freq:\n",
    "            spam_words_freq[word] += freq\n",
    "        else:\n",
    "            spam_words_freq[word] = freq\n",
    "        if word not in ham_words_freq:\n",
    "            ham_words_freq[word] = 0\n",
    "    else:\n",
    "        total_ham_words += freq;\n",
    "        if word in ham_words_freq:\n",
    "            ham_words_freq[word] += freq\n",
    "        else:\n",
    "            ham_words_freq[word] = freq\n",
    "        if word not in spam_words_freq:\n",
    "            spam_words_freq[word] = 0\n",
    "\n",
    "#Now calculate prior probabilities for spam & ham\n",
    "p_sp = (1.0)*spam_email_cnt / (spam_email_cnt + ham_email_cnt )\n",
    "prior_spam = math.log(p_sp)\n",
    "prior_ham = math.log(1-p_sp)\n",
    "\n",
    "#output the prior probabilities\n",
    "print \"OVERALL,\"+str(prior_spam)+\",\"+str(prior_ham)\n",
    "\n",
    "# Calculate Conditional Probability of word given email class spam and ham\n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "for word in ham_words_freq:\n",
    "    #if findword frequency is greater than 0 than calculate probability else set the value to smallest possible value\n",
    "    if(spam_words_freq[word] > 0 ):\n",
    "        pr_word_spam[word] = math.log((1.0)*(spam_words_freq[word])/ (total_spam_words))\n",
    "    else:\n",
    "        #setting value to 0\n",
    "        pr_word_spam[word] = 0      \n",
    "    if(ham_words_freq[word] > 0):\n",
    "        pr_word_ham[word] = math.log((1.0)*(ham_words_freq[word])/(total_ham_words))\n",
    "    else:\n",
    "        pr_word_ham[word] = 0\n",
    "    print word+\",\"+str(pr_word_spam[word])+\",\"+str(pr_word_ham[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:20:38 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/ModelOutput_23\n",
      "16/01/25 22:20:39 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:20:42 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:20:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def hw2_3():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/ModelOutput_23\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -mapper mapper1.py \\\n",
    "    -reducer reducer1.py \\\n",
    "    -input hw2/src/enronemail_1h.txt \\\n",
    "    -output hw2/output/ModelOutput_23\n",
    "    \n",
    "    #move files so next mapper can pick the model\n",
    "    !hdfs dfs -cat hw2/output/ModelOutput_23/part* > Model_23.txt\n",
    "    !hdfs dfs -put Model_23.txt hw2/output/ModelOutput_23/\n",
    "\n",
    "hw2_3()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:blue;\">Mapper 2</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "# Mapper 2.3.\n",
    "# This mapper will use model output from previous reducer and use it for classification of emails \n",
    "    \n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "pr_cc_word={}\n",
    "\n",
    "#Read the model parameters \n",
    "data = None\n",
    "with open('Model_23.txt', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if(row[0]==\"OVERALL\"):\n",
    "            prior_spam = float(row[1])\n",
    "            prior_ham = float(row[2])\n",
    "            continue\n",
    "        pr_cc_word[row[0]]={'spam':float(row[1]),'ham':float(row[2])}\n",
    "\n",
    "        \n",
    "\n",
    "#input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #combine email subject and body  and remove leading and trailing spacers\n",
    "        text = \" \".join(content[-2:]).strip()\n",
    "        # Find all words\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        \n",
    "        pr_spam_doc = prior_spam\n",
    "        pr_ham_doc = prior_ham\n",
    "        #loop through each word and keep on computing the posterior probabilities\n",
    "        for w in result:\n",
    "            word = w.lower()\n",
    "            # calculate prob for spam , ham for each email \n",
    "            if(pr_cc_word[word]['spam'] <> 0):\n",
    "                pr_spam_doc +=  pr_cc_word[word]['spam']\n",
    "            else:\n",
    "                pr_spam_doc += float('-inf')\n",
    "            if(pr_cc_word[word]['ham'] <> 0):\n",
    "                pr_ham_doc += pr_cc_word[word]['ham']\n",
    "            else:\n",
    "                 pr_ham_doc += float('-inf')\n",
    "\n",
    "        predicted_class = \"0\"\n",
    "        #Determine the predicted class\n",
    "        if(pr_spam_doc == float('-inf')) :\n",
    "            predicted_class = \"0\"\n",
    "            pr_spam_doc = 0\n",
    "        elif(pr_ham_doc == float('-inf')):\n",
    "            predicted_class = \"1\"\n",
    "            pr_ham_doc = 0\n",
    "        elif(pr_spam_doc > pr_ham_doc):\n",
    "            predicted_class = \"1\"\n",
    "        \n",
    "        #prepare the value output \n",
    "        value = content[1]+\"\\t\" + predicted_class+ \"\\t\" + str(pr_spam_doc)+ \"\\t\" +str(pr_ham_doc)\n",
    "        \n",
    "        #emit key. value combination\n",
    "        print content[0] + \"\\t\" + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer2.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from __future__ import division\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "\n",
    "incorrect=0\n",
    "total = 0\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # Split the value by <TAB> delimiter\n",
    "    values = re.split(r'\\t+', line)\n",
    "\n",
    "    #Parse the values\n",
    "    true_class, predicted_class = int(values[1]),int(values[2])\n",
    "    \n",
    "    if true_class != predicted_class:#if predicted class is different from true class increment the count\n",
    "        incorrect+=1\n",
    "    total += 1\n",
    "    print line\n",
    "\n",
    "print \"Training error: : %2.3f\" %(1.0*incorrect/total)    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:27:44 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/ClassificationOutput_23\n",
      "16/01/25 22:27:45 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 22:27:45 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "\n",
      "Training error of Mapreduce Multinomial Naive Bayes \n",
      "\n",
      "16/01/25 22:27:48 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Training error: : 0.000\t\n",
      "\n",
      "\n",
      "16/01/25 22:27:49 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def hw2_31():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/ClassificationOutput_23\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -mapper mapper2.py  \\\n",
    "    -reducer reducer2.py \\\n",
    "    -input hw2/src/enronemail_1h.txt \\\n",
    "    -output hw2/output/ClassificationOutput_23 \\\n",
    "    -jobconf mapred.reduce.tasks=1\n",
    "\n",
    "    print \"\\n\"\n",
    "    print\"Training error of Mapreduce Multinomial Naive Bayes \\n\"\n",
    "    !hdfs dfs -cat hw2/output/ClassificationOutput_23/part-00000 | tail -1\n",
    "    \n",
    "    print \"\\n\"\n",
    "    #move files so next mapper can pick the model\n",
    "    !hdfs dfs -cat hw2/output/ClassificationOutput_23/part* > Classification_23.txt\n",
    "        \n",
    "hw2_31()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x114133fd0>"
      ]
     },
     "execution_count": 650,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114133990>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAa8AAAGHCAYAAAAHlN1aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSeVYvCRJ5Vi8JEnlWLwkSeX8f7m5KcxrsiUuAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111864490>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.figure.Figure at 0x114392ed0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbQAAAGHCAYAAADY2qdFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3XuUZWdZ5/Hvk7RMkIR0ByQtIUlxM1FH6MkSVNChhDgS\nggQdDYhcEnSWS0RBxEnCiAGWDMbxkhGQMUOgExRCAAkXcYgxFBgFgRmaawJE6M7NdEw6CREEQ3jm\nj70rnt6py6naVb33+57vZ61eXftc9nl+59Spp/b7nHMqMhNJkkp30NAFSJK0EWxokqQq2NAkSVWw\noUmSqmBDkyRVwYYmSaqCDU37iYjXRcR/26B9HR0RX4mIaLc/EBHP3Yh9t/t7X0Q8a6P2t4bb/e2I\n+KeIuOEA3NaXI+Lx67zusvf3So9NRDwjIv7P+qu+x239UkTc2N7eto3ar9RlQ5shEbE7Ir4WEbdH\nxL6IuCIifnHxhxpAZv5SZr5yin2t+oM2M6/NzPvmBrzZMSLOjogLO/t/Uma+qe++11jH0cCLgOMz\n84FLnP+4iLir/eF9e0RcGRGnHcgap7HSY5OZb87MJy5uR8S3IuIh67mdiNgC/D5wYnt7t66/6rv3\neY/vvYh4TkT8Td99q2w2tNmSwMmZeThwLPA7wBnA+Rt9QxFx8EbvcySOBW7OzFtWuMz17Q/vw4Ez\ngf8dEcd3L1TQfdTnF5LtwL8DrlzPlSd/2ZqCnxIx42xosycAMvOOzHwv8DTgORHxPQAR8caIeEX7\n9f0i4j0RcWtE3BIRH2xPvxA4BnhPeyTy4og4tv1N/rkRsQf464nTJr/PHhYRf98evbwzIra2+3xc\nRFy7X6Htb+IR8ePAS4CnRcQdEfGJ9vzJZbKIiN9sj0JvjIidEXHf9rzFOp4dEXsi4qaIeMmyd1DE\nfSPiwvZyX15cgo2IJwCXAg9sc79htTs7M98F3Ap8z1L3Ubvfp0TEZ9qj5suXaH6PjojPto/B+RFx\nr/Z6W9vH56b2vPdExFGd6y53fy/12Czmv/top33MA/hUm/nUiPh0RJw8cfkt0SzBPrKzn4cDV7Wb\nt0bEZe3pj4mIj7bfV38fET80cZ0PRLOke0VEfBV48Gr38VIi4oyIuLqt+TMR8dROvisi4g/aGq6O\niB9qT7+m/f559npuV8Oyoc24zPwYcB3wI0uc/evAtcD9gAfQNBUy89nANcCT2yOR35u4zn8Ejgd+\nfPEmOvt8FnAazW/udwGvnixnmRrfD/x34K2ZeVhm/oclLnY68GzgccBDgMOA13Qu81jg4cCJwG9F\nxHFL3V57vcOAOWAeeHZEnJ6Zfw2cBNzQ5l5xHtg22Z8EDgc+PXHW3fdR+0P/zcCvAt8B/CXNLwpb\nJi7/DODHgIcCxwG/2Z5+EPAG4GiaXzC+tkTmNd/fk+dl5uPa7e9rM18MXNDud9HJNPfJJ/fbQeYX\nge9tNw/PzBOjmaG9FziX5vvqD4G/iP1na88EfoHmMdizQo2TukdyVwOPzcz7Ai8H/jQijpw4/9HA\nLuAI4C3ARcD309zHzwJeExHfPuVtayRsaAK4geaJ3XUn8J3AgzPzrsz828753R8iCZydmf+Smd9Y\n5rbelJlXZua/AC8FfiZiTctKy3kG8AeZuSczvwacBTx94ggkgZdl5r9m5qeATwKP7O6kvfzTgDMz\n82uZuYdmBrSWF58cFRH7gH+iyfjM9of7Yh2T99HTgPdm5uWZeRfwe8C9gcdM7O/VmXlDZt4GvBL4\nWYDM3JeZ78zMb2TmV4FX0TTLSd37+9R13t+T1/kz4KSIOLTdfiaw2ixz8fonA19o53TfysyLaI7i\nfmLisjsz86r2/LuW2d8l7RHtvva+fu3kmZn5jszc2379NuCLNE1s0Zcz88J2hvhW4EHAyzPzzsz8\nK+BfgYetkkkjY0MTwFHAviVO/x/APwCXtssyZ0yxr+tWOX9yWXEP8G3A/aeqcmUPZP/f5vcAW4DJ\n38r3Tnz9NeBQ7un+7fWu6eyru5S3kusz84jMvH9mntD+QJ00eR/tV3f7A/bazu1NXn5Pex0i4t4R\n8SftMuttwAeBrZ2GteH3d2b+I/C3wH+OiMNpjlr/bMqrdx+nxbom817L6k5p7+MjMvMI4HmTZ7bL\ny59olxRvpTlSnMw9+b3wLwCZeXPntKW+PzRiNrQZFxGPovkhc49XiGXmP2fmizPzocBTgBdFxI8u\nnr3MLlcbzB898fWxNEeBNwNfBe5e4onmBRPfsYb93tDur7vvvUtffFk3t9fr7uv6Ne5nJZNZunVD\ncx9d19merGXx7QIvpllCfVRmbuXfjs5ihev+K03Gvi6kOWr9GeDv2iY3jRtolnInHcP+9+80L+5Y\n9igzIo4BzgOel5nbMnMb8NmVrqM62NBmVEQcFhFPppkfvCkzP7fEZU6OiIe2m3cA36SZw0DTKLov\n5V7qB0b3tGdGxPHtfOLlwNvao5IvAIdExEnt/Og3gXtNXG8vMLfCctlbgF+LiLl2KeyVwEWZ+a0V\naruH9vIXA6+MiEMj4ljg11h9SW1a3TouBk6OiB9tX1zxYuDrwIcnLvPLEXFURBxBM8e8qD39UJoj\nia+0571sidtb7v5eqpbl3Mg9H+tLgBNoZn8X3uMa+5u8nfcBD4+Ip0fEwRHxNOC7gfdMWcs07gN8\nC7g5Ig6KiNOBf7+GGlUoG9rseU9E3E6zpHYWzcxmuRc3PBy4LCLuoFliem1mfqg971XAS9sZxova\n05b6zTo7X7+J5kUFN9A0rBcAZOZXaJaNzqc5OrmD/Y9S3kbzQ+eWiPj4Evt+Q7vvD9Esk36N5oft\nUnUsV+uiX22v/6V2f3+amW9c4fJrsd/tZuYXaGZQr6GZuZ0M/ERmfnPi8m+meXXl1TSzoMX3CZ5L\nc1R7M/B3NM2ie1tL3t9L1LLS/fEy4ML2sf7ptu6vA++geRXin68UeHLfmbkPeDLN0eXN7f8nT7w/\nbZqjsxUvk5lX0sw9P0LTjL8XuGKN+/QtAAWK1d7zGhHn03wD7s3MR7SnbaMZpB4L7AZOzczb2/PO\novkB+U3gBZl56aZVL2kwEfFS4OHtq16lwU1zhPZG/u0l2IvOBC7LzOOAy2l+0yea9zKdSrOEcBLw\nxxv0CjZJI9Iucf488CdD1yItWrWhZeYVNG8MnXQKzTIG7f+Lb1p8Cs3c4puZuZt7vlRWUuEi4hdo\nlqz/Yom3ckiDWe8M7QET7/G4keZNt9C89HbyJbfXs7aXO0saucx8fWYempm/PHQt0qSNelGIA1RJ\n0qC2rH6RJe2NiCMzc29EbAduak+/nv3f9/Iglnn/TkTYBCVJ95CZ63rtxbRHaMH+79N4N83nwwE8\nB3jXxOlPj4h7RcSDaT465qPL7TQzi/539tlnD16DGcwwhn+l12+G8fzrY9UjtIh4M80HtN4vIq4B\nzqb5syNvi+aTzvfQvLKRzPxcRFwMfI7m0xaelytUePPNG/GBBRvr4IMPZtu26f4G4e7duze3mAPA\nDONQeobS6wcz1GDVhpaZz1jmrBOXufyraN50u6ozznjLNBc7oCK+wstffhpHHeVrWSSpJOudoW2I\no4/+lSFvfknXXXch3/jGch8Uv7/TTjttc4s5AMwwDqVnKL1+MEMNVv2kkE274Yg8++zxvS7kuusu\n5CUv+WEe8pB1/cV5SVIPEUFu8otCtISFhYWhS+jNDONQeobS6wcz1MCGJkmqgkuOHS45StJwXHKU\nJM08G1oPNaxXm2EcSs9Qev1ghhrY0CRJVXCG1uEMTZKG4wxNkjTzbGg91LBebYZxKD1D6fWDGWpg\nQ5MkVcEZWoczNEkajjM0SdLMs6H1UMN6tRnGofQMpdcPZqiBDU2SVAVnaB3O0CRpOM7QJEkzz4bW\nQw3r1WYYh9IzlF4/mKEGNjRJUhWcoXU4Q5Ok4ThDkyTNPBtaDzWsV5thHErPUHr9YIYa2NAkSVVw\nhtbhDE2ShuMMTZI082xoPdSwXm2GcSg9Q+n1gxlqYEOTJFXBGVqHMzRJGo4zNEnSzLOh9VDDerUZ\nxqH0DKXXD2aogQ1NklQFZ2gdztAkaTjO0CRJM8+G1kMN69VmGIfSM5ReP5ihBjY0SVIVnKF1OEOT\npOE4Q5MkzTwbWg81rFebYRxKz1B6/WCGGtjQJElVcIbW4QxNkobjDE2SNPNsaD3UsF5thnEoPUPp\n9YMZamBDkyRVwRlahzM0SRqOMzRJ0syzofVQw3q1Gcah9Ayl1w9mqIENTZJUBWdoHc7QJGk4ztAk\nSTPPhtZDDevVZhiH0jOUXj+YoQY2NElSFZyhdThDk6ThOEOTJM08G1oPNaxXm2EcSs9Qev1ghhrY\n0CRJVXCG1uEMTZKG4wxNkjTzejW0iPi1iPhMRHwqIv4sIu4VEdsi4tKI+HxEvD8iDt+oYsemhvVq\nM4xD6RlKrx/MUIN1N7SIeCDwK8AJmfkIYAvws8CZwGWZeRxwOXDWRhQqSdJK1j1Daxvah4EdwB3A\nnwN/BLwGeFxm7o2I7cBCZh6/xPWdoUmS9jPIDC0zbwB+H7gGuB64PTMvA47MzL3tZW4EHrDe25Ak\naVp9lhy3AqcAxwIPBO4TET8HdA+7xncYtkFqWK82wziUnqH0+sEMNdjS47onAl/KzH0AEfFO4DHA\n3og4cmLJ8abldnDJJaexdescAIccspXt23cwNzcPwO7dCwAHfHtLe48sfmPMz88vu71r164Vzy9h\ne9FY6pnV7V27do2qnlmr3+fzcNsLCwvs3LkTgLm5OfroM0N7NHA+8CjgG8AbgY8BxwD7MvOciDgD\n2JaZZy5xfWdokqT99JmhrfsILTM/GhFvBz4B3Nn+fx5wGHBxRDwX2AOcut7bkCRpWr3eh5aZL8/M\n787MR2TmczLzzszcl5knZuZxmfmfMvO2jSp2bLqH+SUywziUnqH0+sEMNfCTQiRJVfCzHDucoUnS\ncPwsR0nSzLOh9VDDerUZxqH0DKXXD2aogQ1NklQFZ2gdztAkaTjO0CRJM8+G1kMN69VmGIfSM5Re\nP5ihBjY0SVIVnKF1OEOTpOE4Q5MkzTwbWg81rFebYRxKz1B6/WCGGtjQJElVcIbW4QxNkobjDE2S\nNPNsaD3UsF5thnEoPUPp9YMZamBDkyRVwRlahzM0SRqOMzRJ0syzofVQw3q1Gcah9Ayl1w9mqIEN\nTZJUBWdoHc7QJGk4ztAkSTPPhtZDDevVZhiH0jOUXj+YoQY2NElSFZyhdThDk6ThOEOTJM08G1oP\nNaxXm2EcSs9Qev1ghhrY0CRJVXCG1uEMTZKG4wxNkjTzbGg91LBebYZxKD1D6fWDGWpgQ5MkVcEZ\nWoczNEkajjM0SdLMs6H1UMN6tRnGofQMpdcPZqiBDU2SVAVnaB3O0CRpOM7QJEkzz4bWQw3r1WYY\nh9IzlF4/mKEGNjRJUhWcoXU4Q5Ok4ThDkyTNPBtaDzWsV5thHErPUHr9YIYa2NAkSVVwhtbhDE2S\nhuMMTZI082xoPdSwXm2GcSg9Q+n1gxlqYEOTJFXBGVqHMzRJGo4zNEnSzLOh9VDDerUZxqH0DKXX\nD2aogQ1NklQFZ2gdztAkaTjO0CRJM8+G1kMN69VmGIfSM5ReP5ihBjY0SVIVnKF1OEOTpOEMNkOL\niMMj4m0RcWVEfDYifiAitkXEpRHx+Yh4f0Qc3uc2JEmaRt8lx/8JvC8zvxt4JHAVcCZwWWYeB1wO\nnNXzNkarhvVqM4xD6RlKrx/MUIN1N7SIuC/wI5n5RoDM/GZm3g6cAlzQXuwC4Km9q5QkaRXrnqFF\nxCOB84DP0RydfRx4IXB9Zm6buNy+zDxiies7Q5Mk7WeoGdoW4ATgtZl5AvBVmuXGbpcaX9eSJFVn\nS4/rXgdcm5kfb7ffQdPQ9kbEkZm5NyK2Azctt4NLLjmNrVvnADjkkK1s376Dubl5AHbvXgA44Ntb\n2ntkcS16fn5+2e1du3bxwhe+cOrLj3F78bSx1LOe7W6WoetZz/a5557Ljh07RlPPrNXv83m47YWF\nBXbu3AnA3NwcffR62X5EfBD4L5n5hYg4G/j29qx9mXlORJwBbMvMM5e4bvFLjgsLC3c/QKUywziU\nnqH0+sEMY9FnybFvQ3sk8Hrg24AvAacDBwMXA0cDe4BTM/O2Ja5bfEOTJG2sPg2tz5IjmflJ4FFL\nnHVin/1KkrRWfvRVD5Pr1qUywziUnqH0+sEMNbChSZKq4Gc5djhDk6Th+PfQJEkzz4bWQw3r1WYY\nh9IzlF4/mKEGNjRJUhWcoXU4Q5Ok4ThDkyTNPBtaDzWsV5thHErPUHr9YIYa2NAkSVVwhtbhDE2S\nhuMMTZI082xoPdSwXm2GcSg9Q+n1gxlqYEOTJFXBGVqHMzRJGo4zNEnSzLOh9VDDerUZxqH0DKXX\nD2aogQ1NklQFZ2gdztAkaTjO0CRJM8+G1kMN69VmGIfSM5ReP5ihBjY0SVIVnKF1OEOTpOE4Q5Mk\nzTwbWg81rFebYRxKz1B6/WCGGtjQJElVcIbW4QxNkobjDE2SNPNsaD3UsF5thnEoPUPp9YMZamBD\nkyRVwRlahzM0SRqOMzRJ0syzofVQw3q1Gcah9Ayl1w9mqIENTZJUBWdoHc7QJGk4ztAkSTPPhtZD\nDevVZhiH0jOUXj+YoQY2NElSFZyhdThDk6ThOEOTJM08G1oPNaxXm2EcSs9Qev1ghhrY0CRJVXCG\n1uEMTZKG4wxNkjTzbGg91LBebYZxKD1D6fWDGWpgQ5MkVcEZWoczNEkajjM0SdLMs6H1UMN6tRnG\nofQMpdcPZqiBDU2SVAVnaB3O0CRpOM7QJEkzz4bWQw3r1WYYh9IzlF4/mKEGNjRJUhWcoXU4Q5Ok\n4ThDkyTNPBtaDzWsV5thHErPUHr9YIYa2NAkSVXoPUOLiIOAjwPXZeZTImIb8FbgWGA3cGpm3r7E\n9ZyhSZL2M/QM7QXA5ya2zwQuy8zjgMuBszbgNiRJWlGvhhYRDwKeBLx+4uRTgAvary8AntrnNsas\nhvVqM4xD6RlKrx/MUIO+R2h/CPwGMLl2eGRm7gXIzBuBB/S8DUmSVrXuGVpEnAyclJnPj4h54EXt\nDO3WzNw2cblbMvN+S1zfGZokaT99ZmhbetzuY4GnRMSTgHsDh0XEm4AbI+LIzNwbEduBm5bbwSWX\nnMbWrXMAHHLIVrZv38Hc3DwAu3cvABzw7S3tPbJ46D4/P++222677fYmbS8sLLBz504A5ubm6GND\nPikkIh4H/Hp7hPa7wC2ZeU5EnAFsy8wzl7hO8UdoCwsLdz9ApTLDOJSeofT6wQxjMfSrHLt+B/ix\niPg88IR2W5KkTeVnOXY4Q5Ok4YztCE2SpAPOhtbD4mCzZGYYh9IzlF4/mKEGNjRJUhWcoXU4Q5Ok\n4ThDkyTNPBtaDzWsV5thHErPUHr9YIYa2NAkSVVwhtbhDE2ShuMMTZI082xoPdSwXm2GcSg9Q+n1\ngxlqYEOTJFXBGVqHMzRJGo4zNEnSzLOh9VDDerUZxqH0DKXXD2aogQ1NklQFZ2gdztAkaTjO0CRJ\nM8+G1kMN69VmGIfSM5ReP5ihBjY0SVIVnKF1OEOTpOE4Q5MkzTwbWg81rFebYRxKz1B6/WCGGtjQ\nJElVcIbW4QxNkobjDE2SNPNsaD3UsF5thnEoPUPp9YMZamBDkyRVwRlahzM0SRqOMzRJ0syzofVQ\nw3q1Gcah9Ayl1w9mqIENTZJUBWdoHc7QJGk4ztAkSTPPhtZDDevVZhiH0jOUXj+YoQY2NElSFZyh\ndThDk6ThOEOTJM08G1oPNaxXm2EcSs9Qev1ghhrY0CRJVXCG1uEMTZKG4wxNkjTzbGg91LBebYZx\nKD1D6fWDGWpgQ5MkVcEZWoczNEkajjM0SdLMs6H1UMN6tRnGofQMpdcPZqiBDU2SVAVnaB3O0CRp\nOM7QJEkzz4bWQw3r1WYYh9IzlF4/mKEGNjRJUhWcoXU4Q5Ok4ThDkyTNPBtaDzWsV5thHErPUHr9\nYIYa2NAkSVVwhtbhDE2ShuMMTZI082xoPdSwXm2GcSg9Q+n1gxlqsO6GFhEPiojLI+KzEfHpiPjV\n9vRtEXFpRHw+It4fEYdvXLmSJC1t3TO0iNgObM/MXRFxKPB/gVOA04FbMvN3I+IMYFtmnrnE9Z2h\nSZL2M8gMLTNvzMxd7df/DFwJPIimqV3QXuwC4KnrvQ1Jkqa1ITO0iJgDdgAfAY7MzL3QND3gARtx\nG2NUw3q1Gcah9Ayl1w9mqEHvhtYuN74deEF7pNZdRxzfuqIkqTpb+lw5IrbQNLM3Zea72pP3RsSR\nmbm3nbPdtNz1L7nkNLZunQPgkEO2sn37Dubm5gHYvXsB4IBvb2nvkcXfdObn51fcXjTt5d3e+O35\n+flR1bOe7cXTxlLPrNXv83m47YWFBXbu3AnA3NwcffR6Y3VEXAjcnJkvmjjtHGBfZp7ji0IkSWsx\nyItCIuKxwM8Bj4+IT0TE/4uIJwLnAD8WEZ8HngD8znpvY+y6v9WVyAzjUHqG0usHM9Rg3UuOmfm3\nwMHLnH3ievcrSdJ6+FmOHS45StJw/CxHSdLMs6H1UMN6tRnGofQMpdcPZqiBDU2SVAVnaB3O0CRp\nOM7QJEkzz4bWQw3r1WYYh9IzlF4/mKEGNjRJUhWcoXU4Q5Ok4ThDkyTNPBtaDzWsV5thHErPUHr9\nYIYa2NAkSVVwhtbhDE2ShuMMTZI082xoPdSwXm2GcSg9Q+n1gxlqYEOTJFXBGVqHMzRJGo4zNEnS\nzLOh9VDDerUZxqH0DKXXD2aogQ1NklQFZ2gdztAkaTjO0CRJM8+G1kMN69VmGIfSM5ReP5ihBjY0\nSVIVnKF1OEOTpOE4Q5MkzTwbWg81rFebYRxKz1B6/WCGGtjQJElVcIbW4QxNkobjDE2SNPNsaD3U\nsF5thnEoPUPp9YMZamBDkyRVwRlahzM0SRqOMzRJ0syzofVQw3q1Gcah9Ayl1w9mqIENTZJUBWdo\nHc7QJGk4ztAkSTPPhtZDDevVZhiH0jOUXj+YoQY2NElSFZyhdThDk6ThOEOTJM08G1oPNaxXm2Ec\nSs9Qev1ghhrY0CRJVXCG1uEMTZKG4wxNkjTzbGg91LBebYZxKD1D6fWDGWpgQ5MkVcEZWoczNEka\njjM0SdLMs6H1UMN6tRnGofQMpdcPZqiBDU2SVAVnaB3O0CRpOM7QJEkzz4bWQw3r1WYYh9IzlF4/\nmKEGNjRJUhWcoXU4Q5Ok4ThDkyTNvE1raBHxxIi4KiK+EBFnbNbtDKmG9WozjEPpGUqvH8xQgy2b\nsdOIOAh4DfAE4AbgYxHxrsy8ajNubyi7du1ifn5+6DJ6McM4lJ6h9Pqhzgy/9Vvncs01tw1X0AqO\nOWYrr3jFCzd0n5vS0IBHA1/MzD0AEXERcApQVUO77bZxfqOshRnGofQMpdcPdWa45prbmJt72TDF\nrGL37pdt+D43a8nxKODaie3r2tMkSdoUm3WENpVrr33zkDe/pLvuupGDDpquz+/evXtzizkAzDAO\npWcovX4wQw025WX7EfGDwMsy84nt9plAZuY5E5cZ32v2JUmDW+/L9jeroR0MfJ7mRSH/CHwU+NnM\nvHLDb0ySJDZpyTEz74qI5wOX0szpzreZSZI202CfFCJJ0kba9E8KmeYN1hHxRxHxxYjYFRE7Nrum\ntVotQ0Q8IyI+2f67IiK+b4g6VzLtG90j4lERcWdE/NSBrG81U34fzUfEJyLiMxHxgQNd42qm+D66\nb0S8u30efDoiThugzBVFxPkRsTciPrXCZUb7fF6t/kKey6s+Bu3lRvlchqm/j9b+fM7MTftH0zCv\nBo4Fvg3YBRzfucxJwF+0X/8A8JHNrGmTMvwgcHj79RNLzDBxub8G3gv81NB1r/ExOBz4LHBUu33/\noeteR4azgFct1g/cAmwZuvZOjT8M7AA+tcz5Y38+r1b/qJ/L02SY+H4b3XN5DY/Dup7Pm32Edvcb\nrDPzTmDxDdaTTgEuBMjMvwcOj4gjN7mutVg1Q2Z+JDNvbzc/wvjeczfN4wDwK8DbgZsOZHFTmKb+\nZwDvyMzrATLz5gNc42qmyZDAYe3XhwG3ZOY3D2CNq8rMK4BbV7jIqJ/Pq9VfwHN5mscAxvtcBqbK\nsK7n82Y3tGneYN29zPVLXGZIa32T+C8Af7mpFa3dqhki4oHAUzPzdcC6XjK7iaZ5DL4LOCIiPhAR\nH4uIZx2w6qYzTYbXAN8TETcAnwRecIBq20hjfz6vxRify6sa+XN5Wut6Pg/6xuraRMSPAqfTHE6X\n5lxgcq5T2hNhC3AC8HjgPsCHI+LDmXn1sGWtyY8Dn8jMx0fEQ4G/iohHZOY/D13YrPG5PLh1PZ83\nu6FdDxwzsf2g9rTuZY5e5TJDmiYDEfEI4DzgiZm52nLAgTZNhu8HLoqIoJnfnBQRd2bmuw9QjSuZ\npv7rgJsz8+vA1yPiQ8AjaeZWYzBNhtOBVwFk5j9ExJeB44GPH5AKN8bYn8+rGvlzeRpjfi5Pa13P\n581ecvwY8LCIODYi7gU8Hejeqe8Gng13f8LIbZm5d5PrWotVM0TEMcA7gGdl5j8MUONqVs2QmQ9p\n/z2YZu39eSN6AkzzffQu4Icj4uCI+HaaFySM6b2P02TYA5wI0M6dvgv40gGtcjrB8r/1j/35DCvU\nX8BzedGyGUb+XJ600vfRup7Pm3qElsu8wToifrE5O8/LzPdFxJMi4mrgqzS/pY7GNBmAlwJHAH/c\n/lZ0Z2Y+eriq9zdlhv2ucsCLXMGU30dXRcT7gU8BdwHnZebnBix7P1M+Br8N7Jx4KfN/zcx9A5W8\npIh4MzAP3C8irgHOBu5FIc/n1epn5M9lmCrDpFE9lxdN8X20ruezb6yWJFVh099YLUnSgWBDkyRV\nwYYmSaqCDU2SVAUbmiQJmP6Dj9ewv7+MiFsjovtWp9e3H169KyIubl+a35sNTZK06I00n1izUX4X\neOYSp78wM3dk5g6aj0p7/kbcmA1NkgQs/aHBEfGQ9kjrYxHxwYj4rjXs7wPAPT66bfHj3Nr3+t2b\nDXq/nA1Vf4zYAAABB0lEQVRNkrSS84DnZ+ajgN8AXrcRO42INwD/CBwHvHoj9umHE0uSlhQR9wEe\nA7ytPZqC5u/5ERE/CbyC/Y+uArguM09abd+Z+dx2n6+m+Si4nX3rtaFJkpZzEHBrZp7QPSMz3wm8\ns8/OMzMj4q00R347++wLXHKUJO3v7g8Nzsw7gC9HxE/ffWbz1wjWtb+JfTy0/T+ApwBX9Sn47v36\nWY6SJNj/Q4OBvTQfGnw58L+A76RZ1bsoM397yv19iGZGdihwC/DzwGXA39D8Vfag+WO2v7QRf/fP\nhiZJqoJLjpKkKtjQJElVsKFJkqpgQ5MkVcGGJkmqgg1NklQFG5okqQo2NElSFf4/y5mW662qgDEA\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x111ac6e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "!head -n $(($(wc -l < Classification_23.txt) - 1))  < Classification_23.txt > plot_input.txt\n",
    "\n",
    "df = pd.read_csv(\"plot_input.txt\",sep='\\t',header=None)\n",
    "df.columns = ['doc_id','true_class','predicted_class','spam','ham']\n",
    "spam_exp = np.exp(df.spam[df.spam!=0])\n",
    "df['spam_exp'] = np.where(df['spam']==0, 0.0, np.exp(df['spam']))\n",
    "df['ham_exp'] = np.where(df['ham']==0, 0.0, np.exp(df['ham']))\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize = (7,6), dpi = 72)\n",
    "plt.title('Distribution of Probability for Spam')\n",
    "df.spam_exp.hist(alpha=0.5)\n",
    "\n",
    "plt.figure()\n",
    "plt.figure(figsize = (7,6), dpi = 72)\n",
    "plt.title('Distribution of Probability for Ham')\n",
    "df.ham_exp.hist(alpha=0.5)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Draw plot "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.4 \n",
    "Repeat HW2.3 with the following modification: use Laplace plus-one smoothing. Compare the misclassifcation error rates for 2.3 versus 2.4 and explain the differences.\n",
    "\n",
    "For a quick reference on the construction of the Multinomial NAIVE BAYES classifier that you will code,\n",
    "please consult the \"Document Classification\" section of the following wikipedia page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Naive_Bayes_classifier#Document_classification\n",
    "\n",
    "OR the original paper by the curators of the Enron email data:\n",
    "\n",
    "http://www.aueb.gr/users/ion/docs/ceas2006_paper.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#we will reuse the Mapper 1 from 2.3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "spam_email_cnt  = 0 # Total count of spam emails \n",
    "ham_email_cnt = 0 #Total count of non spam emails \n",
    "total_spam_words = 0 #Total count of words in all spam emails \n",
    "total_ham_words = 0 # Total count of words in all non spam emails \n",
    "spam_words_freq = {} # Dictionary to store overall frequency of words for spam emails\n",
    "ham_words_freq ={} # Dictionary to store overall frequency of words for non spam emails\n",
    "unique_word_cnt = 0 # Unique vocab length\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    key, value = line.split(',', 1)\n",
    "    \n",
    "    if(key ==\"DOC_CLASS\"):\n",
    "        #increment spam or ham count \n",
    "        spam_email_cnt += int(value)\n",
    "        ham_email_cnt += not(int(value))\n",
    "        continue\n",
    "    \n",
    "    # Split the value by <TAB> delimiter\n",
    "    values = re.split(r'\\t+', value)\n",
    "\n",
    "    #Parse the values\n",
    "    true_class,freq = int(values[0]), int(values[1])\n",
    "    word = key\n",
    "    \n",
    "    #Determine unique word count for laplace smoothing \n",
    "    if (not(word in spam_words_freq or word in ham_words_freq)):\n",
    "        unique_word_cnt += 1;\n",
    "    \n",
    "    #increment spam words, total spam words,ham words, total ham words  based on class of email\n",
    "    if (true_class == 1):\n",
    "        total_spam_words += freq;\n",
    "        if word in spam_words_freq:\n",
    "            spam_words_freq[word] += freq\n",
    "        else:\n",
    "            spam_words_freq[word] = freq\n",
    "        if word not in ham_words_freq:\n",
    "            ham_words_freq[word] = 0\n",
    "    else:\n",
    "        total_ham_words += freq;\n",
    "        if word in ham_words_freq:\n",
    "            ham_words_freq[word] += freq\n",
    "        else:\n",
    "            ham_words_freq[word] = freq\n",
    "        if word not in spam_words_freq:\n",
    "            spam_words_freq[word] = 0\n",
    "\n",
    "#Now calculate prior probabilities for spam & ham\n",
    "p_sp = (1.0)*spam_email_cnt / (spam_email_cnt + ham_email_cnt )\n",
    "prior_spam = math.log(p_sp)\n",
    "prior_ham = math.log(1-p_sp)\n",
    "\n",
    "print \"OVERALL,\"+str(prior_spam)+\",\"+str(prior_ham)\n",
    "\n",
    "# Calculate Conditional Probability of word given email class spam and ham\n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "#One is added here to do LapLace smoothing\n",
    "for word in ham_words_freq:\n",
    "    #if findword frequency is greater than 0 than calculate probability else set the value to smallest possible value\n",
    "    pr_word_spam[word] = math.log((1.0)*(spam_words_freq[word]+1)/ (total_spam_words + unique_word_cnt))     \n",
    "    pr_word_ham[word] = math.log((1.0)*(ham_words_freq[word]+1)/(total_ham_words + unique_word_cnt))\n",
    "    \n",
    "    print word+\",\"+str(pr_word_spam[word])+\",\"+str(pr_word_ham[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:50:59 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/ModelOutput_24\n",
      "16/01/25 22:51:00 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:51:03 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:51:05 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def hw2_4():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/ModelOutput_24\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -mapper mapper1.py \\\n",
    "    -reducer reducer1.py \\\n",
    "    -input hw2/src/enronemail_1h.txt \\\n",
    "    -output hw2/output/ModelOutput_24\n",
    "    \n",
    "    !hdfs dfs -cat hw2/output/ModelOutput_24/part* > Model_24.txt\n",
    "    !hdfs dfs -put Model_24.txt hw2/output/ModelOutput_24/\n",
    "\n",
    "hw2_4()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### We will reuse reducer2 from HW 2.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "# Mapper how Hw1.3.\n",
    "# This mapper will \n",
    "    \n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "pr_cc_word={}\n",
    "\n",
    "#Read the model parameters \n",
    "data = None\n",
    "with open('Model_24.txt', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if(row[0]==\"OVERALL\"):\n",
    "            prior_spam = float(row[1])\n",
    "            prior_ham = float(row[2])\n",
    "            continue\n",
    "        pr_cc_word[row[0]]={'spam':float(row[1]),'ham':float(row[2])}\n",
    "\n",
    "        \n",
    "\n",
    "#input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #combine email subject and body  and remove leading and trailing spacers\n",
    "        text = \" \".join(content[-2:]).strip()\n",
    "        # Find all words\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        \n",
    "        pr_spam_doc = prior_spam\n",
    "        pr_ham_doc = prior_ham\n",
    "        for w in result:\n",
    "            word = w.lower()\n",
    "            # calculate prob for spam , ham for each email \n",
    "            if(pr_cc_word[word]['spam'] <> 0):\n",
    "                pr_spam_doc +=  pr_cc_word[word]['spam']\n",
    "            else:\n",
    "                pr_spam_doc += float('-inf')\n",
    "            if(pr_cc_word[word]['ham'] <> 0):\n",
    "                pr_ham_doc += pr_cc_word[word]['ham']\n",
    "            else:\n",
    "                 pr_ham_doc += float('-inf')\n",
    "\n",
    "        predicted_class = \"0\"\n",
    "        #Determine the predicted class\n",
    "        if(pr_spam_doc == float('-inf')) :\n",
    "            predicted_class = \"0\"\n",
    "        elif(pr_ham_doc == float('-inf')):\n",
    "            predicted_class = \"1\"\n",
    "        elif(pr_spam_doc > pr_ham_doc):\n",
    "            predicted_class = \"1\"\n",
    "        \n",
    "        #prepare the value output \n",
    "        value = content[1]+\"\\t\" + predicted_class+ \"\\t\" + str(pr_spam_doc)+ \"\\t\" +str(pr_ham_doc)\n",
    "        \n",
    "        #emit key. value combination\n",
    "        print content[0] + \"\\t\" + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:51:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/ClassificationOutput_24\n",
      "16/01/25 22:51:16 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 22:51:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "\n",
      "Training error of Mapreduce Multinomial Naive Bayes with Smoothing \n",
      "\n",
      "16/01/25 22:51:20 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Training error: : 0.000\t\n"
     ]
    }
   ],
   "source": [
    "def hw2_41():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/ClassificationOutput_24\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -mapper mapper2.py  \\\n",
    "    -reducer reducer2.py \\\n",
    "    -input hw2/src/enronemail_1h.txt \\\n",
    "    -output hw2/output/ClassificationOutput_24 \\\n",
    "    -jobconf mapred.reduce.tasks=1\n",
    "\n",
    "    print \"\\n\"\n",
    "    print\"Training error of Mapreduce Multinomial Naive Bayes with Smoothing \\n\"\n",
    "    !hdfs dfs -cat hw2/output/ClassificationOutput_24/part-00000 | tail -1\n",
    "\n",
    "           \n",
    "hw2_41()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.5. \n",
    "Repeat HW2.4. This time when modeling and classification ignore tokens with a frequency of less than three (3) in the training set. How does it affect the misclassifcation error of learnt naive multinomial Bayesian Classifier on the training dataset:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\"> We will re-use Mapper1 from HW 2.3</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <span style=\"color:blue;\">Reducer1</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer1.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "from itertools import groupby\n",
    "from operator import itemgetter\n",
    "import sys\n",
    "import re\n",
    "import math\n",
    "\n",
    "spam_email_cnt  = 0 # Total count of spam emails \n",
    "ham_email_cnt = 0 #Total count of non spam emails \n",
    "total_spam_words = 0 #Total count of words in all spam emails \n",
    "total_ham_words = 0 # Total count of words in all non spam emails \n",
    "spam_words_freq = {} # Dictionary to store overall frequency of words for spam emails\n",
    "ham_words_freq ={} # Dictionary to store overall frequency of words for non spam emails\n",
    "unique_word_cnt = 0 # Unique vocab length\n",
    "\n",
    "for line in sys.stdin:\n",
    "    # remove leading and trailing whitespace\n",
    "    line = line.strip()\n",
    "\n",
    "    # parse the input we got from mapper.py\n",
    "    key, value = line.split(',', 1)\n",
    "    \n",
    "    if(key ==\"DOC_CLASS\"):\n",
    "        #increment spam or ham count \n",
    "        spam_email_cnt += int(value)\n",
    "        ham_email_cnt += not(int(value))\n",
    "        continue\n",
    "    \n",
    "    # Split the value by <TAB> delimiter\n",
    "    values = re.split(r'\\t+', value)\n",
    "\n",
    "    #Parse the values\n",
    "    true_class,freq = int(values[0]), int(values[1])\n",
    "    word = key\n",
    "    \n",
    "    #Determine unique word count for laplace smoothing \n",
    "    if (not(word in spam_words_freq or word in ham_words_freq)):\n",
    "        unique_word_cnt += 1;\n",
    "    \n",
    "    #increment spam words, total spam words,ham words, total ham words  based on class of email\n",
    "    if (true_class == 1):\n",
    "        total_spam_words += freq;\n",
    "        if word in spam_words_freq:\n",
    "            spam_words_freq[word] += freq\n",
    "        else:\n",
    "            spam_words_freq[word] = freq\n",
    "        if word not in ham_words_freq:\n",
    "            ham_words_freq[word] = 0\n",
    "    else:\n",
    "        total_ham_words += freq;\n",
    "        if word in ham_words_freq:\n",
    "            ham_words_freq[word] += freq\n",
    "        else:\n",
    "            ham_words_freq[word] = freq\n",
    "        if word not in spam_words_freq:\n",
    "            spam_words_freq[word] = 0\n",
    "\n",
    "#Now calculate prior probabilities for spam & ham\n",
    "p_sp = (1.0)*spam_email_cnt / (spam_email_cnt + ham_email_cnt )\n",
    "prior_spam = math.log(p_sp)\n",
    "prior_ham = math.log(1-p_sp)\n",
    "\n",
    "print \"OVERALL,\"+str(prior_spam)+\",\"+str(prior_ham)\n",
    "\n",
    "# Calculate Conditional Probability of word given email class spam and ham\n",
    "pr_word_spam = {}\n",
    "pr_word_ham = {}\n",
    "\n",
    "#if frequency of word is less than 3 ignore the word from vocab \n",
    "for word in ham_words_freq:\n",
    "    if( (ham_words_freq[word]+spam_words_freq[word])<3):\n",
    "        unique_word_cnt -= 1\n",
    "\n",
    "#if frequency of word is less than 3 ignore the word and do not calculate the propability\n",
    "for word in ham_words_freq:\n",
    "    if( (ham_words_freq[word]+spam_words_freq[word])<3):\n",
    "        continue\n",
    "    #One is added here to do LapLace smoothing\n",
    "    pr_word_spam[word] = math.log((1.0)*(spam_words_freq[word]+1)/ (total_spam_words + unique_word_cnt))     \n",
    "    pr_word_ham[word] = math.log((1.0)*(ham_words_freq[word]+1)/(total_ham_words + unique_word_cnt))\n",
    "    print word+\",\"+str(pr_word_spam[word])+\",\"+str(pr_word_ham[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 22:52:10 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/ModelOutput_25\n",
      "16/01/25 22:52:11 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:52:15 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "16/01/25 22:52:16 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "def hw2_5():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/ModelOutput_25\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -mapper mapper1.py \\\n",
    "    -reducer reducer1.py \\\n",
    "    -input hw2/src/enronemail_1h.txt \\\n",
    "    -output hw2/output/ModelOutput_25\n",
    "    \n",
    "    !hdfs dfs -cat hw2/output/ModelOutput_25/part* > Model_25.txt\n",
    "    !hdfs dfs -put Model_25.txt hw2/output/ModelOutput_25/\n",
    "\n",
    "hw2_5()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### <span style=\"color:blue;\"> Mapper2</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper2.py\n",
    "#!/usr/bin/python\n",
    "# Mapper for HW 2.4\n",
    "# This mapper will \n",
    "    \n",
    "import sys\n",
    "import re\n",
    "import csv\n",
    "\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "pr_cc_word={}\n",
    "\n",
    "#Read the model parameters \n",
    "data = None\n",
    "with open('Model_24.txt', 'rb') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader:\n",
    "        if(row[0]==\"OVERALL\"):\n",
    "            prior_spam = float(row[1])\n",
    "            prior_ham = float(row[2])\n",
    "            continue\n",
    "        pr_cc_word[row[0]]={'spam':float(row[1]),'ham':float(row[2])}\n",
    "\n",
    "        \n",
    "\n",
    "#input comes from STDIN (standard input)\n",
    "for line in sys.stdin:\n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "        # verify correct content structure else ignore bad data\n",
    "        if len(content) <> 4:\n",
    "            continue\n",
    "        #combine email subject and body  and remove leading and trailing spacers\n",
    "        text = \" \".join(content[-2:]).strip()\n",
    "        # Find all words\n",
    "        result = re.findall(WORD_RE,text)\n",
    "        \n",
    "        pr_spam_doc = prior_spam\n",
    "        pr_ham_doc = prior_ham\n",
    "        for w in result:\n",
    "            word = w.lower()\n",
    "            # calculate prob for spam , ham for each email\n",
    "            if(word not in pr_cc_word):\n",
    "                continue\n",
    "            pr_spam_doc +=  pr_cc_word[word]['spam']\n",
    "            pr_ham_doc += pr_cc_word[word]['ham']\n",
    "\n",
    "        predicted_class = \"0\"\n",
    "        #Determine the predicted class\n",
    "        if(pr_spam_doc == float('-inf')) :\n",
    "            predicted_class = \"0\"\n",
    "        elif(pr_ham_doc == float('-inf')):\n",
    "            predicted_class = \"1\"\n",
    "        elif(pr_spam_doc > pr_ham_doc):\n",
    "            predicted_class = \"1\"\n",
    "        \n",
    "        #prepare the value output \n",
    "        value = content[1]+\"\\t\" + predicted_class+ \"\\t\" + str(pr_spam_doc)+ \"\\t\" +str(pr_ham_doc)\n",
    "        \n",
    "        #emit key. value combination\n",
    "        print content[0] + \"\\t\" + value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/01/25 23:38:53 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Deleted hw2/output/ClassificationOutput_25\n",
      "16/01/25 23:38:55 WARN streaming.StreamJob: -jobconf option is deprecated, please use -D instead.\n",
      "16/01/25 23:38:55 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "\n",
      "\n",
      "Training error of Mapreduce Multinomial Naive Bayes with Smoothing \n",
      "\n",
      "16/01/25 23:38:58 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Training error: : 0.000\t\n"
     ]
    }
   ],
   "source": [
    "def hw2_4_1():\n",
    "    #remove output directory if present else hadoop job gives error\n",
    "    !hdfs dfs -rm -r hw2/output/ClassificationOutput_25\n",
    "    \n",
    "    # run map reduce job\n",
    "    !hadoop jar /usr/local/Cellar/hadoop/2.7.1/libexec/share/hadoop/tools/lib/hadoop-*streaming*.jar \\\n",
    "    -mapper mapper2.py  \\\n",
    "    -reducer reducer2.py \\\n",
    "    -input hw2/src/enronemail_1h.txt \\\n",
    "    -output hw2/output/ClassificationOutput_25 \\\n",
    "    -jobconf mapred.reduce.tasks=1\n",
    "\n",
    "    print \"\\n\"\n",
    "    print\"Training error of Mapreduce Multinomial Naive Bayes with Smoothing \\n\"\n",
    "    !hdfs dfs -cat hw2/output/ClassificationOutput_25/part-* | tail -1\n",
    "    \n",
    "hw2_4_1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW2.6 Benchmark your code with the Python SciKit-Learn implementation of the multinomial Naive Bayes algorithm\n",
    "\n",
    "It always a good idea to benchmark your solutions against publicly available libraries such as SciKit-Learn, The Machine Learning toolkit available in Python. In this exercise, we benchmark ourselves against the SciKit-Learn implementation of multinomial Naive Bayes.  For more information on this implementation see: http://scikit-learn.org/stable/modules/naive_bayes.html more  \n",
    "\n",
    "In this exercise, please complete the following:\n",
    "\n",
    "— Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW2.5 and report the misclassification error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)\n",
    "- Prepare a table to present your results, where rows correspond to approach used (SkiKit-Learn versus your Hadoop implementation) and the column presents the training misclassification error\n",
    "— Explain/justify any differences in terms of training error rates over the dataset in HW2.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using our Multinomial classifier\n",
      "sklearn accuracy: 0.96\n",
      "sk learn training error 0.04\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    " \n",
    "#Create empty arrays for training data and label\n",
    "email_text = []\n",
    "train_data = []\n",
    "train_label =[]\n",
    "\n",
    "with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "    for line in myfile.readlines():\n",
    "        \n",
    "        # Split the line by <TAB> delimiter\n",
    "        content = re.split(r'\\t+', line)\n",
    "            \n",
    "        doc_text = content[2].lower() + \" \"+content[3].lower()         \n",
    "        email_text.append(doc_text)\n",
    "        train_label.append(content[1])\n",
    "    \n",
    "train_label = np.array(train_label)\n",
    "    \n",
    "# Create features for train and dev data \n",
    "count_vectorizer = CountVectorizer(min_df=3)\n",
    "train_data = count_vectorizer.fit_transform(email_text)\n",
    "\n",
    "nb =  MultinomialNB()\n",
    "nb.fit(train_data, train_label)   \n",
    "# Compute accuracy on the test data.\n",
    "print \"Using our Multinomial classifier\"\n",
    "accuracy = nb.score(train_data, train_label)\n",
    "tr_error = 1-accuracy\n",
    "print 'sklearn accuracy: %3.2f' %accuracy\n",
    "print 'sk learn training error %3.2f' %tr_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 2.6 - Summary of Results\n",
    "\n",
    "| Model                                                                      | Training Error |\n",
    "|----------------------------------------------------------------------------|----------------|\n",
    "| Multinomial NB, Scikit-Learn Implementation                                | 0.04            |\n",
    "| Multinomial NB, MapReduce implementation                                   | 0.0          |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
